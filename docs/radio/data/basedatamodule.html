<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>radio.data.basedatamodule API documentation</title>
<meta name="description" content="Based on LightningDataModule for managing data. A datamodule is a shareable,
reusable class that encapsulates all the steps needed to process data, …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>radio.data.basedatamodule</code></h1>
</header>
<section id="section-intro">
<p>Based on LightningDataModule for managing data. A datamodule is a shareable,
reusable class that encapsulates all the steps needed to process data, i.e.,
decoupling datasets from models to allow building dataset-agnostic models. They
also allow you to share a full dataset without explaining how to download,
split, transform, and process the data.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
# coding=utf-8
&#34;&#34;&#34;
Based on LightningDataModule for managing data. A datamodule is a shareable,
reusable class that encapsulates all the steps needed to process data, i.e.,
decoupling datasets from models to allow building dataset-agnostic models. They
also allow you to share a full dataset without explaining how to download,
split, transform, and process the data.
&#34;&#34;&#34;

from abc import ABCMeta, abstractmethod
from typing import Any, Optional, Type, Union, Tuple
from pathlib import Path
import tempfile
import torchio as tio
import pytorch_lightning as pl
from radio.settings.pathutils import DATA_ROOT, PathType
from .validation import (EvalDataLoaderType, TrainDataLoaderType,
                         KFoldValidation, OneFoldValidation, ValidationType)
from .dataset import TrainDatasetType, EvalDatasetType
from .datatypes import EvalSizeType, TrainSizeType

__all__ = [&#34;BaseDataModule&#34;]


class BaseDataModule(pl.LightningDataModule, metaclass=ABCMeta):
    &#34;&#34;&#34;
    Base class for making data modules.

    To create a subclass, you need to implement the following functions:

    A BaseDataModule needs to implement 6 key methods:
    &lt;__init__&gt;:
        (Optionally) Initialize the class, first call super.__init__().
    &lt;prepare_data&gt;:
        Things to do on 1 GPU/TPU not on every GPU/TPU in distributed mode.
    &lt;setup&gt;:
        Things to do on every accelerator in distributed mode.
    &lt;train_dataloader&gt;:
        The training dataloader(s).
    &lt;val_dataloader&gt;:
        The validation dataloader(s).
    &lt;test_dataloader&gt;:
        The test dataloader(s).
    &lt;teardown&gt;:
        Things to do on every accelerator in distributed mode when finished.

    Typical Workflow
    ----------------
    data = BaseDataModule()
    data.prepare_data() # download
    data.setup(stage) # process and split
    data.teardown(stage) # clean-up

    Parameters
    ----------
    root : Path or str, optional
        Root directory of dataset. If None, a temporary directory will be used.
        Default = ``DATA_ROOT / &#39;medical_decathlon&#39;``.
    train_transforms : Callable, optional
        A function/transform that takes in a sample and returns a
        transformed version, e.g, ``torchvision.transforms.RandomCrop``.
    val_transforms : Callable, optional
        A function/transform that takes in a sample and returns a
        transformed version, e.g, ``torchvision.transforms.RandomCrop``.
    test_transforms : Callable, optional
        A function/transform that takes in a sample and returns a
        transformed version, e.g, ``torchvision.transforms.RandomCrop``.
    batch_size : int, optional
        How many samples per batch to load. Default = ``32``.
    shuffle : bool, optional
        Whether to shuffle the data at every epoch. Default = ``False``.
    num_workers : int, optional
        How many subprocesses to use for data loading. ``0`` means that the
        data will be loaded in the main process. Default: ``0``.
    pin_memory : bool, optional
        If ``True``, the data loader will copy Tensors into CUDA pinned memory
        before returning them.
    drop_last : bool, optional
        Set to ``True`` to drop the last incomplete batch, if the dataset size
        is not divisible by the batch size. If ``False`` and the size of
        dataset is not divisible by the batch size, then the last batch will be
        smaller. Default = ``False``.
    num_folds : int, optional
        Number of folds. Must be at least ``2``. ``2`` corresponds to a single
        train/validation split. Default = ``2``.
    val_split: int or float, optional
        If ``num_folds = 2``, then ``val_split`` specify how the
        train_dataset should be split into train/validation datasets. If
        ``num_folds &gt; 2``, then it is not used. Default = ``0.2``.
    seed : int, optional
        When `shuffle` is True, `seed` affects the ordering of the indices,
        which controls the randomness of each fold. It is also use to seed the
        RNG used by RandomSampler to generate random indexes and
        multiprocessing to generate `base_seed` for workers. Pass an int for
        reproducible output across multiple function calls. Default = ``41``.
    &#34;&#34;&#34;
    #: Extra arguments for dataset_cls instantiation.
    EXTRA_ARGS: dict = {}
    #: Dataset class to use. E.g., torchvision.datasets.MNIST
    dataset_cls: type
    #: A tuple describing the shape of the data
    dims: Optional[Tuple[int, int, int]]
    #: Dataset name
    name: str

    def __init__(
        self,
        *args: Any,
        root: PathType = DATA_ROOT,
        train_transforms: Optional[tio.Transform] = None,
        val_transforms: Optional[tio.Transform] = None,
        test_transforms: Optional[tio.Transform] = None,
        batch_size: int = 32,
        shuffle: bool = True,
        num_workers: int = 0,
        pin_memory: bool = True,
        drop_last: bool = False,
        num_folds: int = 2,
        val_split: Union[int, float] = 0.2,
        seed: int = 41,
        **kwargs: Any,
    ) -&gt; None:

        super().__init__(*args, **kwargs)
        num_folds_msg = &#34;``num_folds`` must be an integer of at least 2.&#34;
        assert isinstance(num_folds, int) and num_folds &gt; 1, num_folds_msg
        self.is_temp_dir = bool(root is None)
        # Data folder flags to check if data is splitted already
        self.has_train_val_split: bool
        self.has_train_test_split: bool
        # Dataset Init
        self.root = Path(root).expanduser() if root else Path(
            tempfile.mkdtemp())
        self.train_transforms = train_transforms
        self.val_transforms = val_transforms
        self.test_transforms = test_transforms
        self.train_dataset: TrainDatasetType
        self.val_dataset: EvalDatasetType
        self.test_dataset: EvalDatasetType
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.num_workers = num_workers
        self.pin_memory = pin_memory
        self.drop_last = drop_last
        self.seed = seed
        self.size_train: TrainSizeType
        self.size_val: EvalSizeType
        self.size_test: EvalSizeType
        # Dataloader Init
        self.num_folds = num_folds
        self.val_cls: Type[ValidationType] = (OneFoldValidation if num_folds
                                              == 2 else KFoldValidation)
        self.validation: ValidationType
        self.has_validation = False
        self.val_split = val_split

    @abstractmethod
    def prepare_data(self, *args: Any, **kwargs: Any) -&gt; None:
        &#34;&#34;&#34;
        Data operations to be performed that need to be done only from a single
        process in distributed settings. For example, download and saving data.

        Warning
        -------
        DO NOT set state here (use ``&#39;setup&#39;`` instead) since this is NOT
        called on every device.

        Example
        -------
        def prepare_data(self, *args, **kwargs):
            # good
            download_data()
            tokenize()
            etc()

            # good
            torchvison.datasets.MNIST(self.root, train=True, download=True)
            torchvison.datasets.MNIST(self.root, train=False, download=True)

            # bad
            self.split = data_split
            self.some_state = some_other_state()
        &#34;&#34;&#34;

    @abstractmethod
    def setup(self, stage: Optional[str] = None) -&gt; None:
        &#34;&#34;&#34;
        Data operations to be performed on every GPU. This is a good hook when
        you need to build models dynamically or adjust something about them
        based on the data atrributes. For example, count number of classes,
        build vocabulary, perform train/val/test splits, apply transforms.

        Parameters
        ----------
        stage: Optional[str]
            Either ``&#39;fit``, ``&#39;validate&#39;``, or ``&#39;test&#39;``.
            If stage = None, set-up all stages. Default = None.

        Example
        -------
        def setup(self, stage):
            if stage in (None, &#34;fit&#34;):
                # train + validation set-up
                self.data_train, self.data_val = load_fit(...)
                self.dims = self.data_train[0][0].shape
                self.l1 = nn.Linear(28, self.data_train.num_classes)
            if stage in (None, &#34;test&#34;):
                # test set-up
                self.data_test = load_test(...)
                self.dims = self.data_test[0][0].shape
                self.l1 = nn.Linear(28, self.data_test.num_classes)
        &#34;&#34;&#34;

    @abstractmethod
    def teardown(self, stage: Optional[str] = None) -&gt; None:
        &#34;&#34;&#34;
        Called at the end of fit (train + validate), validate, test,
        or predict.

        Parameters
        ----------
        stage: Optional[str]
            Either ``&#39;fit``, ``&#39;validate&#39;``, or ``&#39;test&#39;``.
            If stage = None, set-up all stages. Default = None.
        &#34;&#34;&#34;

    @abstractmethod
    def train_dataloader(self, *args: Any,
                         **kwargs: Any) -&gt; TrainDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for training.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of train dataloaders specifying training samples.

        Examples
        -------
        # single dataloader
        def train_dataloader(self):
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.5,), (1.0,))
            ])
            dataset = MNIST(
                root=&#39;/path/to/mnist/&#39;,
                train=True,
                transform=transform,
                download=True
            )
            loader = torch.utils.data.DataLoader(
                dataset=dataset,
                batch_size=self.batch_size,
                shuffle=True
            )
            return loader

        # multiple dataloaders, return as list
        def train_dataloader(self):
            mnist = MNIST(...)
            cifar = CIFAR(...)
            mnist_loader = torch.utils.data.DataLoader(
                dataset=mnist, batch_size=self.batch_size, shuffle=True
            )
            cifar_loader = torch.utils.data.DataLoader(
                dataset=cifar, batch_size=self.batch_size, shuffle=True
            )
            # each batch will be a list of tensors: [batch_mnist, batch_cifar]
            return [mnist_loader, cifar_loader]

        # multiple dataloader, return as dict
        def train_dataloader(self):
            mnist = MNIST(...)
            cifar = CIFAR(...)
            mnist_loader = torch.utils.data.DataLoader(
                dataset=mnist, batch_size=self.batch_size, shuffle=True
            )
            cifar_loader = torch.utils.data.DataLoader(
                dataset=cifar, batch_size=self.batch_size, shuffle=True
            )
            # each batch will be a dict of
            tensors: {&#39;mnist&#39;: batch_mnist, &#39;cifar&#39;: batch_cifar}
            return {&#39;mnist&#39;: mnist_loader, &#39;cifar&#39;: cifar_loader}
        &#34;&#34;&#34;

    @abstractmethod
    def val_dataloader(self, *args: Any, **kwargs: Any) -&gt; EvalDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for validation.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of validation dataloaders specifying validation samples.

        Examples
        -------
        # single dataloader
        def val_dataloader(self):
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.5,), (1.0,))
            ])
            dataset = MNIST(
                root=&#39;/path/to/mnist/&#39;,
                train=False,
                transform=transform,
                download=True
            )
            loader = torch.utils.data.DataLoader(
                dataset=dataset,
                batch_size=self.batch_size,
                shuffle=False
            )
            return loader

        # multiple dataloaders, return as list
        def val_dataloader(self):
            mnist = MNIST(...)
            cifar = CIFAR(...)
            mnist_loader = torch.utils.data.DataLoader(
                dataset=mnist, batch_size=self.batch_size, shuffle=False
            )
            cifar_loader = torch.utils.data.DataLoader(
                dataset=cifar, batch_size=self.batch_size, shuffle=False
            )
            # each batch will be a list of tensors: [batch_mnist, batch_cifar]
            return [mnist_loader, cifar_loader]
        &#34;&#34;&#34;

    @abstractmethod
    def test_dataloader(self, *args: Any, **kwargs: Any) -&gt; EvalDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for testing.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of test dataloaders specifying testing samples.

        Examples
        -------
        # single dataloader
        def test_dataloader(self):
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.5,), (1.0,))
            ])
            dataset = MNIST(
                root=&#39;/path/to/mnist/&#39;,
                train=False,
                transform=transform,
                download=True
            )
            loader = torch.utils.data.DataLoader(
                dataset=dataset,
                batch_size=self.batch_size,
                shuffle=False
            )
            return loader

        # multiple dataloaders, return as list
        def test_dataloader(self):
            mnist = MNIST(...)
            cifar = CIFAR(...)
            mnist_loader = torch.utils.data.DataLoader(
                dataset=mnist, batch_size=self.batch_size, shuffle=False
            )
            cifar_loader = torch.utils.data.DataLoader(
                dataset=cifar, batch_size=self.batch_size, shuffle=False
            )
            # each batch will be a list of tensors: [batch_mnist, batch_cifar]
            return [mnist_loader, cifar_loader]
        &#34;&#34;&#34;</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="radio.data.basedatamodule.BaseDataModule"><code class="flex name class">
<span>class <span class="ident">BaseDataModule</span></span>
<span>(</span><span>*args: Any, root: Union[str, pathlib.Path] = PosixPath('/home/wangl15@acct.upmchs.net/radio/dataset'), train_transforms: Optional[torchio.transforms.transform.Transform] = None, val_transforms: Optional[torchio.transforms.transform.Transform] = None, test_transforms: Optional[torchio.transforms.transform.Transform] = None, batch_size: int = 32, shuffle: bool = True, num_workers: int = 0, pin_memory: bool = True, drop_last: bool = False, num_folds: int = 2, val_split: Union[int, float] = 0.2, seed: int = 41, **kwargs: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for making data modules.</p>
<p>To create a subclass, you need to implement the following functions:</p>
<p>A BaseDataModule needs to implement 6 key methods:
&lt;<strong>init</strong>&gt;:
(Optionally) Initialize the class, first call super.<strong>init</strong>().
<prepare_data>:
Things to do on 1 GPU/TPU not on every GPU/TPU in distributed mode.
<setup>:
Things to do on every accelerator in distributed mode.
<train_dataloader>:
The training dataloader(s).
<val_dataloader>:
The validation dataloader(s).
<test_dataloader>:
The test dataloader(s).
<teardown>:
Things to do on every accelerator in distributed mode when finished.</p>
<h2 id="typical-workflow">Typical Workflow</h2>
<p>data = BaseDataModule()
data.prepare_data() # download
data.setup(stage) # process and split
data.teardown(stage) # clean-up</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>root</code></strong> :&ensp;<code>Path</code> or <code>str</code>, optional</dt>
<dd>Root directory of dataset. If None, a temporary directory will be used.
Default = <code>DATA_ROOT / 'medical_decathlon'</code>.</dd>
<dt><strong><code>train_transforms</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>A function/transform that takes in a sample and returns a
transformed version, e.g, <code>torchvision.transforms.RandomCrop</code>.</dd>
<dt><strong><code>val_transforms</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>A function/transform that takes in a sample and returns a
transformed version, e.g, <code>torchvision.transforms.RandomCrop</code>.</dd>
<dt><strong><code>test_transforms</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>A function/transform that takes in a sample and returns a
transformed version, e.g, <code>torchvision.transforms.RandomCrop</code>.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>How many samples per batch to load. Default = <code>32</code>.</dd>
<dt><strong><code>shuffle</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to shuffle the data at every epoch. Default = <code>False</code>.</dd>
<dt><strong><code>num_workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>How many subprocesses to use for data loading. <code>0</code> means that the
data will be loaded in the main process. Default: <code>0</code>.</dd>
<dt><strong><code>pin_memory</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, the data loader will copy Tensors into CUDA pinned memory
before returning them.</dd>
<dt><strong><code>drop_last</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Set to <code>True</code> to drop the last incomplete batch, if the dataset size
is not divisible by the batch size. If <code>False</code> and the size of
dataset is not divisible by the batch size, then the last batch will be
smaller. Default = <code>False</code>.</dd>
<dt><strong><code>num_folds</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of folds. Must be at least <code>2</code>. <code>2</code> corresponds to a single
train/validation split. Default = <code>2</code>.</dd>
<dt><strong><code>val_split</code></strong> :&ensp;<code>int</code> or <code>float</code>, optional</dt>
<dd>If <code>num_folds = 2</code>, then <code>val_split</code> specify how the
train_dataset should be split into train/validation datasets. If
<code>num_folds &gt; 2</code>, then it is not used. Default = <code>0.2</code>.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>When <code>shuffle</code> is True, <code>seed</code> affects the ordering of the indices,
which controls the randomness of each fold. It is also use to seed the
RNG used by RandomSampler to generate random indexes and
multiprocessing to generate <code>base_seed</code> for workers. Pass an int for
reproducible output across multiple function calls. Default = <code>41</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseDataModule(pl.LightningDataModule, metaclass=ABCMeta):
    &#34;&#34;&#34;
    Base class for making data modules.

    To create a subclass, you need to implement the following functions:

    A BaseDataModule needs to implement 6 key methods:
    &lt;__init__&gt;:
        (Optionally) Initialize the class, first call super.__init__().
    &lt;prepare_data&gt;:
        Things to do on 1 GPU/TPU not on every GPU/TPU in distributed mode.
    &lt;setup&gt;:
        Things to do on every accelerator in distributed mode.
    &lt;train_dataloader&gt;:
        The training dataloader(s).
    &lt;val_dataloader&gt;:
        The validation dataloader(s).
    &lt;test_dataloader&gt;:
        The test dataloader(s).
    &lt;teardown&gt;:
        Things to do on every accelerator in distributed mode when finished.

    Typical Workflow
    ----------------
    data = BaseDataModule()
    data.prepare_data() # download
    data.setup(stage) # process and split
    data.teardown(stage) # clean-up

    Parameters
    ----------
    root : Path or str, optional
        Root directory of dataset. If None, a temporary directory will be used.
        Default = ``DATA_ROOT / &#39;medical_decathlon&#39;``.
    train_transforms : Callable, optional
        A function/transform that takes in a sample and returns a
        transformed version, e.g, ``torchvision.transforms.RandomCrop``.
    val_transforms : Callable, optional
        A function/transform that takes in a sample and returns a
        transformed version, e.g, ``torchvision.transforms.RandomCrop``.
    test_transforms : Callable, optional
        A function/transform that takes in a sample and returns a
        transformed version, e.g, ``torchvision.transforms.RandomCrop``.
    batch_size : int, optional
        How many samples per batch to load. Default = ``32``.
    shuffle : bool, optional
        Whether to shuffle the data at every epoch. Default = ``False``.
    num_workers : int, optional
        How many subprocesses to use for data loading. ``0`` means that the
        data will be loaded in the main process. Default: ``0``.
    pin_memory : bool, optional
        If ``True``, the data loader will copy Tensors into CUDA pinned memory
        before returning them.
    drop_last : bool, optional
        Set to ``True`` to drop the last incomplete batch, if the dataset size
        is not divisible by the batch size. If ``False`` and the size of
        dataset is not divisible by the batch size, then the last batch will be
        smaller. Default = ``False``.
    num_folds : int, optional
        Number of folds. Must be at least ``2``. ``2`` corresponds to a single
        train/validation split. Default = ``2``.
    val_split: int or float, optional
        If ``num_folds = 2``, then ``val_split`` specify how the
        train_dataset should be split into train/validation datasets. If
        ``num_folds &gt; 2``, then it is not used. Default = ``0.2``.
    seed : int, optional
        When `shuffle` is True, `seed` affects the ordering of the indices,
        which controls the randomness of each fold. It is also use to seed the
        RNG used by RandomSampler to generate random indexes and
        multiprocessing to generate `base_seed` for workers. Pass an int for
        reproducible output across multiple function calls. Default = ``41``.
    &#34;&#34;&#34;
    #: Extra arguments for dataset_cls instantiation.
    EXTRA_ARGS: dict = {}
    #: Dataset class to use. E.g., torchvision.datasets.MNIST
    dataset_cls: type
    #: A tuple describing the shape of the data
    dims: Optional[Tuple[int, int, int]]
    #: Dataset name
    name: str

    def __init__(
        self,
        *args: Any,
        root: PathType = DATA_ROOT,
        train_transforms: Optional[tio.Transform] = None,
        val_transforms: Optional[tio.Transform] = None,
        test_transforms: Optional[tio.Transform] = None,
        batch_size: int = 32,
        shuffle: bool = True,
        num_workers: int = 0,
        pin_memory: bool = True,
        drop_last: bool = False,
        num_folds: int = 2,
        val_split: Union[int, float] = 0.2,
        seed: int = 41,
        **kwargs: Any,
    ) -&gt; None:

        super().__init__(*args, **kwargs)
        num_folds_msg = &#34;``num_folds`` must be an integer of at least 2.&#34;
        assert isinstance(num_folds, int) and num_folds &gt; 1, num_folds_msg
        self.is_temp_dir = bool(root is None)
        # Data folder flags to check if data is splitted already
        self.has_train_val_split: bool
        self.has_train_test_split: bool
        # Dataset Init
        self.root = Path(root).expanduser() if root else Path(
            tempfile.mkdtemp())
        self.train_transforms = train_transforms
        self.val_transforms = val_transforms
        self.test_transforms = test_transforms
        self.train_dataset: TrainDatasetType
        self.val_dataset: EvalDatasetType
        self.test_dataset: EvalDatasetType
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.num_workers = num_workers
        self.pin_memory = pin_memory
        self.drop_last = drop_last
        self.seed = seed
        self.size_train: TrainSizeType
        self.size_val: EvalSizeType
        self.size_test: EvalSizeType
        # Dataloader Init
        self.num_folds = num_folds
        self.val_cls: Type[ValidationType] = (OneFoldValidation if num_folds
                                              == 2 else KFoldValidation)
        self.validation: ValidationType
        self.has_validation = False
        self.val_split = val_split

    @abstractmethod
    def prepare_data(self, *args: Any, **kwargs: Any) -&gt; None:
        &#34;&#34;&#34;
        Data operations to be performed that need to be done only from a single
        process in distributed settings. For example, download and saving data.

        Warning
        -------
        DO NOT set state here (use ``&#39;setup&#39;`` instead) since this is NOT
        called on every device.

        Example
        -------
        def prepare_data(self, *args, **kwargs):
            # good
            download_data()
            tokenize()
            etc()

            # good
            torchvison.datasets.MNIST(self.root, train=True, download=True)
            torchvison.datasets.MNIST(self.root, train=False, download=True)

            # bad
            self.split = data_split
            self.some_state = some_other_state()
        &#34;&#34;&#34;

    @abstractmethod
    def setup(self, stage: Optional[str] = None) -&gt; None:
        &#34;&#34;&#34;
        Data operations to be performed on every GPU. This is a good hook when
        you need to build models dynamically or adjust something about them
        based on the data atrributes. For example, count number of classes,
        build vocabulary, perform train/val/test splits, apply transforms.

        Parameters
        ----------
        stage: Optional[str]
            Either ``&#39;fit``, ``&#39;validate&#39;``, or ``&#39;test&#39;``.
            If stage = None, set-up all stages. Default = None.

        Example
        -------
        def setup(self, stage):
            if stage in (None, &#34;fit&#34;):
                # train + validation set-up
                self.data_train, self.data_val = load_fit(...)
                self.dims = self.data_train[0][0].shape
                self.l1 = nn.Linear(28, self.data_train.num_classes)
            if stage in (None, &#34;test&#34;):
                # test set-up
                self.data_test = load_test(...)
                self.dims = self.data_test[0][0].shape
                self.l1 = nn.Linear(28, self.data_test.num_classes)
        &#34;&#34;&#34;

    @abstractmethod
    def teardown(self, stage: Optional[str] = None) -&gt; None:
        &#34;&#34;&#34;
        Called at the end of fit (train + validate), validate, test,
        or predict.

        Parameters
        ----------
        stage: Optional[str]
            Either ``&#39;fit``, ``&#39;validate&#39;``, or ``&#39;test&#39;``.
            If stage = None, set-up all stages. Default = None.
        &#34;&#34;&#34;

    @abstractmethod
    def train_dataloader(self, *args: Any,
                         **kwargs: Any) -&gt; TrainDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for training.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of train dataloaders specifying training samples.

        Examples
        -------
        # single dataloader
        def train_dataloader(self):
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.5,), (1.0,))
            ])
            dataset = MNIST(
                root=&#39;/path/to/mnist/&#39;,
                train=True,
                transform=transform,
                download=True
            )
            loader = torch.utils.data.DataLoader(
                dataset=dataset,
                batch_size=self.batch_size,
                shuffle=True
            )
            return loader

        # multiple dataloaders, return as list
        def train_dataloader(self):
            mnist = MNIST(...)
            cifar = CIFAR(...)
            mnist_loader = torch.utils.data.DataLoader(
                dataset=mnist, batch_size=self.batch_size, shuffle=True
            )
            cifar_loader = torch.utils.data.DataLoader(
                dataset=cifar, batch_size=self.batch_size, shuffle=True
            )
            # each batch will be a list of tensors: [batch_mnist, batch_cifar]
            return [mnist_loader, cifar_loader]

        # multiple dataloader, return as dict
        def train_dataloader(self):
            mnist = MNIST(...)
            cifar = CIFAR(...)
            mnist_loader = torch.utils.data.DataLoader(
                dataset=mnist, batch_size=self.batch_size, shuffle=True
            )
            cifar_loader = torch.utils.data.DataLoader(
                dataset=cifar, batch_size=self.batch_size, shuffle=True
            )
            # each batch will be a dict of
            tensors: {&#39;mnist&#39;: batch_mnist, &#39;cifar&#39;: batch_cifar}
            return {&#39;mnist&#39;: mnist_loader, &#39;cifar&#39;: cifar_loader}
        &#34;&#34;&#34;

    @abstractmethod
    def val_dataloader(self, *args: Any, **kwargs: Any) -&gt; EvalDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for validation.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of validation dataloaders specifying validation samples.

        Examples
        -------
        # single dataloader
        def val_dataloader(self):
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.5,), (1.0,))
            ])
            dataset = MNIST(
                root=&#39;/path/to/mnist/&#39;,
                train=False,
                transform=transform,
                download=True
            )
            loader = torch.utils.data.DataLoader(
                dataset=dataset,
                batch_size=self.batch_size,
                shuffle=False
            )
            return loader

        # multiple dataloaders, return as list
        def val_dataloader(self):
            mnist = MNIST(...)
            cifar = CIFAR(...)
            mnist_loader = torch.utils.data.DataLoader(
                dataset=mnist, batch_size=self.batch_size, shuffle=False
            )
            cifar_loader = torch.utils.data.DataLoader(
                dataset=cifar, batch_size=self.batch_size, shuffle=False
            )
            # each batch will be a list of tensors: [batch_mnist, batch_cifar]
            return [mnist_loader, cifar_loader]
        &#34;&#34;&#34;

    @abstractmethod
    def test_dataloader(self, *args: Any, **kwargs: Any) -&gt; EvalDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for testing.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of test dataloaders specifying testing samples.

        Examples
        -------
        # single dataloader
        def test_dataloader(self):
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.5,), (1.0,))
            ])
            dataset = MNIST(
                root=&#39;/path/to/mnist/&#39;,
                train=False,
                transform=transform,
                download=True
            )
            loader = torch.utils.data.DataLoader(
                dataset=dataset,
                batch_size=self.batch_size,
                shuffle=False
            )
            return loader

        # multiple dataloaders, return as list
        def test_dataloader(self):
            mnist = MNIST(...)
            cifar = CIFAR(...)
            mnist_loader = torch.utils.data.DataLoader(
                dataset=mnist, batch_size=self.batch_size, shuffle=False
            )
            cifar_loader = torch.utils.data.DataLoader(
                dataset=cifar, batch_size=self.batch_size, shuffle=False
            )
            # each batch will be a list of tensors: [batch_mnist, batch_cifar]
            return [mnist_loader, cifar_loader]
        &#34;&#34;&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pytorch_lightning.core.datamodule.LightningDataModule</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="radio.data.visiondatamodule.VisionDataModule" href="visiondatamodule.html#radio.data.visiondatamodule.VisionDataModule">VisionDataModule</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="radio.data.basedatamodule.BaseDataModule.EXTRA_ARGS"><code class="name">var <span class="ident">EXTRA_ARGS</span> : dict</code></dt>
<dd>
<div class="desc"><p>Extra arguments for dataset_cls instantiation.</p></div>
</dd>
<dt id="radio.data.basedatamodule.BaseDataModule.dataset_cls"><code class="name">var <span class="ident">dataset_cls</span> : type</code></dt>
<dd>
<div class="desc"><p>Dataset class to use. E.g., torchvision.datasets.MNIST</p></div>
</dd>
<dt id="radio.data.basedatamodule.BaseDataModule.name"><code class="name">var <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"><p>Dataset name</p></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="radio.data.basedatamodule.BaseDataModule.dims"><code class="name">var <span class="ident">dims</span> : Optional[Tuple[int, int, int]]</code></dt>
<dd>
<div class="desc"><p>A tuple describing the shape of the data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dims(self):
    &#34;&#34;&#34;A tuple describing the shape of your data. Extra functionality exposed in ``size``.

    .. deprecated:: v1.5     Will be removed in v1.7.0.
    &#34;&#34;&#34;
    rank_zero_deprecation(&#34;DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.&#34;)
    return self._dims</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="radio.data.basedatamodule.BaseDataModule.prepare_data"><code class="name flex">
<span>def <span class="ident">prepare_data</span></span>(<span>self, *args: Any, **kwargs: Any) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Data operations to be performed that need to be done only from a single
process in distributed settings. For example, download and saving data.</p>
<h2 id="warning">Warning</h2>
<p>DO NOT set state here (use <code>'setup'</code> instead) since this is NOT
called on every device.</p>
<h2 id="example">Example</h2>
<p>def prepare_data(self, <em>args, </em>*kwargs):
# good
download_data()
tokenize()
etc()</p>
<pre><code># good
torchvison.datasets.MNIST(self.root, train=True, download=True)
torchvison.datasets.MNIST(self.root, train=False, download=True)

# bad
self.split = data_split
self.some_state = some_other_state()
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def prepare_data(self, *args: Any, **kwargs: Any) -&gt; None:
    &#34;&#34;&#34;
    Data operations to be performed that need to be done only from a single
    process in distributed settings. For example, download and saving data.

    Warning
    -------
    DO NOT set state here (use ``&#39;setup&#39;`` instead) since this is NOT
    called on every device.

    Example
    -------
    def prepare_data(self, *args, **kwargs):
        # good
        download_data()
        tokenize()
        etc()

        # good
        torchvison.datasets.MNIST(self.root, train=True, download=True)
        torchvison.datasets.MNIST(self.root, train=False, download=True)

        # bad
        self.split = data_split
        self.some_state = some_other_state()
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="radio.data.basedatamodule.BaseDataModule.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self, stage: Optional[str] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Data operations to be performed on every GPU. This is a good hook when
you need to build models dynamically or adjust something about them
based on the data atrributes. For example, count number of classes,
build vocabulary, perform train/val/test splits, apply transforms.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>stage</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Either <code>'fit</code>, <code>'validate'</code>, or <code>'test'</code>.
If stage = None, set-up all stages. Default = None.</dd>
</dl>
<h2 id="example">Example</h2>
<p>def setup(self, stage):
if stage in (None, "fit"):
# train + validation set-up
self.data_train, self.data_val = load_fit(&hellip;)
self.dims = self.data_train[0][0].shape
self.l1 = nn.Linear(28, self.data_train.num_classes)
if stage in (None, "test"):
# test set-up
self.data_test = load_test(&hellip;)
self.dims = self.data_test[0][0].shape
self.l1 = nn.Linear(28, self.data_test.num_classes)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def setup(self, stage: Optional[str] = None) -&gt; None:
    &#34;&#34;&#34;
    Data operations to be performed on every GPU. This is a good hook when
    you need to build models dynamically or adjust something about them
    based on the data atrributes. For example, count number of classes,
    build vocabulary, perform train/val/test splits, apply transforms.

    Parameters
    ----------
    stage: Optional[str]
        Either ``&#39;fit``, ``&#39;validate&#39;``, or ``&#39;test&#39;``.
        If stage = None, set-up all stages. Default = None.

    Example
    -------
    def setup(self, stage):
        if stage in (None, &#34;fit&#34;):
            # train + validation set-up
            self.data_train, self.data_val = load_fit(...)
            self.dims = self.data_train[0][0].shape
            self.l1 = nn.Linear(28, self.data_train.num_classes)
        if stage in (None, &#34;test&#34;):
            # test set-up
            self.data_test = load_test(...)
            self.dims = self.data_test[0][0].shape
            self.l1 = nn.Linear(28, self.data_test.num_classes)
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="radio.data.basedatamodule.BaseDataModule.teardown"><code class="name flex">
<span>def <span class="ident">teardown</span></span>(<span>self, stage: Optional[str] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Called at the end of fit (train + validate), validate, test,
or predict.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>stage</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Either <code>'fit</code>, <code>'validate'</code>, or <code>'test'</code>.
If stage = None, set-up all stages. Default = None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def teardown(self, stage: Optional[str] = None) -&gt; None:
    &#34;&#34;&#34;
    Called at the end of fit (train + validate), validate, test,
    or predict.

    Parameters
    ----------
    stage: Optional[str]
        Either ``&#39;fit``, ``&#39;validate&#39;``, or ``&#39;test&#39;``.
        If stage = None, set-up all stages. Default = None.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="radio.data.basedatamodule.BaseDataModule.test_dataloader"><code class="name flex">
<span>def <span class="ident">test_dataloader</span></span>(<span>self, *args: Any, **kwargs: Any) ‑> Union[torch.utils.data.dataloader.DataLoader, Sequence[torch.utils.data.dataloader.DataLoader]]</span>
</code></dt>
<dd>
<div class="desc"><p>Generates one or multiple Pytorch DataLoaders for testing.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>_</code></strong> :&ensp;<code>Collection</code> of <code>DataLoader</code></dt>
<dd>Collection of test dataloaders specifying testing samples.</dd>
</dl>
<h2 id="examples">Examples</h2>
<h1 id="single-dataloader">single dataloader</h1>
<p>def test_dataloader(self):
transform = transforms.Compose([
transforms.ToTensor(),
transforms.Normalize((0.5,), (1.0,))
])
dataset = MNIST(
root='/path/to/mnist/',
train=False,
transform=transform,
download=True
)
loader = torch.utils.data.DataLoader(
dataset=dataset,
batch_size=self.batch_size,
shuffle=False
)
return loader</p>
<h1 id="multiple-dataloaders-return-as-list">multiple dataloaders, return as list</h1>
<p>def test_dataloader(self):
mnist = MNIST(&hellip;)
cifar = CIFAR(&hellip;)
mnist_loader = torch.utils.data.DataLoader(
dataset=mnist, batch_size=self.batch_size, shuffle=False
)
cifar_loader = torch.utils.data.DataLoader(
dataset=cifar, batch_size=self.batch_size, shuffle=False
)
# each batch will be a list of tensors: [batch_mnist, batch_cifar]
return [mnist_loader, cifar_loader]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def test_dataloader(self, *args: Any, **kwargs: Any) -&gt; EvalDataLoaderType:
    &#34;&#34;&#34;
    Generates one or multiple Pytorch DataLoaders for testing.

    Returns
    -------
    _ : Collection of DataLoader
        Collection of test dataloaders specifying testing samples.

    Examples
    -------
    # single dataloader
    def test_dataloader(self):
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5,), (1.0,))
        ])
        dataset = MNIST(
            root=&#39;/path/to/mnist/&#39;,
            train=False,
            transform=transform,
            download=True
        )
        loader = torch.utils.data.DataLoader(
            dataset=dataset,
            batch_size=self.batch_size,
            shuffle=False
        )
        return loader

    # multiple dataloaders, return as list
    def test_dataloader(self):
        mnist = MNIST(...)
        cifar = CIFAR(...)
        mnist_loader = torch.utils.data.DataLoader(
            dataset=mnist, batch_size=self.batch_size, shuffle=False
        )
        cifar_loader = torch.utils.data.DataLoader(
            dataset=cifar, batch_size=self.batch_size, shuffle=False
        )
        # each batch will be a list of tensors: [batch_mnist, batch_cifar]
        return [mnist_loader, cifar_loader]
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="radio.data.basedatamodule.BaseDataModule.train_dataloader"><code class="name flex">
<span>def <span class="ident">train_dataloader</span></span>(<span>self, *args: Any, **kwargs: Any) ‑> Union[torch.utils.data.dataloader.DataLoader, Sequence[torch.utils.data.dataloader.DataLoader], Sequence[Sequence[torch.utils.data.dataloader.DataLoader]], Sequence[Dict[str, torch.utils.data.dataloader.DataLoader]], Dict[str, torch.utils.data.dataloader.DataLoader], Dict[str, Dict[str, torch.utils.data.dataloader.DataLoader]], Dict[str, Sequence[torch.utils.data.dataloader.DataLoader]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Generates one or multiple Pytorch DataLoaders for training.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>_</code></strong> :&ensp;<code>Collection</code> of <code>DataLoader</code></dt>
<dd>Collection of train dataloaders specifying training samples.</dd>
</dl>
<h2 id="examples">Examples</h2>
<h1 id="single-dataloader">single dataloader</h1>
<p>def train_dataloader(self):
transform = transforms.Compose([
transforms.ToTensor(),
transforms.Normalize((0.5,), (1.0,))
])
dataset = MNIST(
root='/path/to/mnist/',
train=True,
transform=transform,
download=True
)
loader = torch.utils.data.DataLoader(
dataset=dataset,
batch_size=self.batch_size,
shuffle=True
)
return loader</p>
<h1 id="multiple-dataloaders-return-as-list">multiple dataloaders, return as list</h1>
<p>def train_dataloader(self):
mnist = MNIST(&hellip;)
cifar = CIFAR(&hellip;)
mnist_loader = torch.utils.data.DataLoader(
dataset=mnist, batch_size=self.batch_size, shuffle=True
)
cifar_loader = torch.utils.data.DataLoader(
dataset=cifar, batch_size=self.batch_size, shuffle=True
)
# each batch will be a list of tensors: [batch_mnist, batch_cifar]
return [mnist_loader, cifar_loader]</p>
<h1 id="multiple-dataloader-return-as-dict">multiple dataloader, return as dict</h1>
<p>def train_dataloader(self):
mnist = MNIST(&hellip;)
cifar = CIFAR(&hellip;)
mnist_loader = torch.utils.data.DataLoader(
dataset=mnist, batch_size=self.batch_size, shuffle=True
)
cifar_loader = torch.utils.data.DataLoader(
dataset=cifar, batch_size=self.batch_size, shuffle=True
)
# each batch will be a dict of
tensors: {'mnist': batch_mnist, 'cifar': batch_cifar}
return {'mnist': mnist_loader, 'cifar': cifar_loader}</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def train_dataloader(self, *args: Any,
                     **kwargs: Any) -&gt; TrainDataLoaderType:
    &#34;&#34;&#34;
    Generates one or multiple Pytorch DataLoaders for training.

    Returns
    -------
    _ : Collection of DataLoader
        Collection of train dataloaders specifying training samples.

    Examples
    -------
    # single dataloader
    def train_dataloader(self):
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5,), (1.0,))
        ])
        dataset = MNIST(
            root=&#39;/path/to/mnist/&#39;,
            train=True,
            transform=transform,
            download=True
        )
        loader = torch.utils.data.DataLoader(
            dataset=dataset,
            batch_size=self.batch_size,
            shuffle=True
        )
        return loader

    # multiple dataloaders, return as list
    def train_dataloader(self):
        mnist = MNIST(...)
        cifar = CIFAR(...)
        mnist_loader = torch.utils.data.DataLoader(
            dataset=mnist, batch_size=self.batch_size, shuffle=True
        )
        cifar_loader = torch.utils.data.DataLoader(
            dataset=cifar, batch_size=self.batch_size, shuffle=True
        )
        # each batch will be a list of tensors: [batch_mnist, batch_cifar]
        return [mnist_loader, cifar_loader]

    # multiple dataloader, return as dict
    def train_dataloader(self):
        mnist = MNIST(...)
        cifar = CIFAR(...)
        mnist_loader = torch.utils.data.DataLoader(
            dataset=mnist, batch_size=self.batch_size, shuffle=True
        )
        cifar_loader = torch.utils.data.DataLoader(
            dataset=cifar, batch_size=self.batch_size, shuffle=True
        )
        # each batch will be a dict of
        tensors: {&#39;mnist&#39;: batch_mnist, &#39;cifar&#39;: batch_cifar}
        return {&#39;mnist&#39;: mnist_loader, &#39;cifar&#39;: cifar_loader}
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="radio.data.basedatamodule.BaseDataModule.val_dataloader"><code class="name flex">
<span>def <span class="ident">val_dataloader</span></span>(<span>self, *args: Any, **kwargs: Any) ‑> Union[torch.utils.data.dataloader.DataLoader, Sequence[torch.utils.data.dataloader.DataLoader]]</span>
</code></dt>
<dd>
<div class="desc"><p>Generates one or multiple Pytorch DataLoaders for validation.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>_</code></strong> :&ensp;<code>Collection</code> of <code>DataLoader</code></dt>
<dd>Collection of validation dataloaders specifying validation samples.</dd>
</dl>
<h2 id="examples">Examples</h2>
<h1 id="single-dataloader">single dataloader</h1>
<p>def val_dataloader(self):
transform = transforms.Compose([
transforms.ToTensor(),
transforms.Normalize((0.5,), (1.0,))
])
dataset = MNIST(
root='/path/to/mnist/',
train=False,
transform=transform,
download=True
)
loader = torch.utils.data.DataLoader(
dataset=dataset,
batch_size=self.batch_size,
shuffle=False
)
return loader</p>
<h1 id="multiple-dataloaders-return-as-list">multiple dataloaders, return as list</h1>
<p>def val_dataloader(self):
mnist = MNIST(&hellip;)
cifar = CIFAR(&hellip;)
mnist_loader = torch.utils.data.DataLoader(
dataset=mnist, batch_size=self.batch_size, shuffle=False
)
cifar_loader = torch.utils.data.DataLoader(
dataset=cifar, batch_size=self.batch_size, shuffle=False
)
# each batch will be a list of tensors: [batch_mnist, batch_cifar]
return [mnist_loader, cifar_loader]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def val_dataloader(self, *args: Any, **kwargs: Any) -&gt; EvalDataLoaderType:
    &#34;&#34;&#34;
    Generates one or multiple Pytorch DataLoaders for validation.

    Returns
    -------
    _ : Collection of DataLoader
        Collection of validation dataloaders specifying validation samples.

    Examples
    -------
    # single dataloader
    def val_dataloader(self):
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5,), (1.0,))
        ])
        dataset = MNIST(
            root=&#39;/path/to/mnist/&#39;,
            train=False,
            transform=transform,
            download=True
        )
        loader = torch.utils.data.DataLoader(
            dataset=dataset,
            batch_size=self.batch_size,
            shuffle=False
        )
        return loader

    # multiple dataloaders, return as list
    def val_dataloader(self):
        mnist = MNIST(...)
        cifar = CIFAR(...)
        mnist_loader = torch.utils.data.DataLoader(
            dataset=mnist, batch_size=self.batch_size, shuffle=False
        )
        cifar_loader = torch.utils.data.DataLoader(
            dataset=cifar, batch_size=self.batch_size, shuffle=False
        )
        # each batch will be a list of tensors: [batch_mnist, batch_cifar]
        return [mnist_loader, cifar_loader]
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="radio.data" href="index.html">radio.data</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="radio.data.basedatamodule.BaseDataModule" href="#radio.data.basedatamodule.BaseDataModule">BaseDataModule</a></code></h4>
<ul class="two-column">
<li><code><a title="radio.data.basedatamodule.BaseDataModule.EXTRA_ARGS" href="#radio.data.basedatamodule.BaseDataModule.EXTRA_ARGS">EXTRA_ARGS</a></code></li>
<li><code><a title="radio.data.basedatamodule.BaseDataModule.dataset_cls" href="#radio.data.basedatamodule.BaseDataModule.dataset_cls">dataset_cls</a></code></li>
<li><code><a title="radio.data.basedatamodule.BaseDataModule.dims" href="#radio.data.basedatamodule.BaseDataModule.dims">dims</a></code></li>
<li><code><a title="radio.data.basedatamodule.BaseDataModule.name" href="#radio.data.basedatamodule.BaseDataModule.name">name</a></code></li>
<li><code><a title="radio.data.basedatamodule.BaseDataModule.prepare_data" href="#radio.data.basedatamodule.BaseDataModule.prepare_data">prepare_data</a></code></li>
<li><code><a title="radio.data.basedatamodule.BaseDataModule.setup" href="#radio.data.basedatamodule.BaseDataModule.setup">setup</a></code></li>
<li><code><a title="radio.data.basedatamodule.BaseDataModule.teardown" href="#radio.data.basedatamodule.BaseDataModule.teardown">teardown</a></code></li>
<li><code><a title="radio.data.basedatamodule.BaseDataModule.test_dataloader" href="#radio.data.basedatamodule.BaseDataModule.test_dataloader">test_dataloader</a></code></li>
<li><code><a title="radio.data.basedatamodule.BaseDataModule.train_dataloader" href="#radio.data.basedatamodule.BaseDataModule.train_dataloader">train_dataloader</a></code></li>
<li><code><a title="radio.data.basedatamodule.BaseDataModule.val_dataloader" href="#radio.data.basedatamodule.BaseDataModule.val_dataloader">val_dataloader</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>
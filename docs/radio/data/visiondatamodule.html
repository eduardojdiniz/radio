<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>radio.data.visiondatamodule API documentation</title>
<meta name="description" content="Based on LightningDataModule for managing data. A datamodule is a shareable,
reusable class that encapsulates all the steps needed to process data, …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>radio.data.visiondatamodule</code></h1>
</header>
<section id="section-intro">
<p>Based on LightningDataModule for managing data. A datamodule is a shareable,
reusable class that encapsulates all the steps needed to process data, i.e.,
decoupling datasets from models to allow building dataset-agnostic models. They
also allow you to share a full dataset without explaining how to download,
split, transform, and process the data.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
# coding=utf-8
&#34;&#34;&#34;
Based on LightningDataModule for managing data. A datamodule is a shareable,
reusable class that encapsulates all the steps needed to process data, i.e.,
decoupling datasets from models to allow building dataset-agnostic models. They
also allow you to share a full dataset without explaining how to download,
split, transform, and process the data.
&#34;&#34;&#34;

from abc import abstractmethod
from typing import (Any, Callable, Mapping, Optional, Sequence, Sized, List,
                    Tuple, cast)
import shutil
from torch.utils.data import DataLoader, IterableDataset
import torchio as tio
import numpy as np
from ..settings.pathutils import is_dir_or_symlink
from .dataset import DatasetType
from .validation import TrainDataLoaderType, EvalDataLoaderType
from .basedatamodule import BaseDataModule
from .datatypes import TrainSizeType, EvalSizeType

__all__ = [&#34;VisionDataModule&#34;]


class VisionDataModule(BaseDataModule):
    &#34;&#34;&#34;
    Base class For making datasets which are compatible with torchvision.

    To create a subclass, you need to implement the following functions:

    A VisionDataModule needs to implement 2 key methods + an optional __init__:
    &lt;__init__&gt;:
        (Optionally) Initialize the class, first call super.__init__().
    &lt;default_transforms&gt;:
        Default transforms to use in lieu of train_transforms, val_transforms,
        or test_transforms.
    &lt;teardown&gt;:
        Things to do on every accelerator in distributed mode when finished.

    Typical Workflow
    ----------------
    data = VisionDataModule()
    data.prepare_data() # download
    data.setup(stage) # process and split
    data.teardown(stage) # clean-up

    Parameters
    ----------
    root : Path or str, optional
        Root directory of dataset. Default = ``DATA_ROOT``.
    train_transforms : Callable, optional
        A function/transform that takes in a sample and returns a
        transformed version, e.g, ``torchvision.transforms.RandomCrop``.
    val_transforms : Callable, optional
        A function/transform that takes in a sample and returns a
        transformed version, e.g, ``torchvision.transforms.RandomCrop``.
    test_transforms : Callable, optional
        A function/transform that takes in a sample and returns a
        transformed version, e.g, ``torchvision.transforms.RandomCrop``.
    batch_size : int, optional
        How many samples per batch to load. Default = ``32``.
    shuffle : bool, optional
        Whether to shuffle the data at every epoch. Default = ``False``.
    num_workers : int, optional
        How many subprocesses to use for data loading. ``0`` means that the
        data will be loaded in the main process. Default: ``0``.
    pin_memory : bool, optional
        If ``True``, the data loader will copy Tensors into CUDA pinned memory
        before returning them.
    drop_last : bool, optional
        Set to ``True`` to drop the last incomplete batch, if the dataset size
        is not divisible by the batch size. If ``False`` and the size of
        dataset is not divisible by the batch size, then the last batch will be
        smaller. Default = ``False``.
    num_folds : int, optional
        Number of folds. Must be at least ``2``. ``2`` corresponds to a single
        train/validation split. Default = ``2``.
    val_split: int or float, optional
        If ``num_folds = 2``, then ``val_split`` specify how the
        train_dataset should be split into train/validation datasets. If
        ``num_folds &gt; 2``, then it is not used. Default = ``0.2``.
    seed : int, optional
        When `shuffle` is True, `seed` affects the ordering of the indices,
        which controls the randomness of each fold. It is also use to seed the
        RNG used by RandomSampler to generate random indexes and
        multiprocessing to generate `base_seed` for workers. Pass an int for
        reproducible output across multiple function calls. Default = ``41``.
    &#34;&#34;&#34;

    def check_if_data_split(self, stem: str = &#34;&#34;) -&gt; None:
        &#34;&#34;&#34;Check if data is splitted in train, test and val folders&#34;&#34;&#34;
        has_train_folder = is_dir_or_symlink(self.root / stem / &#34;train&#34;)
        has_test_folder = is_dir_or_symlink(self.root / stem / &#34;test&#34;)
        has_val_folder = is_dir_or_symlink(self.root / stem / &#34;val&#34;)
        self.has_train_test_split = bool(has_train_folder and has_test_folder)
        self.has_train_val_split = bool(has_train_folder and has_val_folder)

    def prepare_data(self, *args: Any, **kwargs: Any) -&gt; None:
        &#34;&#34;&#34;
        Saves files to data root dir.
        Verify data directory exists.
        Verify if test/train/val splitted.
        &#34;&#34;&#34;
        if not is_dir_or_symlink(self.root):
            raise OSError(&#39;Study data directory not found!&#39;)
        self.check_if_data_split()
        self.dataset_cls(self.root, train=True, download=True)
        self.dataset_cls(self.root, train=False, download=True)

    @staticmethod
    def get_max_shape(subjects: List[tio.Subject]) -&gt; Tuple[int, int, int]:
        &#34;&#34;&#34;
        Get max height, width, and depth accross all subjects.

        Parameters
        ----------
        subjects : List[tio.Subject]
            List of TorchIO Subject objects.

        Returns
        -------
        shapes_tuple : Tuple[int, int, int]
            Max height, width and depth across all subjects.
        &#34;&#34;&#34;
        dataset = tio.SubjectsDataset(subjects)
        shapes = np.array([
            image.spatial_shape for subject in dataset.dry_iter()
            for image in subject.get_images()
        ])

        shapes_tuple = tuple(map(int, shapes.max(axis=0).tolist()))
        return cast(Tuple[int, int, int], shapes_tuple)

    def setup(self, stage: Optional[str] = None) -&gt; None:
        &#34;&#34;&#34;
        Creates train, validation and test collection of samplers.

        Parameters
        ----------
        stage: Optional[str]
            Either ``&#39;fit``, ``&#39;validate&#39;``, or ``&#39;test&#39;``.
            If stage = None, set-up all stages. Default = None.
        &#34;&#34;&#34;
        if stage in (None, &#34;fit&#34;):
            train_transforms = self.default_transforms(
                stage=&#34;fit&#34;
            ) if self.train_transforms is None else self.train_transforms

            val_transforms = self.default_transforms(
                stage=&#34;fit&#34;
            ) if self.val_transforms is None else self.val_transforms

            train_dataset = self.dataset_cls(
                self.root,
                train=True,
                transform=train_transforms,
                **self.EXTRA_ARGS,
            )

            val_dataset = self.dataset_cls(self.root,
                                           train=True,
                                           transform=val_transforms,
                                           **self.EXTRA_ARGS)

            self.validation = self.val_cls(train_dataset=train_dataset,
                                           val_dataset=val_dataset,
                                           batch_size=self.batch_size,
                                           shuffle=self.shuffle,
                                           num_workers=self.num_workers,
                                           pin_memory=self.pin_memory,
                                           drop_last=self.drop_last,
                                           num_folds=self.num_folds,
                                           seed=self.seed)

            self.validation.setup(self.val_split)
            self.has_validation = True

            self.train_dataset = train_dataset
            self.size_train = self.size_train_dataset(
                self.validation.train_samplers)
            self.val_dataset = val_dataset
            self.size_val = self.size_eval_dataset(
                self.validation.val_samplers)

        if stage in (None, &#34;test&#34;):
            test_transforms = self.default_transforms(
                stage=&#34;test&#34;
            ) if self.test_transforms is None else self.test_transforms
            self.test_dataset = self.dataset_cls(self.root,
                                                 train=False,
                                                 transform=test_transforms,
                                                 **self.EXTRA_ARGS)
            self.size_test = self.size_eval_dataset(self.test_dataset)

    @abstractmethod
    def default_transforms(self, stage: Optional[str] = None) -&gt; Callable:
        &#34;&#34;&#34;
        Default transforms and augmentations for the dataset.

        Parameters
        ----------
        stage: Optional[str]
            Either ``&#39;fit``, ``&#39;validate&#39;``, or ``&#39;test&#39;``.
            If stage = None, set-up all stages. Default = None.

        Returns
        -------
        _: Callable
            All preprocessing steps (and if ``&#39;fit&#39;``, augmentation steps too)
            that should be applied to the images.
        &#34;&#34;&#34;

    @staticmethod
    def size_train_dataset(train_dataset: Sized) -&gt; TrainSizeType:
        &#34;&#34;&#34;
        Compute the size of the train datasets.

        Parameters
        ----------
        train_dataset: TrainDatasetType
            Collection of train datasets.

        Returns
        -------
        _ : TrainSizeType
            Collection of train datasets&#39; sizes.
        &#34;&#34;&#34;

        def _handle_is_mapping(dataset):
            mapping = {}
            for key, dset in dataset.items():
                if isinstance(dset, Mapping):
                    mapping[key] = _handle_is_mapping(dset)
                if isinstance(dset, Sequence):
                    mapping[key] = _handle_is_sequence(dset)
                mapping[key] = len(dset)
            return mapping

        def _handle_is_sequence(dataset):
            sequence = []
            for dset in dataset:
                if isinstance(dset, Mapping):
                    sequence.append(_handle_is_mapping(dset))
                if isinstance(dset, Sequence):
                    sequence.append(_handle_is_sequence(dset))
                sequence.append(len(dset))
            return sequence

        if isinstance(train_dataset, Mapping):
            return _handle_is_mapping(train_dataset)
        if isinstance(train_dataset, Sequence):
            if len(train_dataset) == 1:
                return VisionDataModule.size_train_dataset(train_dataset[0])
            return _handle_is_sequence(train_dataset)
        return len(train_dataset)

    @staticmethod
    def size_eval_dataset(eval_dataset: Sized) -&gt; EvalSizeType:
        &#34;&#34;&#34;
        Compute the size of the test or validation datasets.

        Parameters
        ----------
        eval_dataset: EvalDatasetType
            Collection of test or validation datasets.

        Returns
        -------
        _ : EvalSizeType
            Collection of test or validation datasets&#39; sizes.
        &#34;&#34;&#34;
        if isinstance(eval_dataset, Sequence):
            if len(eval_dataset) == 1:
                return len(eval_dataset[0])
            return [len(ds) for ds in eval_dataset]
        return len(eval_dataset)

    def dataloader(
        self,
        dataset: DatasetType,
        batch_size: Optional[int] = None,
        shuffle: Optional[bool] = None,
        num_workers: Optional[int] = None,
        pin_memory: Optional[bool] = None,
        drop_last: Optional[bool] = None,
    ) -&gt; DataLoader:
        &#34;&#34;&#34;
        Instantiate a DataLoader.

        Parameters
        ----------
        batch_size : int, optional
            How many samples per batch to load. Default = ``32``.
        shuffle : bool, optional
            Whether to shuffle the data at every epoch. Default = ``False``.
        num_workers : int, optional
            How many subprocesses to use for data loading. ``0`` means that the
            data will be loaded in the main process. Default: ``0``.
        pin_memory : bool, optional
            If ``True``, the data loader will copy Tensors into CUDA pinned
            memory before returning them.
        drop_last : bool, optional
            Set to ``True`` to drop the last incomplete batch, if the dataset
            size is not divisible by the batch size. If ``False`` and the size
            of dataset is not divisible by the batch size, then the last batch
            will be smaller. Default = ``False``.

        Returns
        -------
        _ : DataLoader
        &#34;&#34;&#34;
        shuffle = shuffle if shuffle else self.shuffle
        shuffle &amp;= not isinstance(dataset, IterableDataset)
        return DataLoader(
            dataset=dataset,
            batch_size=batch_size if batch_size else self.batch_size,
            shuffle=shuffle,
            num_workers=num_workers if num_workers else self.num_workers,
            pin_memory=pin_memory if pin_memory else self.pin_memory,
            drop_last=drop_last if drop_last else self.drop_last,
        )

    def train_dataloader(self, *args: Any,
                         **kwargs: Any) -&gt; TrainDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for train.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of train dataloaders specifying training samples.
        &#34;&#34;&#34;
        loader_kwargs = {}
        loader_kwargs[&#34;batch_size&#34;] = kwargs.get(&#34;batch_size&#34;, None)
        loader_kwargs[&#34;shuffle&#34;] = kwargs.get(&#34;shuffle&#34;, None)
        loader_kwargs[&#34;num_workers&#34;] = kwargs.get(&#34;num_workers&#34;, None)
        loader_kwargs[&#34;pin_memory&#34;] = kwargs.get(&#34;pin_memory&#34;, None)
        loader_kwargs[&#34;drop_last&#34;] = kwargs.get(&#34;drop_last&#34;, None)

        def _handle_is_mapping(dataset):
            mapping = {}
            for key, dset in dataset.items():
                if isinstance(dset, Mapping):
                    mapping[key] = _handle_is_mapping(dset)
                if isinstance(dset, Sequence):
                    mapping[key] = _handle_is_sequence(dset)
                mapping[key] = self.dataloader(dset, **loader_kwargs)
            return mapping

        def _handle_is_sequence(dataset):
            sequence = []
            for dset in dataset:
                if isinstance(dset, Mapping):
                    sequence.append(_handle_is_mapping(dset))
                if isinstance(dset, Sequence):
                    sequence.append(_handle_is_sequence(dset))
                sequence.append(self.dataloader(dset, **loader_kwargs))
            return sequence

        if self.has_validation:
            return self.validation.train_dataloader()

        if isinstance(self.train_dataset, Mapping):
            return _handle_is_mapping(self.train_dataset)
        if isinstance(self.train_dataset, Sequence):
            if len(self.train_dataset) == 1:
                if isinstance(self.train_dataset[0], Mapping):
                    return _handle_is_mapping(self.train_dataset[0])
                if isinstance(self.train_dataset[0], Sequence):
                    if len(self.train_dataset[0]) == 1:
                        return self.dataloader(self.train_dataset[0][0],
                                               **loader_kwargs)
                    return _handle_is_sequence(self.train_dataset[0])
                return self.dataloader(self.train_dataset[0], **loader_kwargs)
            return _handle_is_sequence(self.train_dataset)
        return self.dataloader(self.train_dataset, **loader_kwargs)

    def val_dataloader(self, *args: Any, **kwargs: Any) -&gt; EvalDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for validation.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of validation dataloaders specifying validation samples.
        &#34;&#34;&#34;
        if self.has_validation:
            return self.validation.val_dataloader()

        loader_kwargs = {}
        loader_kwargs[&#34;batch_size&#34;] = kwargs.get(&#34;batch_size&#34;, None)
        loader_kwargs[&#34;shuffle&#34;] = kwargs.get(&#34;shuffle&#34;, None)
        loader_kwargs[&#34;num_workers&#34;] = kwargs.get(&#34;num_workers&#34;, None)
        loader_kwargs[&#34;pin_memory&#34;] = kwargs.get(&#34;pin_memory&#34;, None)
        loader_kwargs[&#34;drop_last&#34;] = kwargs.get(&#34;drop_last&#34;, None)

        if isinstance(self.val_dataset, Sequence):
            if len(self.val_dataset) == 1:
                return self.dataloader(self.val_dataset[0], **loader_kwargs)
            return [
                self.dataloader(ds, **loader_kwargs) for ds in self.val_dataset
            ]
        return self.dataloader(self.val_dataset, **loader_kwargs)

    def test_dataloader(self, *args: Any, **kwargs: Any) -&gt; EvalDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for test.

        Returns
        -------
        _ : Collection of DataLoaders
            Collection of test dataloaders specifying test samples.
        &#34;&#34;&#34;
        loader_kwargs = {}
        loader_kwargs[&#34;batch_size&#34;] = kwargs.get(&#34;batch_size&#34;, None)
        loader_kwargs[&#34;shuffle&#34;] = kwargs.get(&#34;shuffle&#34;, None)
        loader_kwargs[&#34;num_workers&#34;] = kwargs.get(&#34;num_workers&#34;, None)
        loader_kwargs[&#34;pin_memory&#34;] = kwargs.get(&#34;pin_memory&#34;, None)
        loader_kwargs[&#34;drop_last&#34;] = kwargs.get(&#34;drop_last&#34;, None)

        if isinstance(self.test_dataset, Sequence):
            if len(self.test_dataset) == 1:
                return self.dataloader(self.test_dataset[0], **loader_kwargs)
            return [
                self.dataloader(ds, **loader_kwargs)
                for ds in self.test_dataset
            ]
        return self.dataloader(self.test_dataset, **loader_kwargs)

    def predict_dataloader(self, *args: Any,
                           **kwargs: Any) -&gt; EvalDataLoaderType:
        pass

    def teardown(self, stage: Optional[str] = None) -&gt; None:
        &#34;&#34;&#34;
        Called at the end of fit (train + validate), validate, test,
        or predict. Remove root directory if a temporary was used.

        Parameters
        ----------
        stage: Optional[str]
            Either ``&#39;fit``, ``&#39;validate&#39;``, or ``&#39;test&#39;``.
            If stage = None, set-up all stages. Default = None.
        &#34;&#34;&#34;
        if self.is_temp_dir:
            shutil.rmtree(self.root)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="radio.data.visiondatamodule.VisionDataModule"><code class="flex name class">
<span>class <span class="ident">VisionDataModule</span></span>
<span>(</span><span>*args: Any, root: Union[str, pathlib.Path] = PosixPath('/home/wangl15@acct.upmchs.net/radio/dataset'), train_transforms: Optional[torchio.transforms.transform.Transform] = None, val_transforms: Optional[torchio.transforms.transform.Transform] = None, test_transforms: Optional[torchio.transforms.transform.Transform] = None, batch_size: int = 32, shuffle: bool = True, num_workers: int = 0, pin_memory: bool = True, drop_last: bool = False, num_folds: int = 2, val_split: Union[int, float] = 0.2, seed: int = 41, **kwargs: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class For making datasets which are compatible with torchvision.</p>
<p>To create a subclass, you need to implement the following functions:</p>
<p>A VisionDataModule needs to implement 2 key methods + an optional <strong>init</strong>:
&lt;<strong>init</strong>&gt;:
(Optionally) Initialize the class, first call super.<strong>init</strong>().
<default_transforms>:
Default transforms to use in lieu of train_transforms, val_transforms,
or test_transforms.
<teardown>:
Things to do on every accelerator in distributed mode when finished.</p>
<h2 id="typical-workflow">Typical Workflow</h2>
<p>data = VisionDataModule()
data.prepare_data() # download
data.setup(stage) # process and split
data.teardown(stage) # clean-up</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>root</code></strong> :&ensp;<code>Path</code> or <code>str</code>, optional</dt>
<dd>Root directory of dataset. Default = <code>DATA_ROOT</code>.</dd>
<dt><strong><code>train_transforms</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>A function/transform that takes in a sample and returns a
transformed version, e.g, <code>torchvision.transforms.RandomCrop</code>.</dd>
<dt><strong><code>val_transforms</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>A function/transform that takes in a sample and returns a
transformed version, e.g, <code>torchvision.transforms.RandomCrop</code>.</dd>
<dt><strong><code>test_transforms</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>A function/transform that takes in a sample and returns a
transformed version, e.g, <code>torchvision.transforms.RandomCrop</code>.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>How many samples per batch to load. Default = <code>32</code>.</dd>
<dt><strong><code>shuffle</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to shuffle the data at every epoch. Default = <code>False</code>.</dd>
<dt><strong><code>num_workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>How many subprocesses to use for data loading. <code>0</code> means that the
data will be loaded in the main process. Default: <code>0</code>.</dd>
<dt><strong><code>pin_memory</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, the data loader will copy Tensors into CUDA pinned memory
before returning them.</dd>
<dt><strong><code>drop_last</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Set to <code>True</code> to drop the last incomplete batch, if the dataset size
is not divisible by the batch size. If <code>False</code> and the size of
dataset is not divisible by the batch size, then the last batch will be
smaller. Default = <code>False</code>.</dd>
<dt><strong><code>num_folds</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of folds. Must be at least <code>2</code>. <code>2</code> corresponds to a single
train/validation split. Default = <code>2</code>.</dd>
<dt><strong><code>val_split</code></strong> :&ensp;<code>int</code> or <code>float</code>, optional</dt>
<dd>If <code>num_folds = 2</code>, then <code>val_split</code> specify how the
train_dataset should be split into train/validation datasets. If
<code>num_folds &gt; 2</code>, then it is not used. Default = <code>0.2</code>.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>When <code>shuffle</code> is True, <code>seed</code> affects the ordering of the indices,
which controls the randomness of each fold. It is also use to seed the
RNG used by RandomSampler to generate random indexes and
multiprocessing to generate <code>base_seed</code> for workers. Pass an int for
reproducible output across multiple function calls. Default = <code>41</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VisionDataModule(BaseDataModule):
    &#34;&#34;&#34;
    Base class For making datasets which are compatible with torchvision.

    To create a subclass, you need to implement the following functions:

    A VisionDataModule needs to implement 2 key methods + an optional __init__:
    &lt;__init__&gt;:
        (Optionally) Initialize the class, first call super.__init__().
    &lt;default_transforms&gt;:
        Default transforms to use in lieu of train_transforms, val_transforms,
        or test_transforms.
    &lt;teardown&gt;:
        Things to do on every accelerator in distributed mode when finished.

    Typical Workflow
    ----------------
    data = VisionDataModule()
    data.prepare_data() # download
    data.setup(stage) # process and split
    data.teardown(stage) # clean-up

    Parameters
    ----------
    root : Path or str, optional
        Root directory of dataset. Default = ``DATA_ROOT``.
    train_transforms : Callable, optional
        A function/transform that takes in a sample and returns a
        transformed version, e.g, ``torchvision.transforms.RandomCrop``.
    val_transforms : Callable, optional
        A function/transform that takes in a sample and returns a
        transformed version, e.g, ``torchvision.transforms.RandomCrop``.
    test_transforms : Callable, optional
        A function/transform that takes in a sample and returns a
        transformed version, e.g, ``torchvision.transforms.RandomCrop``.
    batch_size : int, optional
        How many samples per batch to load. Default = ``32``.
    shuffle : bool, optional
        Whether to shuffle the data at every epoch. Default = ``False``.
    num_workers : int, optional
        How many subprocesses to use for data loading. ``0`` means that the
        data will be loaded in the main process. Default: ``0``.
    pin_memory : bool, optional
        If ``True``, the data loader will copy Tensors into CUDA pinned memory
        before returning them.
    drop_last : bool, optional
        Set to ``True`` to drop the last incomplete batch, if the dataset size
        is not divisible by the batch size. If ``False`` and the size of
        dataset is not divisible by the batch size, then the last batch will be
        smaller. Default = ``False``.
    num_folds : int, optional
        Number of folds. Must be at least ``2``. ``2`` corresponds to a single
        train/validation split. Default = ``2``.
    val_split: int or float, optional
        If ``num_folds = 2``, then ``val_split`` specify how the
        train_dataset should be split into train/validation datasets. If
        ``num_folds &gt; 2``, then it is not used. Default = ``0.2``.
    seed : int, optional
        When `shuffle` is True, `seed` affects the ordering of the indices,
        which controls the randomness of each fold. It is also use to seed the
        RNG used by RandomSampler to generate random indexes and
        multiprocessing to generate `base_seed` for workers. Pass an int for
        reproducible output across multiple function calls. Default = ``41``.
    &#34;&#34;&#34;

    def check_if_data_split(self, stem: str = &#34;&#34;) -&gt; None:
        &#34;&#34;&#34;Check if data is splitted in train, test and val folders&#34;&#34;&#34;
        has_train_folder = is_dir_or_symlink(self.root / stem / &#34;train&#34;)
        has_test_folder = is_dir_or_symlink(self.root / stem / &#34;test&#34;)
        has_val_folder = is_dir_or_symlink(self.root / stem / &#34;val&#34;)
        self.has_train_test_split = bool(has_train_folder and has_test_folder)
        self.has_train_val_split = bool(has_train_folder and has_val_folder)

    def prepare_data(self, *args: Any, **kwargs: Any) -&gt; None:
        &#34;&#34;&#34;
        Saves files to data root dir.
        Verify data directory exists.
        Verify if test/train/val splitted.
        &#34;&#34;&#34;
        if not is_dir_or_symlink(self.root):
            raise OSError(&#39;Study data directory not found!&#39;)
        self.check_if_data_split()
        self.dataset_cls(self.root, train=True, download=True)
        self.dataset_cls(self.root, train=False, download=True)

    @staticmethod
    def get_max_shape(subjects: List[tio.Subject]) -&gt; Tuple[int, int, int]:
        &#34;&#34;&#34;
        Get max height, width, and depth accross all subjects.

        Parameters
        ----------
        subjects : List[tio.Subject]
            List of TorchIO Subject objects.

        Returns
        -------
        shapes_tuple : Tuple[int, int, int]
            Max height, width and depth across all subjects.
        &#34;&#34;&#34;
        dataset = tio.SubjectsDataset(subjects)
        shapes = np.array([
            image.spatial_shape for subject in dataset.dry_iter()
            for image in subject.get_images()
        ])

        shapes_tuple = tuple(map(int, shapes.max(axis=0).tolist()))
        return cast(Tuple[int, int, int], shapes_tuple)

    def setup(self, stage: Optional[str] = None) -&gt; None:
        &#34;&#34;&#34;
        Creates train, validation and test collection of samplers.

        Parameters
        ----------
        stage: Optional[str]
            Either ``&#39;fit``, ``&#39;validate&#39;``, or ``&#39;test&#39;``.
            If stage = None, set-up all stages. Default = None.
        &#34;&#34;&#34;
        if stage in (None, &#34;fit&#34;):
            train_transforms = self.default_transforms(
                stage=&#34;fit&#34;
            ) if self.train_transforms is None else self.train_transforms

            val_transforms = self.default_transforms(
                stage=&#34;fit&#34;
            ) if self.val_transforms is None else self.val_transforms

            train_dataset = self.dataset_cls(
                self.root,
                train=True,
                transform=train_transforms,
                **self.EXTRA_ARGS,
            )

            val_dataset = self.dataset_cls(self.root,
                                           train=True,
                                           transform=val_transforms,
                                           **self.EXTRA_ARGS)

            self.validation = self.val_cls(train_dataset=train_dataset,
                                           val_dataset=val_dataset,
                                           batch_size=self.batch_size,
                                           shuffle=self.shuffle,
                                           num_workers=self.num_workers,
                                           pin_memory=self.pin_memory,
                                           drop_last=self.drop_last,
                                           num_folds=self.num_folds,
                                           seed=self.seed)

            self.validation.setup(self.val_split)
            self.has_validation = True

            self.train_dataset = train_dataset
            self.size_train = self.size_train_dataset(
                self.validation.train_samplers)
            self.val_dataset = val_dataset
            self.size_val = self.size_eval_dataset(
                self.validation.val_samplers)

        if stage in (None, &#34;test&#34;):
            test_transforms = self.default_transforms(
                stage=&#34;test&#34;
            ) if self.test_transforms is None else self.test_transforms
            self.test_dataset = self.dataset_cls(self.root,
                                                 train=False,
                                                 transform=test_transforms,
                                                 **self.EXTRA_ARGS)
            self.size_test = self.size_eval_dataset(self.test_dataset)

    @abstractmethod
    def default_transforms(self, stage: Optional[str] = None) -&gt; Callable:
        &#34;&#34;&#34;
        Default transforms and augmentations for the dataset.

        Parameters
        ----------
        stage: Optional[str]
            Either ``&#39;fit``, ``&#39;validate&#39;``, or ``&#39;test&#39;``.
            If stage = None, set-up all stages. Default = None.

        Returns
        -------
        _: Callable
            All preprocessing steps (and if ``&#39;fit&#39;``, augmentation steps too)
            that should be applied to the images.
        &#34;&#34;&#34;

    @staticmethod
    def size_train_dataset(train_dataset: Sized) -&gt; TrainSizeType:
        &#34;&#34;&#34;
        Compute the size of the train datasets.

        Parameters
        ----------
        train_dataset: TrainDatasetType
            Collection of train datasets.

        Returns
        -------
        _ : TrainSizeType
            Collection of train datasets&#39; sizes.
        &#34;&#34;&#34;

        def _handle_is_mapping(dataset):
            mapping = {}
            for key, dset in dataset.items():
                if isinstance(dset, Mapping):
                    mapping[key] = _handle_is_mapping(dset)
                if isinstance(dset, Sequence):
                    mapping[key] = _handle_is_sequence(dset)
                mapping[key] = len(dset)
            return mapping

        def _handle_is_sequence(dataset):
            sequence = []
            for dset in dataset:
                if isinstance(dset, Mapping):
                    sequence.append(_handle_is_mapping(dset))
                if isinstance(dset, Sequence):
                    sequence.append(_handle_is_sequence(dset))
                sequence.append(len(dset))
            return sequence

        if isinstance(train_dataset, Mapping):
            return _handle_is_mapping(train_dataset)
        if isinstance(train_dataset, Sequence):
            if len(train_dataset) == 1:
                return VisionDataModule.size_train_dataset(train_dataset[0])
            return _handle_is_sequence(train_dataset)
        return len(train_dataset)

    @staticmethod
    def size_eval_dataset(eval_dataset: Sized) -&gt; EvalSizeType:
        &#34;&#34;&#34;
        Compute the size of the test or validation datasets.

        Parameters
        ----------
        eval_dataset: EvalDatasetType
            Collection of test or validation datasets.

        Returns
        -------
        _ : EvalSizeType
            Collection of test or validation datasets&#39; sizes.
        &#34;&#34;&#34;
        if isinstance(eval_dataset, Sequence):
            if len(eval_dataset) == 1:
                return len(eval_dataset[0])
            return [len(ds) for ds in eval_dataset]
        return len(eval_dataset)

    def dataloader(
        self,
        dataset: DatasetType,
        batch_size: Optional[int] = None,
        shuffle: Optional[bool] = None,
        num_workers: Optional[int] = None,
        pin_memory: Optional[bool] = None,
        drop_last: Optional[bool] = None,
    ) -&gt; DataLoader:
        &#34;&#34;&#34;
        Instantiate a DataLoader.

        Parameters
        ----------
        batch_size : int, optional
            How many samples per batch to load. Default = ``32``.
        shuffle : bool, optional
            Whether to shuffle the data at every epoch. Default = ``False``.
        num_workers : int, optional
            How many subprocesses to use for data loading. ``0`` means that the
            data will be loaded in the main process. Default: ``0``.
        pin_memory : bool, optional
            If ``True``, the data loader will copy Tensors into CUDA pinned
            memory before returning them.
        drop_last : bool, optional
            Set to ``True`` to drop the last incomplete batch, if the dataset
            size is not divisible by the batch size. If ``False`` and the size
            of dataset is not divisible by the batch size, then the last batch
            will be smaller. Default = ``False``.

        Returns
        -------
        _ : DataLoader
        &#34;&#34;&#34;
        shuffle = shuffle if shuffle else self.shuffle
        shuffle &amp;= not isinstance(dataset, IterableDataset)
        return DataLoader(
            dataset=dataset,
            batch_size=batch_size if batch_size else self.batch_size,
            shuffle=shuffle,
            num_workers=num_workers if num_workers else self.num_workers,
            pin_memory=pin_memory if pin_memory else self.pin_memory,
            drop_last=drop_last if drop_last else self.drop_last,
        )

    def train_dataloader(self, *args: Any,
                         **kwargs: Any) -&gt; TrainDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for train.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of train dataloaders specifying training samples.
        &#34;&#34;&#34;
        loader_kwargs = {}
        loader_kwargs[&#34;batch_size&#34;] = kwargs.get(&#34;batch_size&#34;, None)
        loader_kwargs[&#34;shuffle&#34;] = kwargs.get(&#34;shuffle&#34;, None)
        loader_kwargs[&#34;num_workers&#34;] = kwargs.get(&#34;num_workers&#34;, None)
        loader_kwargs[&#34;pin_memory&#34;] = kwargs.get(&#34;pin_memory&#34;, None)
        loader_kwargs[&#34;drop_last&#34;] = kwargs.get(&#34;drop_last&#34;, None)

        def _handle_is_mapping(dataset):
            mapping = {}
            for key, dset in dataset.items():
                if isinstance(dset, Mapping):
                    mapping[key] = _handle_is_mapping(dset)
                if isinstance(dset, Sequence):
                    mapping[key] = _handle_is_sequence(dset)
                mapping[key] = self.dataloader(dset, **loader_kwargs)
            return mapping

        def _handle_is_sequence(dataset):
            sequence = []
            for dset in dataset:
                if isinstance(dset, Mapping):
                    sequence.append(_handle_is_mapping(dset))
                if isinstance(dset, Sequence):
                    sequence.append(_handle_is_sequence(dset))
                sequence.append(self.dataloader(dset, **loader_kwargs))
            return sequence

        if self.has_validation:
            return self.validation.train_dataloader()

        if isinstance(self.train_dataset, Mapping):
            return _handle_is_mapping(self.train_dataset)
        if isinstance(self.train_dataset, Sequence):
            if len(self.train_dataset) == 1:
                if isinstance(self.train_dataset[0], Mapping):
                    return _handle_is_mapping(self.train_dataset[0])
                if isinstance(self.train_dataset[0], Sequence):
                    if len(self.train_dataset[0]) == 1:
                        return self.dataloader(self.train_dataset[0][0],
                                               **loader_kwargs)
                    return _handle_is_sequence(self.train_dataset[0])
                return self.dataloader(self.train_dataset[0], **loader_kwargs)
            return _handle_is_sequence(self.train_dataset)
        return self.dataloader(self.train_dataset, **loader_kwargs)

    def val_dataloader(self, *args: Any, **kwargs: Any) -&gt; EvalDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for validation.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of validation dataloaders specifying validation samples.
        &#34;&#34;&#34;
        if self.has_validation:
            return self.validation.val_dataloader()

        loader_kwargs = {}
        loader_kwargs[&#34;batch_size&#34;] = kwargs.get(&#34;batch_size&#34;, None)
        loader_kwargs[&#34;shuffle&#34;] = kwargs.get(&#34;shuffle&#34;, None)
        loader_kwargs[&#34;num_workers&#34;] = kwargs.get(&#34;num_workers&#34;, None)
        loader_kwargs[&#34;pin_memory&#34;] = kwargs.get(&#34;pin_memory&#34;, None)
        loader_kwargs[&#34;drop_last&#34;] = kwargs.get(&#34;drop_last&#34;, None)

        if isinstance(self.val_dataset, Sequence):
            if len(self.val_dataset) == 1:
                return self.dataloader(self.val_dataset[0], **loader_kwargs)
            return [
                self.dataloader(ds, **loader_kwargs) for ds in self.val_dataset
            ]
        return self.dataloader(self.val_dataset, **loader_kwargs)

    def test_dataloader(self, *args: Any, **kwargs: Any) -&gt; EvalDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for test.

        Returns
        -------
        _ : Collection of DataLoaders
            Collection of test dataloaders specifying test samples.
        &#34;&#34;&#34;
        loader_kwargs = {}
        loader_kwargs[&#34;batch_size&#34;] = kwargs.get(&#34;batch_size&#34;, None)
        loader_kwargs[&#34;shuffle&#34;] = kwargs.get(&#34;shuffle&#34;, None)
        loader_kwargs[&#34;num_workers&#34;] = kwargs.get(&#34;num_workers&#34;, None)
        loader_kwargs[&#34;pin_memory&#34;] = kwargs.get(&#34;pin_memory&#34;, None)
        loader_kwargs[&#34;drop_last&#34;] = kwargs.get(&#34;drop_last&#34;, None)

        if isinstance(self.test_dataset, Sequence):
            if len(self.test_dataset) == 1:
                return self.dataloader(self.test_dataset[0], **loader_kwargs)
            return [
                self.dataloader(ds, **loader_kwargs)
                for ds in self.test_dataset
            ]
        return self.dataloader(self.test_dataset, **loader_kwargs)

    def predict_dataloader(self, *args: Any,
                           **kwargs: Any) -&gt; EvalDataLoaderType:
        pass

    def teardown(self, stage: Optional[str] = None) -&gt; None:
        &#34;&#34;&#34;
        Called at the end of fit (train + validate), validate, test,
        or predict. Remove root directory if a temporary was used.

        Parameters
        ----------
        stage: Optional[str]
            Either ``&#39;fit``, ``&#39;validate&#39;``, or ``&#39;test&#39;``.
            If stage = None, set-up all stages. Default = None.
        &#34;&#34;&#34;
        if self.is_temp_dir:
            shutil.rmtree(self.root)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="radio.data.basedatamodule.BaseDataModule" href="basedatamodule.html#radio.data.basedatamodule.BaseDataModule">BaseDataModule</a></li>
<li>pytorch_lightning.core.datamodule.LightningDataModule</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule" href="datamodules/brain_aging_prediction.html#radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule">BrainAgingPredictionDataModule</a></li>
<li><a title="radio.data.datamodules.klu_apc2.KLUAPC2DataModule" href="datamodules/klu_apc2.html#radio.data.datamodules.klu_apc2.KLUAPC2DataModule">KLUAPC2DataModule</a></li>
<li><a title="radio.data.datamodules.medical_decathlon.MedicalDecathlonDataModule" href="datamodules/medical_decathlon.html#radio.data.datamodules.medical_decathlon.MedicalDecathlonDataModule">MedicalDecathlonDataModule</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="radio.data.visiondatamodule.VisionDataModule.get_max_shape"><code class="name flex">
<span>def <span class="ident">get_max_shape</span></span>(<span>subjects: List[torchio.data.subject.Subject]) ‑> Tuple[int, int, int]</span>
</code></dt>
<dd>
<div class="desc"><p>Get max height, width, and depth accross all subjects.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>subjects</code></strong> :&ensp;<code>List[tio.Subject]</code></dt>
<dd>List of TorchIO Subject objects.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>shapes_tuple</code></strong> :&ensp;<code>Tuple[int, int, int]</code></dt>
<dd>Max height, width and depth across all subjects.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_max_shape(subjects: List[tio.Subject]) -&gt; Tuple[int, int, int]:
    &#34;&#34;&#34;
    Get max height, width, and depth accross all subjects.

    Parameters
    ----------
    subjects : List[tio.Subject]
        List of TorchIO Subject objects.

    Returns
    -------
    shapes_tuple : Tuple[int, int, int]
        Max height, width and depth across all subjects.
    &#34;&#34;&#34;
    dataset = tio.SubjectsDataset(subjects)
    shapes = np.array([
        image.spatial_shape for subject in dataset.dry_iter()
        for image in subject.get_images()
    ])

    shapes_tuple = tuple(map(int, shapes.max(axis=0).tolist()))
    return cast(Tuple[int, int, int], shapes_tuple)</code></pre>
</details>
</dd>
<dt id="radio.data.visiondatamodule.VisionDataModule.size_eval_dataset"><code class="name flex">
<span>def <span class="ident">size_eval_dataset</span></span>(<span>eval_dataset: Sized) ‑> Union[int, Sequence[int]]</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the size of the test or validation datasets.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>eval_dataset</code></strong> :&ensp;<code>EvalDatasetType</code></dt>
<dd>Collection of test or validation datasets.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>_</code></strong> :&ensp;<code>EvalSizeType</code></dt>
<dd>Collection of test or validation datasets' sizes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def size_eval_dataset(eval_dataset: Sized) -&gt; EvalSizeType:
    &#34;&#34;&#34;
    Compute the size of the test or validation datasets.

    Parameters
    ----------
    eval_dataset: EvalDatasetType
        Collection of test or validation datasets.

    Returns
    -------
    _ : EvalSizeType
        Collection of test or validation datasets&#39; sizes.
    &#34;&#34;&#34;
    if isinstance(eval_dataset, Sequence):
        if len(eval_dataset) == 1:
            return len(eval_dataset[0])
        return [len(ds) for ds in eval_dataset]
    return len(eval_dataset)</code></pre>
</details>
</dd>
<dt id="radio.data.visiondatamodule.VisionDataModule.size_train_dataset"><code class="name flex">
<span>def <span class="ident">size_train_dataset</span></span>(<span>train_dataset: Sized) ‑> Union[int, Sequence[int], Sequence[Sequence[int]], Sequence[Dict[str, int]], Dict[str, int], Dict[str, Dict[str, int]], Dict[str, Sequence[int]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the size of the train datasets.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>train_dataset</code></strong> :&ensp;<code>TrainDatasetType</code></dt>
<dd>Collection of train datasets.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>_</code></strong> :&ensp;<code>TrainSizeType</code></dt>
<dd>Collection of train datasets' sizes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def size_train_dataset(train_dataset: Sized) -&gt; TrainSizeType:
    &#34;&#34;&#34;
    Compute the size of the train datasets.

    Parameters
    ----------
    train_dataset: TrainDatasetType
        Collection of train datasets.

    Returns
    -------
    _ : TrainSizeType
        Collection of train datasets&#39; sizes.
    &#34;&#34;&#34;

    def _handle_is_mapping(dataset):
        mapping = {}
        for key, dset in dataset.items():
            if isinstance(dset, Mapping):
                mapping[key] = _handle_is_mapping(dset)
            if isinstance(dset, Sequence):
                mapping[key] = _handle_is_sequence(dset)
            mapping[key] = len(dset)
        return mapping

    def _handle_is_sequence(dataset):
        sequence = []
        for dset in dataset:
            if isinstance(dset, Mapping):
                sequence.append(_handle_is_mapping(dset))
            if isinstance(dset, Sequence):
                sequence.append(_handle_is_sequence(dset))
            sequence.append(len(dset))
        return sequence

    if isinstance(train_dataset, Mapping):
        return _handle_is_mapping(train_dataset)
    if isinstance(train_dataset, Sequence):
        if len(train_dataset) == 1:
            return VisionDataModule.size_train_dataset(train_dataset[0])
        return _handle_is_sequence(train_dataset)
    return len(train_dataset)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="radio.data.visiondatamodule.VisionDataModule.check_if_data_split"><code class="name flex">
<span>def <span class="ident">check_if_data_split</span></span>(<span>self, stem: str = '') ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Check if data is splitted in train, test and val folders</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_if_data_split(self, stem: str = &#34;&#34;) -&gt; None:
    &#34;&#34;&#34;Check if data is splitted in train, test and val folders&#34;&#34;&#34;
    has_train_folder = is_dir_or_symlink(self.root / stem / &#34;train&#34;)
    has_test_folder = is_dir_or_symlink(self.root / stem / &#34;test&#34;)
    has_val_folder = is_dir_or_symlink(self.root / stem / &#34;val&#34;)
    self.has_train_test_split = bool(has_train_folder and has_test_folder)
    self.has_train_val_split = bool(has_train_folder and has_val_folder)</code></pre>
</details>
</dd>
<dt id="radio.data.visiondatamodule.VisionDataModule.dataloader"><code class="name flex">
<span>def <span class="ident">dataloader</span></span>(<span>self, dataset: Union[torch.utils.data.dataset.Dataset, <a title="radio.data.dataset.BaseVisionDataset" href="dataset.html#radio.data.dataset.BaseVisionDataset">BaseVisionDataset</a>], batch_size: Optional[int] = None, shuffle: Optional[bool] = None, num_workers: Optional[int] = None, pin_memory: Optional[bool] = None, drop_last: Optional[bool] = None) ‑> torch.utils.data.dataloader.DataLoader</span>
</code></dt>
<dd>
<div class="desc"><p>Instantiate a DataLoader.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>How many samples per batch to load. Default = <code>32</code>.</dd>
<dt><strong><code>shuffle</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to shuffle the data at every epoch. Default = <code>False</code>.</dd>
<dt><strong><code>num_workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>How many subprocesses to use for data loading. <code>0</code> means that the
data will be loaded in the main process. Default: <code>0</code>.</dd>
<dt><strong><code>pin_memory</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, the data loader will copy Tensors into CUDA pinned
memory before returning them.</dd>
<dt><strong><code>drop_last</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Set to <code>True</code> to drop the last incomplete batch, if the dataset
size is not divisible by the batch size. If <code>False</code> and the size
of dataset is not divisible by the batch size, then the last batch
will be smaller. Default = <code>False</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>_</code></strong> :&ensp;<code>DataLoader</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dataloader(
    self,
    dataset: DatasetType,
    batch_size: Optional[int] = None,
    shuffle: Optional[bool] = None,
    num_workers: Optional[int] = None,
    pin_memory: Optional[bool] = None,
    drop_last: Optional[bool] = None,
) -&gt; DataLoader:
    &#34;&#34;&#34;
    Instantiate a DataLoader.

    Parameters
    ----------
    batch_size : int, optional
        How many samples per batch to load. Default = ``32``.
    shuffle : bool, optional
        Whether to shuffle the data at every epoch. Default = ``False``.
    num_workers : int, optional
        How many subprocesses to use for data loading. ``0`` means that the
        data will be loaded in the main process. Default: ``0``.
    pin_memory : bool, optional
        If ``True``, the data loader will copy Tensors into CUDA pinned
        memory before returning them.
    drop_last : bool, optional
        Set to ``True`` to drop the last incomplete batch, if the dataset
        size is not divisible by the batch size. If ``False`` and the size
        of dataset is not divisible by the batch size, then the last batch
        will be smaller. Default = ``False``.

    Returns
    -------
    _ : DataLoader
    &#34;&#34;&#34;
    shuffle = shuffle if shuffle else self.shuffle
    shuffle &amp;= not isinstance(dataset, IterableDataset)
    return DataLoader(
        dataset=dataset,
        batch_size=batch_size if batch_size else self.batch_size,
        shuffle=shuffle,
        num_workers=num_workers if num_workers else self.num_workers,
        pin_memory=pin_memory if pin_memory else self.pin_memory,
        drop_last=drop_last if drop_last else self.drop_last,
    )</code></pre>
</details>
</dd>
<dt id="radio.data.visiondatamodule.VisionDataModule.default_transforms"><code class="name flex">
<span>def <span class="ident">default_transforms</span></span>(<span>self, stage: Optional[str] = None) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Default transforms and augmentations for the dataset.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>stage</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Either <code>'fit</code>, <code>'validate'</code>, or <code>'test'</code>.
If stage = None, set-up all stages. Default = None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>_</code></strong> :&ensp;<code>Callable</code></dt>
<dd>All preprocessing steps (and if <code>'fit'</code>, augmentation steps too)
that should be applied to the images.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def default_transforms(self, stage: Optional[str] = None) -&gt; Callable:
    &#34;&#34;&#34;
    Default transforms and augmentations for the dataset.

    Parameters
    ----------
    stage: Optional[str]
        Either ``&#39;fit``, ``&#39;validate&#39;``, or ``&#39;test&#39;``.
        If stage = None, set-up all stages. Default = None.

    Returns
    -------
    _: Callable
        All preprocessing steps (and if ``&#39;fit&#39;``, augmentation steps too)
        that should be applied to the images.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="radio.data.visiondatamodule.VisionDataModule.predict_dataloader"><code class="name flex">
<span>def <span class="ident">predict_dataloader</span></span>(<span>self, *args: Any, **kwargs: Any) ‑> Union[torch.utils.data.dataloader.DataLoader, Sequence[torch.utils.data.dataloader.DataLoader]]</span>
</code></dt>
<dd>
<div class="desc"><p>Implement one or multiple PyTorch DataLoaders for prediction.</p>
<p>It's recommended that all data downloads and preparation happen in :meth:<code>prepare_data</code>.</p>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.trainer.Trainer.predict</code></li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
<h2 id="return">Return</h2>
<p>A :class:<code>torch.utils.data.DataLoader</code> or a sequence of them specifying prediction samples.</p>
<h2 id="note_1">Note</h2>
<p>In the case where you return multiple prediction dataloaders, the :meth:<code>predict_step</code>
will have an argument <code>dataloader_idx</code> which matches the order here.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_dataloader(self, *args: Any,
                       **kwargs: Any) -&gt; EvalDataLoaderType:
    pass</code></pre>
</details>
</dd>
<dt id="radio.data.visiondatamodule.VisionDataModule.prepare_data"><code class="name flex">
<span>def <span class="ident">prepare_data</span></span>(<span>self, *args: Any, **kwargs: Any) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Saves files to data root dir.
Verify data directory exists.
Verify if test/train/val splitted.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_data(self, *args: Any, **kwargs: Any) -&gt; None:
    &#34;&#34;&#34;
    Saves files to data root dir.
    Verify data directory exists.
    Verify if test/train/val splitted.
    &#34;&#34;&#34;
    if not is_dir_or_symlink(self.root):
        raise OSError(&#39;Study data directory not found!&#39;)
    self.check_if_data_split()
    self.dataset_cls(self.root, train=True, download=True)
    self.dataset_cls(self.root, train=False, download=True)</code></pre>
</details>
</dd>
<dt id="radio.data.visiondatamodule.VisionDataModule.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self, stage: Optional[str] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Creates train, validation and test collection of samplers.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>stage</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Either <code>'fit</code>, <code>'validate'</code>, or <code>'test'</code>.
If stage = None, set-up all stages. Default = None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup(self, stage: Optional[str] = None) -&gt; None:
    &#34;&#34;&#34;
    Creates train, validation and test collection of samplers.

    Parameters
    ----------
    stage: Optional[str]
        Either ``&#39;fit``, ``&#39;validate&#39;``, or ``&#39;test&#39;``.
        If stage = None, set-up all stages. Default = None.
    &#34;&#34;&#34;
    if stage in (None, &#34;fit&#34;):
        train_transforms = self.default_transforms(
            stage=&#34;fit&#34;
        ) if self.train_transforms is None else self.train_transforms

        val_transforms = self.default_transforms(
            stage=&#34;fit&#34;
        ) if self.val_transforms is None else self.val_transforms

        train_dataset = self.dataset_cls(
            self.root,
            train=True,
            transform=train_transforms,
            **self.EXTRA_ARGS,
        )

        val_dataset = self.dataset_cls(self.root,
                                       train=True,
                                       transform=val_transforms,
                                       **self.EXTRA_ARGS)

        self.validation = self.val_cls(train_dataset=train_dataset,
                                       val_dataset=val_dataset,
                                       batch_size=self.batch_size,
                                       shuffle=self.shuffle,
                                       num_workers=self.num_workers,
                                       pin_memory=self.pin_memory,
                                       drop_last=self.drop_last,
                                       num_folds=self.num_folds,
                                       seed=self.seed)

        self.validation.setup(self.val_split)
        self.has_validation = True

        self.train_dataset = train_dataset
        self.size_train = self.size_train_dataset(
            self.validation.train_samplers)
        self.val_dataset = val_dataset
        self.size_val = self.size_eval_dataset(
            self.validation.val_samplers)

    if stage in (None, &#34;test&#34;):
        test_transforms = self.default_transforms(
            stage=&#34;test&#34;
        ) if self.test_transforms is None else self.test_transforms
        self.test_dataset = self.dataset_cls(self.root,
                                             train=False,
                                             transform=test_transforms,
                                             **self.EXTRA_ARGS)
        self.size_test = self.size_eval_dataset(self.test_dataset)</code></pre>
</details>
</dd>
<dt id="radio.data.visiondatamodule.VisionDataModule.teardown"><code class="name flex">
<span>def <span class="ident">teardown</span></span>(<span>self, stage: Optional[str] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Called at the end of fit (train + validate), validate, test,
or predict. Remove root directory if a temporary was used.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>stage</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Either <code>'fit</code>, <code>'validate'</code>, or <code>'test'</code>.
If stage = None, set-up all stages. Default = None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def teardown(self, stage: Optional[str] = None) -&gt; None:
    &#34;&#34;&#34;
    Called at the end of fit (train + validate), validate, test,
    or predict. Remove root directory if a temporary was used.

    Parameters
    ----------
    stage: Optional[str]
        Either ``&#39;fit``, ``&#39;validate&#39;``, or ``&#39;test&#39;``.
        If stage = None, set-up all stages. Default = None.
    &#34;&#34;&#34;
    if self.is_temp_dir:
        shutil.rmtree(self.root)</code></pre>
</details>
</dd>
<dt id="radio.data.visiondatamodule.VisionDataModule.test_dataloader"><code class="name flex">
<span>def <span class="ident">test_dataloader</span></span>(<span>self, *args: Any, **kwargs: Any) ‑> Union[torch.utils.data.dataloader.DataLoader, Sequence[torch.utils.data.dataloader.DataLoader]]</span>
</code></dt>
<dd>
<div class="desc"><p>Generates one or multiple Pytorch DataLoaders for test.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>_</code></strong> :&ensp;<code>Collection</code> of <code>DataLoaders</code></dt>
<dd>Collection of test dataloaders specifying test samples.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_dataloader(self, *args: Any, **kwargs: Any) -&gt; EvalDataLoaderType:
    &#34;&#34;&#34;
    Generates one or multiple Pytorch DataLoaders for test.

    Returns
    -------
    _ : Collection of DataLoaders
        Collection of test dataloaders specifying test samples.
    &#34;&#34;&#34;
    loader_kwargs = {}
    loader_kwargs[&#34;batch_size&#34;] = kwargs.get(&#34;batch_size&#34;, None)
    loader_kwargs[&#34;shuffle&#34;] = kwargs.get(&#34;shuffle&#34;, None)
    loader_kwargs[&#34;num_workers&#34;] = kwargs.get(&#34;num_workers&#34;, None)
    loader_kwargs[&#34;pin_memory&#34;] = kwargs.get(&#34;pin_memory&#34;, None)
    loader_kwargs[&#34;drop_last&#34;] = kwargs.get(&#34;drop_last&#34;, None)

    if isinstance(self.test_dataset, Sequence):
        if len(self.test_dataset) == 1:
            return self.dataloader(self.test_dataset[0], **loader_kwargs)
        return [
            self.dataloader(ds, **loader_kwargs)
            for ds in self.test_dataset
        ]
    return self.dataloader(self.test_dataset, **loader_kwargs)</code></pre>
</details>
</dd>
<dt id="radio.data.visiondatamodule.VisionDataModule.train_dataloader"><code class="name flex">
<span>def <span class="ident">train_dataloader</span></span>(<span>self, *args: Any, **kwargs: Any) ‑> Union[torch.utils.data.dataloader.DataLoader, Sequence[torch.utils.data.dataloader.DataLoader], Sequence[Sequence[torch.utils.data.dataloader.DataLoader]], Sequence[Dict[str, torch.utils.data.dataloader.DataLoader]], Dict[str, torch.utils.data.dataloader.DataLoader], Dict[str, Dict[str, torch.utils.data.dataloader.DataLoader]], Dict[str, Sequence[torch.utils.data.dataloader.DataLoader]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Generates one or multiple Pytorch DataLoaders for train.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>_</code></strong> :&ensp;<code>Collection</code> of <code>DataLoader</code></dt>
<dd>Collection of train dataloaders specifying training samples.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_dataloader(self, *args: Any,
                     **kwargs: Any) -&gt; TrainDataLoaderType:
    &#34;&#34;&#34;
    Generates one or multiple Pytorch DataLoaders for train.

    Returns
    -------
    _ : Collection of DataLoader
        Collection of train dataloaders specifying training samples.
    &#34;&#34;&#34;
    loader_kwargs = {}
    loader_kwargs[&#34;batch_size&#34;] = kwargs.get(&#34;batch_size&#34;, None)
    loader_kwargs[&#34;shuffle&#34;] = kwargs.get(&#34;shuffle&#34;, None)
    loader_kwargs[&#34;num_workers&#34;] = kwargs.get(&#34;num_workers&#34;, None)
    loader_kwargs[&#34;pin_memory&#34;] = kwargs.get(&#34;pin_memory&#34;, None)
    loader_kwargs[&#34;drop_last&#34;] = kwargs.get(&#34;drop_last&#34;, None)

    def _handle_is_mapping(dataset):
        mapping = {}
        for key, dset in dataset.items():
            if isinstance(dset, Mapping):
                mapping[key] = _handle_is_mapping(dset)
            if isinstance(dset, Sequence):
                mapping[key] = _handle_is_sequence(dset)
            mapping[key] = self.dataloader(dset, **loader_kwargs)
        return mapping

    def _handle_is_sequence(dataset):
        sequence = []
        for dset in dataset:
            if isinstance(dset, Mapping):
                sequence.append(_handle_is_mapping(dset))
            if isinstance(dset, Sequence):
                sequence.append(_handle_is_sequence(dset))
            sequence.append(self.dataloader(dset, **loader_kwargs))
        return sequence

    if self.has_validation:
        return self.validation.train_dataloader()

    if isinstance(self.train_dataset, Mapping):
        return _handle_is_mapping(self.train_dataset)
    if isinstance(self.train_dataset, Sequence):
        if len(self.train_dataset) == 1:
            if isinstance(self.train_dataset[0], Mapping):
                return _handle_is_mapping(self.train_dataset[0])
            if isinstance(self.train_dataset[0], Sequence):
                if len(self.train_dataset[0]) == 1:
                    return self.dataloader(self.train_dataset[0][0],
                                           **loader_kwargs)
                return _handle_is_sequence(self.train_dataset[0])
            return self.dataloader(self.train_dataset[0], **loader_kwargs)
        return _handle_is_sequence(self.train_dataset)
    return self.dataloader(self.train_dataset, **loader_kwargs)</code></pre>
</details>
</dd>
<dt id="radio.data.visiondatamodule.VisionDataModule.val_dataloader"><code class="name flex">
<span>def <span class="ident">val_dataloader</span></span>(<span>self, *args: Any, **kwargs: Any) ‑> Union[torch.utils.data.dataloader.DataLoader, Sequence[torch.utils.data.dataloader.DataLoader]]</span>
</code></dt>
<dd>
<div class="desc"><p>Generates one or multiple Pytorch DataLoaders for validation.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>_</code></strong> :&ensp;<code>Collection</code> of <code>DataLoader</code></dt>
<dd>Collection of validation dataloaders specifying validation samples.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def val_dataloader(self, *args: Any, **kwargs: Any) -&gt; EvalDataLoaderType:
    &#34;&#34;&#34;
    Generates one or multiple Pytorch DataLoaders for validation.

    Returns
    -------
    _ : Collection of DataLoader
        Collection of validation dataloaders specifying validation samples.
    &#34;&#34;&#34;
    if self.has_validation:
        return self.validation.val_dataloader()

    loader_kwargs = {}
    loader_kwargs[&#34;batch_size&#34;] = kwargs.get(&#34;batch_size&#34;, None)
    loader_kwargs[&#34;shuffle&#34;] = kwargs.get(&#34;shuffle&#34;, None)
    loader_kwargs[&#34;num_workers&#34;] = kwargs.get(&#34;num_workers&#34;, None)
    loader_kwargs[&#34;pin_memory&#34;] = kwargs.get(&#34;pin_memory&#34;, None)
    loader_kwargs[&#34;drop_last&#34;] = kwargs.get(&#34;drop_last&#34;, None)

    if isinstance(self.val_dataset, Sequence):
        if len(self.val_dataset) == 1:
            return self.dataloader(self.val_dataset[0], **loader_kwargs)
        return [
            self.dataloader(ds, **loader_kwargs) for ds in self.val_dataset
        ]
    return self.dataloader(self.val_dataset, **loader_kwargs)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="radio.data.basedatamodule.BaseDataModule" href="basedatamodule.html#radio.data.basedatamodule.BaseDataModule">BaseDataModule</a></b></code>:
<ul class="hlist">
<li><code><a title="radio.data.basedatamodule.BaseDataModule.EXTRA_ARGS" href="basedatamodule.html#radio.data.basedatamodule.BaseDataModule.EXTRA_ARGS">EXTRA_ARGS</a></code></li>
<li><code><a title="radio.data.basedatamodule.BaseDataModule.dataset_cls" href="basedatamodule.html#radio.data.basedatamodule.BaseDataModule.dataset_cls">dataset_cls</a></code></li>
<li><code><a title="radio.data.basedatamodule.BaseDataModule.dims" href="basedatamodule.html#radio.data.basedatamodule.BaseDataModule.dims">dims</a></code></li>
<li><code><a title="radio.data.basedatamodule.BaseDataModule.name" href="basedatamodule.html#radio.data.basedatamodule.BaseDataModule.name">name</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="radio.data" href="index.html">radio.data</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="radio.data.visiondatamodule.VisionDataModule" href="#radio.data.visiondatamodule.VisionDataModule">VisionDataModule</a></code></h4>
<ul class="two-column">
<li><code><a title="radio.data.visiondatamodule.VisionDataModule.check_if_data_split" href="#radio.data.visiondatamodule.VisionDataModule.check_if_data_split">check_if_data_split</a></code></li>
<li><code><a title="radio.data.visiondatamodule.VisionDataModule.dataloader" href="#radio.data.visiondatamodule.VisionDataModule.dataloader">dataloader</a></code></li>
<li><code><a title="radio.data.visiondatamodule.VisionDataModule.default_transforms" href="#radio.data.visiondatamodule.VisionDataModule.default_transforms">default_transforms</a></code></li>
<li><code><a title="radio.data.visiondatamodule.VisionDataModule.get_max_shape" href="#radio.data.visiondatamodule.VisionDataModule.get_max_shape">get_max_shape</a></code></li>
<li><code><a title="radio.data.visiondatamodule.VisionDataModule.predict_dataloader" href="#radio.data.visiondatamodule.VisionDataModule.predict_dataloader">predict_dataloader</a></code></li>
<li><code><a title="radio.data.visiondatamodule.VisionDataModule.prepare_data" href="#radio.data.visiondatamodule.VisionDataModule.prepare_data">prepare_data</a></code></li>
<li><code><a title="radio.data.visiondatamodule.VisionDataModule.setup" href="#radio.data.visiondatamodule.VisionDataModule.setup">setup</a></code></li>
<li><code><a title="radio.data.visiondatamodule.VisionDataModule.size_eval_dataset" href="#radio.data.visiondatamodule.VisionDataModule.size_eval_dataset">size_eval_dataset</a></code></li>
<li><code><a title="radio.data.visiondatamodule.VisionDataModule.size_train_dataset" href="#radio.data.visiondatamodule.VisionDataModule.size_train_dataset">size_train_dataset</a></code></li>
<li><code><a title="radio.data.visiondatamodule.VisionDataModule.teardown" href="#radio.data.visiondatamodule.VisionDataModule.teardown">teardown</a></code></li>
<li><code><a title="radio.data.visiondatamodule.VisionDataModule.test_dataloader" href="#radio.data.visiondatamodule.VisionDataModule.test_dataloader">test_dataloader</a></code></li>
<li><code><a title="radio.data.visiondatamodule.VisionDataModule.train_dataloader" href="#radio.data.visiondatamodule.VisionDataModule.train_dataloader">train_dataloader</a></code></li>
<li><code><a title="radio.data.visiondatamodule.VisionDataModule.val_dataloader" href="#radio.data.visiondatamodule.VisionDataModule.val_dataloader">val_dataloader</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>
<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>radio.data.validation API documentation</title>
<meta name="description" content="Dataloaders are based on the PyTorch ``torch.utils.data.Dataloader`` data
primitive. They are wrappers around ``torch.utils.data.Dataset`` that
â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>radio.data.validation</code></h1>
</header>
<section id="section-intro">
<p>Dataloaders are based on the PyTorch <code>torch.utils.data.Dataloader</code> data
primitive. They are wrappers around <code>torch.utils.data.Dataset</code> that enable
easy access to the dataset samples, i.e., they prepare your data for
training/testing. Specifically, dataloaders are iterables that abstracts the
complexity of retrieving "minibatches" from Datasets, reshuffling the data at
every epoch to reduce model overfitting, use Python's <code>multiprocessing</code>
to speed up data retrieval, and automatic memory pinning, in an easy API.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
# coding=utf-8
&#34;&#34;&#34;
Dataloaders are based on the PyTorch ``torch.utils.data.Dataloader`` data
primitive. They are wrappers around ``torch.utils.data.Dataset`` that enable
easy access to the dataset samples, i.e., they prepare your data for
training/testing. Specifically, dataloaders are iterables that abstracts the
complexity of retrieving &#34;minibatches&#34; from Datasets, reshuffling the data at
every epoch to reduce model overfitting, use Python&#39;s ``multiprocessing``
to speed up data retrieval, and automatic memory pinning, in an easy API.
&#34;&#34;&#34;

from typing import (Any, Callable, List, TypeVar, Iterator, Tuple, Union, Optional)
import numpy as np
from sklearn.model_selection import KFold  # type: ignore
import torch
from torch.utils.data import DataLoader
from torch.utils.data import SubsetRandomSampler
from radio.data.dataset import DatasetType
from .datatypes import GenericEvalType, GenericTrainType

Type = TypeVar(&#34;Type&#34;)

TrainDataLoaderType = GenericTrainType[DataLoader]
EvalDataLoaderType = GenericEvalType[DataLoader]

WorkerInitFnType = Callable[[int], None]

# Ideally we would parameterize `DataLoader` by the return type of
# `collate_fn`, but there is currently no way to have that type parameter set
# to a default value if the user doesn&#39;t pass in a custom &#39;collate_fn&#39;.
# See https://github.com/python/mypy/issues/3737.
CollateFnType = Callable[[List[Type]], Any]

__all__ = [
    &#34;KFoldValidation&#34;, &#34;OneFoldValidation&#34;, &#34;TrainDataLoaderType&#34;,
    &#34;EvalDataLoaderType&#34;, &#34;ValidationType&#34;
]


class KFoldValidation:
    &#34;&#34;&#34;
    Create train and validation dataloaders for K-Fold Cross-Validation.

    Parameters
    ----------
    train_dataset : DatasetType
        Dataset from which to load the train data.
    val_dataset : DatasetType or None
        Dataset from which to load the validation data. If None, load the
        validation data from the train_dataset. ``val_dataset`` must be of the
        same size as ``train_dataset``. Default = None.
    batch_size : int, optional
        How many samples per batch to load. Default = ``32``.
    shuffle : bool, optional
        Whether to shuffle the data before splitting into batches. Note that
        the samples within each split will not be shuffled.
        Default = ``False``.
    num_workers : int, optional
        How many subprocesses to use for data loading. ``0`` means that the
        data will be loaded in the main process. Default: ``0``.
    collate_fn : Callable, optional
        Merges a list of samples to form a mini-batch of Tensor(s). Used when
        using batched loading from a map-style dataset.
    pin_memory : bool, optional
        If ``True``, the data loader will copy Tensors into CUDA pinned memory
        before returning them.
    drop_last : bool, optional
        Set to ``True`` to drop the last incomplete batch, if the dataset size
        is not divisible by the batch size. If ``False`` and the size of
        dataset is not divisible by the batch size, then the last batch will be
        smaller. Default = ``False``.
    worker_init_fn : Callable, optional
        If not ``None``, this will be called on each worker subprocess with the
        worker id (an int in ``[0, num_workers - 1]``) as input, after seeding
        and before data loading. Default = ``None``.
    num_folds : int, optional
        Number of folds. Must be at least ``2``. Default = ``5``.
    seed : int, optional
        When `shuffle` is True, `seed` affects the ordering of the indices,
        which controls the randomness of each fold. It is also use to seed the
        RNG used by RandomSampler to generate random indexes and
        multiprocessing to generate `base_seed` for workers. Pass an int for
        reproducible output across multiple function calls. Default = ``41``.
    &#34;&#34;&#34;
    def __init__(
        self,
        train_dataset: DatasetType,
        val_dataset: DatasetType = None,
        batch_size: int = 32,
        shuffle: bool = True,
        num_workers: int = 0,
        collate_fn: CollateFnType = None,
        pin_memory: bool = True,
        drop_last: bool = False,
        worker_init_fn: WorkerInitFnType = None,
        num_folds: int = 5,
        seed: int = 41,
    ) -&gt; None:

        if val_dataset:
            msg = &#34;len of val_dataset must be the same len of train_dataset.&#34;
            assert len(train_dataset) == len(val_dataset), msg
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset if val_dataset else train_dataset
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.collate_fn = collate_fn
        self.pin_memory = pin_memory
        self.drop_last = drop_last
        self.worker_init_fn = worker_init_fn
        self.generator = torch.Generator().manual_seed(seed)
        self.kfold = KFold(n_splits=num_folds,
                           shuffle=shuffle,
                           random_state=seed)
        self.fold: int
        self.kfold_split: Iterator[np.ndarray]
        self.train_samplers: List[SubsetRandomSampler] = []
        self.val_samplers: List[SubsetRandomSampler] = []
        self.size_train: Optional[int] = None
        self.size_val: Optional[int] = None

    def __iter__(self):
        self.fold = -1
        self.kfold_split = self.kfold.split(self.train_dataset)
        return self

    def __next__(self) -&gt; Tuple[int, DataLoader, DataLoader]:
        train_sampler, val_sampler = self._get_samplers()
        self.train_samplers.append(train_sampler)
        self.val_samplers.append(val_sampler)
        self.fold += 1
        return (
            self.fold,
            self._get_dataloader(self.fold),
            self._get_dataloader(self.fold, train=False),
        )

    def setup(self, val_split: Union[int, float] = 0.2) -&gt; None:
        &#34;&#34;&#34;
        Creates train and validation collection of samplers.

        Parameters
        ----------
        val_split: int or float, optional
            WARNING: val_split is not used in K-Fold validation. Left here just
            for compatibility with `OneFoldValidation`. Specify how the
            train_dataset should be split into train/validation datasets.
            Default = ``0.2``.
        &#34;&#34;&#34;
        if val_split != 0.2:
            print((
                &#39;WARNING: val_split is not used in K-Fold validation&#39;,
                &#39;Left here just for compatibilit with ``OneFoldValidation``.&#39;))

        self.kfold_split = self.kfold.split(self.train_dataset)
        for train_idx, val_idx in self.kfold_split:
            train_sampler = SubsetRandomSampler(train_idx,
                                                generator=self.generator)
            val_sampler = SubsetRandomSampler(val_idx,
                                              generator=self.generator)
            self.train_samplers.append(train_sampler)
            self.val_samplers.append(val_sampler)
        self.size_train = len(self.train_samplers[0])
        self.size_val = len(self.val_samplers[0])

    def _get_samplers(self) -&gt; Tuple[SubsetRandomSampler, SubsetRandomSampler]:
        &#34;&#34;&#34;Splits the dataset into train and validation samplers.&#34;&#34;&#34;
        train_idx, val_idx = self._get_indexes()
        train_sampler = SubsetRandomSampler(train_idx,
                                            generator=self.generator)
        val_sampler = SubsetRandomSampler(val_idx, generator=self.generator)
        return (train_sampler, val_sampler)

    def _get_indexes(self) -&gt; Tuple[List[int], List[int]]:
        &#34;&#34;&#34;Get train and validation sample indexes.&#34;&#34;&#34;
        train_idx, val_idx = next(self.kfold_split)
        return (train_idx, val_idx)

    def train_dataloader(self) -&gt; TrainDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for train.

        Parameters
        ----------
        sampler : Sampler
            Sampler for validation samples.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of train dataloaders specifying training samples.
        &#34;&#34;&#34;
        dataloaders = []
        num_dataloaders = len(self.train_samplers)
        for idx in range(num_dataloaders):
            dataloaders.append(self._get_dataloader(idx))
        return dataloaders

    def val_dataloader(self) -&gt; EvalDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for validation.

        Parameters
        ----------
        sampler : Sampler
            Sampler for validation samples.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of validation dataloaders specifying validation samples.
        &#34;&#34;&#34;
        dataloaders = []
        num_dataloaders = len(self.val_samplers)
        for idx in range(num_dataloaders):
            dataloaders.append(self._get_dataloader(idx, train=False))
        return dataloaders

    def _get_dataloader(self,
                        dataloader_idx: int,
                        train: bool = True) -&gt; DataLoader:
        &#34;&#34;&#34;
        Get train or validation dataloader.

        Parameters
        ----------
        dataloader_idx: int
            Dataloader index.
        train : bool, optional
            If True, return a loader for the train dataset, else for the
            validation dataset. Default = ``True``.

        Returns
        -------
        _ : DataLoader
            Train or validation dataloader.
        &#34;&#34;&#34;
        dataset = self.train_dataset if train else self.val_dataset
        sampler = (self.train_samplers[dataloader_idx]
                   if train else self.val_samplers[dataloader_idx])
        return DataLoader(
            dataset=dataset,
            batch_size=self.batch_size,
            sampler=sampler,
            num_workers=self.num_workers,
            collate_fn=self.collate_fn,
            pin_memory=self.pin_memory,
            drop_last=self.drop_last,
            worker_init_fn=self.worker_init_fn,
            generator=self.generator,
        )


class OneFoldValidation:
    &#34;&#34;&#34;
    Random split dataset into train and validation dataloaders.

    Parameters
    ----------
    train_dataset : DatasetType
        Dataset from which to load the train data.
    val_dataset : DatasetType or None
        Dataset from which to load the validation data. If None, load the
        validation data from the train_dataset. ``val_dataset`` must be of the
        same size as ``train_dataset``. Default = None.
    batch_size : int, optional
        How many samples per batch to load. Default = ``32``.
    shuffle : bool, optional
        Whether to shuffle the data at every epoch. Default = ``False``.
    num_workers : int, optional
        How many subprocesses to use for data loading. ``0`` means that the
        data will be loaded in the main process. Default: ``0``.
    collate_fn : Callable, optional
        Merges a list of samples to form a mini-batch of Tensor(s). Used when
        using batched loading from a map-style dataset.
    pin_memory : bool, optional
        If ``True``, the data loader will copy Tensors into CUDA pinned memory
        before returning them.
    drop_last : bool, optional
        Set to ``True`` to drop the last incomplete batch, if the dataset size
        is not divisible by the batch size. If ``False`` and the size of
        dataset is not divisible by the batch size, then the last batch will be
        smaller. Default = ``False``.
    worker_init_fn : Callable, optional
        If not ``None``, this will be called on each worker subprocess with the
        worker id (an int in ``[0, num_workers - 1]``) as input, after seeding
        and before data loading. Default = ``None``.
    num_folds : int, optional
        WARNING: ``num_folds`` shouldn&#39;t be set, it is hard-coded to ``2``.
        Parameter was only added for compatibility with KFoldValidation.
        Default = ``2``.
    seed : int, optional
        When `shuffle` is True, `seed` affects the ordering of the indices,
        which controls the randomness of each fold. It is also use to seed the
        RNG used by RandomSampler to generate random indexes and
        multiprocessing to generate `base_seed` for workers. Pass an int for
        reproducible output across multiple function calls. Default = ``41``.
    &#34;&#34;&#34;

    def __init__(
        self,
        train_dataset: DatasetType,
        val_dataset: DatasetType = None,
        batch_size: int = 32,
        shuffle: bool = True,
        num_workers: int = 0,
        collate_fn: CollateFnType = None,
        pin_memory: bool = True,
        drop_last: bool = False,
        worker_init_fn: WorkerInitFnType = None,
        num_folds: int = 2,
        seed: int = 41,
    ) -&gt; None:
        if val_dataset:
            msg = &#34;len of val_dataset must be the same len of train_dataset.&#34;
            assert len(train_dataset) == len(val_dataset), msg
        if num_folds != 2:
            print(
                (&#34;Warning: ``num_folds`` provided but will not be used. &#34;,
                 &#34;``num_folds`` is set to ``2``, i.e., train and val folds.&#34;))
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset if val_dataset else train_dataset
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.num_workers = num_workers
        self.collate_fn = collate_fn
        self.pin_memory = pin_memory
        self.drop_last = drop_last
        self.worker_init_fn = worker_init_fn
        self.generator = torch.Generator().manual_seed(seed)
        self.train_samplers: List[SubsetRandomSampler] = []
        self.val_samplers: List[SubsetRandomSampler] = []
        self.size_train: Optional[int] = None
        self.size_val: Optional[int] = None

    def __call__(
        self,
        val_split: Union[int, float] = 0.2
    ) -&gt; Tuple[TrainDataLoaderType, EvalDataLoaderType]:
        &#34;&#34;&#34;
        Returns train and validation dataloaders.

        Parameters
        ----------
        val_split: int or float, optional
            Specify how the train_dataset should be split into
            train/validation datasets. Default = ``0.2``.

        Returns
        -------
        _ : Tuple[TrainDataLoaderType, EvalDataLoaderType]
            Tuple where the first element is a collection of train dataloaders
            and the second element is a collection of validation dataloaders.
        &#34;&#34;&#34;
        self.setup(val_split)
        return (self.train_dataloader(), self.val_dataloader())

    def setup(self, val_split: Union[int, float] = 0.2) -&gt; None:
        &#34;&#34;&#34;
        Creates train and validation collection of samplers.

        Parameters
        ----------
        val_split: int or float, optional
            Specify how the train_dataset should be split into
            train/validation datasets. Default = ``0.2``.
        &#34;&#34;&#34;
        train_sampler, val_sampler = self._get_samplers(val_split)
        self.train_samplers.append(train_sampler)
        self.val_samplers.append(val_sampler)
        self.size_train = len(self.train_samplers[0])
        self.size_val = len(self.val_samplers[0])

    def _get_samplers(
        self,
        val_split: Union[int, float] = 0.2,
    ) -&gt; Tuple[SubsetRandomSampler, SubsetRandomSampler]:
        &#34;&#34;&#34;get train and validation samplers.&#34;&#34;&#34;
        len_dataset = len(self.train_dataset)
        train_idx, val_idx = self._get_indexes(val_split,
                                               len_dataset,
                                               shuffle=self.shuffle)
        train_sampler = SubsetRandomSampler(train_idx,
                                            generator=self.generator)
        val_sampler = SubsetRandomSampler(val_idx, generator=self.generator)
        return (train_sampler, val_sampler)

    @staticmethod
    def _get_indexes(val_split: Union[int, float],
                     len_dataset: int,
                     shuffle: bool = True) -&gt; Tuple[List[int], List[int]]:
        &#34;&#34;&#34;Get train and validation sample indexes.&#34;&#34;&#34;
        if isinstance(val_split, int):
            train_len = len_dataset - val_split
            splits = [train_len, val_split]
        elif isinstance(val_split, float):
            val_len = int(np.floor(val_split * len_dataset))
            train_len = len_dataset - val_len
            splits = [train_len, val_len]
        else:
            raise ValueError(f&#34;Unsupported type {type(val_split)}&#34;)
        dataset_idx = list(range(len_dataset))
        if shuffle:
            np.random.shuffle(dataset_idx)
        train_idx, val_idx = dataset_idx[:splits[0]], dataset_idx[:splits[1]]

        return (train_idx, val_idx)

    def train_dataloader(self) -&gt; TrainDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for train.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of train dataloaders specifying training samples.
        &#34;&#34;&#34;
        num_dataloaders = len(self.train_samplers)
        if num_dataloaders == 1:
            return self._get_dataloader(0)
        dataloaders = []
        for idx in range(num_dataloaders):
            dataloaders.append(self._get_dataloader(idx))
        return dataloaders

    def val_dataloader(self) -&gt; EvalDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for validation.

        Returns
        -------
        _ : Collection of DataLoaders
            Collection of validation dataloaders specifying validation samples.
        &#34;&#34;&#34;
        num_dataloaders = len(self.val_samplers)
        if num_dataloaders == 1:
            return self._get_dataloader(0, train=False)
        dataloaders = []
        for idx in range(num_dataloaders):
            dataloaders.append(self._get_dataloader(idx, train=False))
        return dataloaders

    def _get_dataloader(self,
                        dataloader_idx: int,
                        train: bool = True) -&gt; DataLoader:
        &#34;&#34;&#34;
        Get train or validation dataloader.

        Parameters
        ----------
        dataloader_idx: int
            Dataloader index.
        train : bool, optional
            If True, return a loader for the train dataset, else for the
            validation dataset. Default = ``True``.

        Returns
        -------
        _ : DataLoader
            Train or validation dataloader.
        &#34;&#34;&#34;
        dataset = self.train_dataset if train else self.val_dataset
        sampler = (self.train_samplers[dataloader_idx]
                   if train else self.val_samplers[dataloader_idx])
        return DataLoader(
            dataset=dataset,
            batch_size=self.batch_size,
            sampler=sampler,
            num_workers=self.num_workers,
            collate_fn=self.collate_fn,
            pin_memory=self.pin_memory,
            drop_last=self.drop_last,
            worker_init_fn=self.worker_init_fn,
            generator=self.generator,
        )


ValidationType = Union[OneFoldValidation, KFoldValidation]</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="radio.data.validation.KFoldValidation"><code class="flex name class">
<span>class <span class="ident">KFoldValidation</span></span>
<span>(</span><span>train_dataset:Â Union[torch.utils.data.dataset.Dataset,Â <a title="radio.data.dataset.BaseVisionDataset" href="dataset.html#radio.data.dataset.BaseVisionDataset">BaseVisionDataset</a>], val_dataset:Â Union[torch.utils.data.dataset.Dataset,Â <a title="radio.data.dataset.BaseVisionDataset" href="dataset.html#radio.data.dataset.BaseVisionDataset">BaseVisionDataset</a>]Â =Â None, batch_size:Â intÂ =Â 32, shuffle:Â boolÂ =Â True, num_workers:Â intÂ =Â 0, collate_fn:Â Callable[[List[~Type]],Â Any]Â =Â None, pin_memory:Â boolÂ =Â True, drop_last:Â boolÂ =Â False, worker_init_fn:Â Callable[[int],Â None]Â =Â None, num_folds:Â intÂ =Â 5, seed:Â intÂ =Â 41)</span>
</code></dt>
<dd>
<div class="desc"><p>Create train and validation dataloaders for K-Fold Cross-Validation.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>train_dataset</code></strong> :&ensp;<code>DatasetType</code></dt>
<dd>Dataset from which to load the train data.</dd>
<dt><strong><code>val_dataset</code></strong> :&ensp;<code>DatasetType</code> or <code>None</code></dt>
<dd>Dataset from which to load the validation data. If None, load the
validation data from the train_dataset. <code>val_dataset</code> must be of the
same size as <code>train_dataset</code>. Default = None.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>How many samples per batch to load. Default = <code>32</code>.</dd>
<dt><strong><code>shuffle</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to shuffle the data before splitting into batches. Note that
the samples within each split will not be shuffled.
Default = <code>False</code>.</dd>
<dt><strong><code>num_workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>How many subprocesses to use for data loading. <code>0</code> means that the
data will be loaded in the main process. Default: <code>0</code>.</dd>
<dt><strong><code>collate_fn</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>Merges a list of samples to form a mini-batch of Tensor(s). Used when
using batched loading from a map-style dataset.</dd>
<dt><strong><code>pin_memory</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, the data loader will copy Tensors into CUDA pinned memory
before returning them.</dd>
<dt><strong><code>drop_last</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Set to <code>True</code> to drop the last incomplete batch, if the dataset size
is not divisible by the batch size. If <code>False</code> and the size of
dataset is not divisible by the batch size, then the last batch will be
smaller. Default = <code>False</code>.</dd>
<dt><strong><code>worker_init_fn</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>If not <code>None</code>, this will be called on each worker subprocess with the
worker id (an int in <code>[0, num_workers - 1]</code>) as input, after seeding
and before data loading. Default = <code>None</code>.</dd>
<dt><strong><code>num_folds</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of folds. Must be at least <code>2</code>. Default = <code>5</code>.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>When <code>shuffle</code> is True, <code>seed</code> affects the ordering of the indices,
which controls the randomness of each fold. It is also use to seed the
RNG used by RandomSampler to generate random indexes and
multiprocessing to generate <code>base_seed</code> for workers. Pass an int for
reproducible output across multiple function calls. Default = <code>41</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class KFoldValidation:
    &#34;&#34;&#34;
    Create train and validation dataloaders for K-Fold Cross-Validation.

    Parameters
    ----------
    train_dataset : DatasetType
        Dataset from which to load the train data.
    val_dataset : DatasetType or None
        Dataset from which to load the validation data. If None, load the
        validation data from the train_dataset. ``val_dataset`` must be of the
        same size as ``train_dataset``. Default = None.
    batch_size : int, optional
        How many samples per batch to load. Default = ``32``.
    shuffle : bool, optional
        Whether to shuffle the data before splitting into batches. Note that
        the samples within each split will not be shuffled.
        Default = ``False``.
    num_workers : int, optional
        How many subprocesses to use for data loading. ``0`` means that the
        data will be loaded in the main process. Default: ``0``.
    collate_fn : Callable, optional
        Merges a list of samples to form a mini-batch of Tensor(s). Used when
        using batched loading from a map-style dataset.
    pin_memory : bool, optional
        If ``True``, the data loader will copy Tensors into CUDA pinned memory
        before returning them.
    drop_last : bool, optional
        Set to ``True`` to drop the last incomplete batch, if the dataset size
        is not divisible by the batch size. If ``False`` and the size of
        dataset is not divisible by the batch size, then the last batch will be
        smaller. Default = ``False``.
    worker_init_fn : Callable, optional
        If not ``None``, this will be called on each worker subprocess with the
        worker id (an int in ``[0, num_workers - 1]``) as input, after seeding
        and before data loading. Default = ``None``.
    num_folds : int, optional
        Number of folds. Must be at least ``2``. Default = ``5``.
    seed : int, optional
        When `shuffle` is True, `seed` affects the ordering of the indices,
        which controls the randomness of each fold. It is also use to seed the
        RNG used by RandomSampler to generate random indexes and
        multiprocessing to generate `base_seed` for workers. Pass an int for
        reproducible output across multiple function calls. Default = ``41``.
    &#34;&#34;&#34;
    def __init__(
        self,
        train_dataset: DatasetType,
        val_dataset: DatasetType = None,
        batch_size: int = 32,
        shuffle: bool = True,
        num_workers: int = 0,
        collate_fn: CollateFnType = None,
        pin_memory: bool = True,
        drop_last: bool = False,
        worker_init_fn: WorkerInitFnType = None,
        num_folds: int = 5,
        seed: int = 41,
    ) -&gt; None:

        if val_dataset:
            msg = &#34;len of val_dataset must be the same len of train_dataset.&#34;
            assert len(train_dataset) == len(val_dataset), msg
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset if val_dataset else train_dataset
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.collate_fn = collate_fn
        self.pin_memory = pin_memory
        self.drop_last = drop_last
        self.worker_init_fn = worker_init_fn
        self.generator = torch.Generator().manual_seed(seed)
        self.kfold = KFold(n_splits=num_folds,
                           shuffle=shuffle,
                           random_state=seed)
        self.fold: int
        self.kfold_split: Iterator[np.ndarray]
        self.train_samplers: List[SubsetRandomSampler] = []
        self.val_samplers: List[SubsetRandomSampler] = []
        self.size_train: Optional[int] = None
        self.size_val: Optional[int] = None

    def __iter__(self):
        self.fold = -1
        self.kfold_split = self.kfold.split(self.train_dataset)
        return self

    def __next__(self) -&gt; Tuple[int, DataLoader, DataLoader]:
        train_sampler, val_sampler = self._get_samplers()
        self.train_samplers.append(train_sampler)
        self.val_samplers.append(val_sampler)
        self.fold += 1
        return (
            self.fold,
            self._get_dataloader(self.fold),
            self._get_dataloader(self.fold, train=False),
        )

    def setup(self, val_split: Union[int, float] = 0.2) -&gt; None:
        &#34;&#34;&#34;
        Creates train and validation collection of samplers.

        Parameters
        ----------
        val_split: int or float, optional
            WARNING: val_split is not used in K-Fold validation. Left here just
            for compatibility with `OneFoldValidation`. Specify how the
            train_dataset should be split into train/validation datasets.
            Default = ``0.2``.
        &#34;&#34;&#34;
        if val_split != 0.2:
            print((
                &#39;WARNING: val_split is not used in K-Fold validation&#39;,
                &#39;Left here just for compatibilit with ``OneFoldValidation``.&#39;))

        self.kfold_split = self.kfold.split(self.train_dataset)
        for train_idx, val_idx in self.kfold_split:
            train_sampler = SubsetRandomSampler(train_idx,
                                                generator=self.generator)
            val_sampler = SubsetRandomSampler(val_idx,
                                              generator=self.generator)
            self.train_samplers.append(train_sampler)
            self.val_samplers.append(val_sampler)
        self.size_train = len(self.train_samplers[0])
        self.size_val = len(self.val_samplers[0])

    def _get_samplers(self) -&gt; Tuple[SubsetRandomSampler, SubsetRandomSampler]:
        &#34;&#34;&#34;Splits the dataset into train and validation samplers.&#34;&#34;&#34;
        train_idx, val_idx = self._get_indexes()
        train_sampler = SubsetRandomSampler(train_idx,
                                            generator=self.generator)
        val_sampler = SubsetRandomSampler(val_idx, generator=self.generator)
        return (train_sampler, val_sampler)

    def _get_indexes(self) -&gt; Tuple[List[int], List[int]]:
        &#34;&#34;&#34;Get train and validation sample indexes.&#34;&#34;&#34;
        train_idx, val_idx = next(self.kfold_split)
        return (train_idx, val_idx)

    def train_dataloader(self) -&gt; TrainDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for train.

        Parameters
        ----------
        sampler : Sampler
            Sampler for validation samples.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of train dataloaders specifying training samples.
        &#34;&#34;&#34;
        dataloaders = []
        num_dataloaders = len(self.train_samplers)
        for idx in range(num_dataloaders):
            dataloaders.append(self._get_dataloader(idx))
        return dataloaders

    def val_dataloader(self) -&gt; EvalDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for validation.

        Parameters
        ----------
        sampler : Sampler
            Sampler for validation samples.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of validation dataloaders specifying validation samples.
        &#34;&#34;&#34;
        dataloaders = []
        num_dataloaders = len(self.val_samplers)
        for idx in range(num_dataloaders):
            dataloaders.append(self._get_dataloader(idx, train=False))
        return dataloaders

    def _get_dataloader(self,
                        dataloader_idx: int,
                        train: bool = True) -&gt; DataLoader:
        &#34;&#34;&#34;
        Get train or validation dataloader.

        Parameters
        ----------
        dataloader_idx: int
            Dataloader index.
        train : bool, optional
            If True, return a loader for the train dataset, else for the
            validation dataset. Default = ``True``.

        Returns
        -------
        _ : DataLoader
            Train or validation dataloader.
        &#34;&#34;&#34;
        dataset = self.train_dataset if train else self.val_dataset
        sampler = (self.train_samplers[dataloader_idx]
                   if train else self.val_samplers[dataloader_idx])
        return DataLoader(
            dataset=dataset,
            batch_size=self.batch_size,
            sampler=sampler,
            num_workers=self.num_workers,
            collate_fn=self.collate_fn,
            pin_memory=self.pin_memory,
            drop_last=self.drop_last,
            worker_init_fn=self.worker_init_fn,
            generator=self.generator,
        )</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="radio.data.validation.KFoldValidation.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self, val_split:Â Union[int,Â float]Â =Â 0.2) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>Creates train and validation collection of samplers.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>val_split</code></strong> :&ensp;<code>int</code> or <code>float</code>, optional</dt>
<dd>WARNING: val_split is not used in K-Fold validation. Left here just
for compatibility with <code><a title="radio.data.validation.OneFoldValidation" href="#radio.data.validation.OneFoldValidation">OneFoldValidation</a></code>. Specify how the
train_dataset should be split into train/validation datasets.
Default = <code>0.2</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup(self, val_split: Union[int, float] = 0.2) -&gt; None:
    &#34;&#34;&#34;
    Creates train and validation collection of samplers.

    Parameters
    ----------
    val_split: int or float, optional
        WARNING: val_split is not used in K-Fold validation. Left here just
        for compatibility with `OneFoldValidation`. Specify how the
        train_dataset should be split into train/validation datasets.
        Default = ``0.2``.
    &#34;&#34;&#34;
    if val_split != 0.2:
        print((
            &#39;WARNING: val_split is not used in K-Fold validation&#39;,
            &#39;Left here just for compatibilit with ``OneFoldValidation``.&#39;))

    self.kfold_split = self.kfold.split(self.train_dataset)
    for train_idx, val_idx in self.kfold_split:
        train_sampler = SubsetRandomSampler(train_idx,
                                            generator=self.generator)
        val_sampler = SubsetRandomSampler(val_idx,
                                          generator=self.generator)
        self.train_samplers.append(train_sampler)
        self.val_samplers.append(val_sampler)
    self.size_train = len(self.train_samplers[0])
    self.size_val = len(self.val_samplers[0])</code></pre>
</details>
</dd>
<dt id="radio.data.validation.KFoldValidation.train_dataloader"><code class="name flex">
<span>def <span class="ident">train_dataloader</span></span>(<span>self) â€‘>Â Union[torch.utils.data.dataloader.DataLoader,Â Sequence[torch.utils.data.dataloader.DataLoader],Â Sequence[Sequence[torch.utils.data.dataloader.DataLoader]],Â Sequence[Dict[str,Â torch.utils.data.dataloader.DataLoader]],Â Dict[str,Â torch.utils.data.dataloader.DataLoader],Â Dict[str,Â Dict[str,Â torch.utils.data.dataloader.DataLoader]],Â Dict[str,Â Sequence[torch.utils.data.dataloader.DataLoader]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Generates one or multiple Pytorch DataLoaders for train.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sampler</code></strong> :&ensp;<code>Sampler</code></dt>
<dd>Sampler for validation samples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>_</code></strong> :&ensp;<code>Collection</code> of <code>DataLoader</code></dt>
<dd>Collection of train dataloaders specifying training samples.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_dataloader(self) -&gt; TrainDataLoaderType:
    &#34;&#34;&#34;
    Generates one or multiple Pytorch DataLoaders for train.

    Parameters
    ----------
    sampler : Sampler
        Sampler for validation samples.

    Returns
    -------
    _ : Collection of DataLoader
        Collection of train dataloaders specifying training samples.
    &#34;&#34;&#34;
    dataloaders = []
    num_dataloaders = len(self.train_samplers)
    for idx in range(num_dataloaders):
        dataloaders.append(self._get_dataloader(idx))
    return dataloaders</code></pre>
</details>
</dd>
<dt id="radio.data.validation.KFoldValidation.val_dataloader"><code class="name flex">
<span>def <span class="ident">val_dataloader</span></span>(<span>self) â€‘>Â Union[torch.utils.data.dataloader.DataLoader,Â Sequence[torch.utils.data.dataloader.DataLoader]]</span>
</code></dt>
<dd>
<div class="desc"><p>Generates one or multiple Pytorch DataLoaders for validation.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sampler</code></strong> :&ensp;<code>Sampler</code></dt>
<dd>Sampler for validation samples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>_</code></strong> :&ensp;<code>Collection</code> of <code>DataLoader</code></dt>
<dd>Collection of validation dataloaders specifying validation samples.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def val_dataloader(self) -&gt; EvalDataLoaderType:
    &#34;&#34;&#34;
    Generates one or multiple Pytorch DataLoaders for validation.

    Parameters
    ----------
    sampler : Sampler
        Sampler for validation samples.

    Returns
    -------
    _ : Collection of DataLoader
        Collection of validation dataloaders specifying validation samples.
    &#34;&#34;&#34;
    dataloaders = []
    num_dataloaders = len(self.val_samplers)
    for idx in range(num_dataloaders):
        dataloaders.append(self._get_dataloader(idx, train=False))
    return dataloaders</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="radio.data.validation.OneFoldValidation"><code class="flex name class">
<span>class <span class="ident">OneFoldValidation</span></span>
<span>(</span><span>train_dataset:Â Union[torch.utils.data.dataset.Dataset,Â <a title="radio.data.dataset.BaseVisionDataset" href="dataset.html#radio.data.dataset.BaseVisionDataset">BaseVisionDataset</a>], val_dataset:Â Union[torch.utils.data.dataset.Dataset,Â <a title="radio.data.dataset.BaseVisionDataset" href="dataset.html#radio.data.dataset.BaseVisionDataset">BaseVisionDataset</a>]Â =Â None, batch_size:Â intÂ =Â 32, shuffle:Â boolÂ =Â True, num_workers:Â intÂ =Â 0, collate_fn:Â Callable[[List[~Type]],Â Any]Â =Â None, pin_memory:Â boolÂ =Â True, drop_last:Â boolÂ =Â False, worker_init_fn:Â Callable[[int],Â None]Â =Â None, num_folds:Â intÂ =Â 2, seed:Â intÂ =Â 41)</span>
</code></dt>
<dd>
<div class="desc"><p>Random split dataset into train and validation dataloaders.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>train_dataset</code></strong> :&ensp;<code>DatasetType</code></dt>
<dd>Dataset from which to load the train data.</dd>
<dt><strong><code>val_dataset</code></strong> :&ensp;<code>DatasetType</code> or <code>None</code></dt>
<dd>Dataset from which to load the validation data. If None, load the
validation data from the train_dataset. <code>val_dataset</code> must be of the
same size as <code>train_dataset</code>. Default = None.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>How many samples per batch to load. Default = <code>32</code>.</dd>
<dt><strong><code>shuffle</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to shuffle the data at every epoch. Default = <code>False</code>.</dd>
<dt><strong><code>num_workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>How many subprocesses to use for data loading. <code>0</code> means that the
data will be loaded in the main process. Default: <code>0</code>.</dd>
<dt><strong><code>collate_fn</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>Merges a list of samples to form a mini-batch of Tensor(s). Used when
using batched loading from a map-style dataset.</dd>
<dt><strong><code>pin_memory</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, the data loader will copy Tensors into CUDA pinned memory
before returning them.</dd>
<dt><strong><code>drop_last</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Set to <code>True</code> to drop the last incomplete batch, if the dataset size
is not divisible by the batch size. If <code>False</code> and the size of
dataset is not divisible by the batch size, then the last batch will be
smaller. Default = <code>False</code>.</dd>
<dt><strong><code>worker_init_fn</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>If not <code>None</code>, this will be called on each worker subprocess with the
worker id (an int in <code>[0, num_workers - 1]</code>) as input, after seeding
and before data loading. Default = <code>None</code>.</dd>
<dt><strong><code>num_folds</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>WARNING: <code>num_folds</code> shouldn't be set, it is hard-coded to <code>2</code>.
Parameter was only added for compatibility with KFoldValidation.
Default = <code>2</code>.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>When <code>shuffle</code> is True, <code>seed</code> affects the ordering of the indices,
which controls the randomness of each fold. It is also use to seed the
RNG used by RandomSampler to generate random indexes and
multiprocessing to generate <code>base_seed</code> for workers. Pass an int for
reproducible output across multiple function calls. Default = <code>41</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OneFoldValidation:
    &#34;&#34;&#34;
    Random split dataset into train and validation dataloaders.

    Parameters
    ----------
    train_dataset : DatasetType
        Dataset from which to load the train data.
    val_dataset : DatasetType or None
        Dataset from which to load the validation data. If None, load the
        validation data from the train_dataset. ``val_dataset`` must be of the
        same size as ``train_dataset``. Default = None.
    batch_size : int, optional
        How many samples per batch to load. Default = ``32``.
    shuffle : bool, optional
        Whether to shuffle the data at every epoch. Default = ``False``.
    num_workers : int, optional
        How many subprocesses to use for data loading. ``0`` means that the
        data will be loaded in the main process. Default: ``0``.
    collate_fn : Callable, optional
        Merges a list of samples to form a mini-batch of Tensor(s). Used when
        using batched loading from a map-style dataset.
    pin_memory : bool, optional
        If ``True``, the data loader will copy Tensors into CUDA pinned memory
        before returning them.
    drop_last : bool, optional
        Set to ``True`` to drop the last incomplete batch, if the dataset size
        is not divisible by the batch size. If ``False`` and the size of
        dataset is not divisible by the batch size, then the last batch will be
        smaller. Default = ``False``.
    worker_init_fn : Callable, optional
        If not ``None``, this will be called on each worker subprocess with the
        worker id (an int in ``[0, num_workers - 1]``) as input, after seeding
        and before data loading. Default = ``None``.
    num_folds : int, optional
        WARNING: ``num_folds`` shouldn&#39;t be set, it is hard-coded to ``2``.
        Parameter was only added for compatibility with KFoldValidation.
        Default = ``2``.
    seed : int, optional
        When `shuffle` is True, `seed` affects the ordering of the indices,
        which controls the randomness of each fold. It is also use to seed the
        RNG used by RandomSampler to generate random indexes and
        multiprocessing to generate `base_seed` for workers. Pass an int for
        reproducible output across multiple function calls. Default = ``41``.
    &#34;&#34;&#34;

    def __init__(
        self,
        train_dataset: DatasetType,
        val_dataset: DatasetType = None,
        batch_size: int = 32,
        shuffle: bool = True,
        num_workers: int = 0,
        collate_fn: CollateFnType = None,
        pin_memory: bool = True,
        drop_last: bool = False,
        worker_init_fn: WorkerInitFnType = None,
        num_folds: int = 2,
        seed: int = 41,
    ) -&gt; None:
        if val_dataset:
            msg = &#34;len of val_dataset must be the same len of train_dataset.&#34;
            assert len(train_dataset) == len(val_dataset), msg
        if num_folds != 2:
            print(
                (&#34;Warning: ``num_folds`` provided but will not be used. &#34;,
                 &#34;``num_folds`` is set to ``2``, i.e., train and val folds.&#34;))
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset if val_dataset else train_dataset
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.num_workers = num_workers
        self.collate_fn = collate_fn
        self.pin_memory = pin_memory
        self.drop_last = drop_last
        self.worker_init_fn = worker_init_fn
        self.generator = torch.Generator().manual_seed(seed)
        self.train_samplers: List[SubsetRandomSampler] = []
        self.val_samplers: List[SubsetRandomSampler] = []
        self.size_train: Optional[int] = None
        self.size_val: Optional[int] = None

    def __call__(
        self,
        val_split: Union[int, float] = 0.2
    ) -&gt; Tuple[TrainDataLoaderType, EvalDataLoaderType]:
        &#34;&#34;&#34;
        Returns train and validation dataloaders.

        Parameters
        ----------
        val_split: int or float, optional
            Specify how the train_dataset should be split into
            train/validation datasets. Default = ``0.2``.

        Returns
        -------
        _ : Tuple[TrainDataLoaderType, EvalDataLoaderType]
            Tuple where the first element is a collection of train dataloaders
            and the second element is a collection of validation dataloaders.
        &#34;&#34;&#34;
        self.setup(val_split)
        return (self.train_dataloader(), self.val_dataloader())

    def setup(self, val_split: Union[int, float] = 0.2) -&gt; None:
        &#34;&#34;&#34;
        Creates train and validation collection of samplers.

        Parameters
        ----------
        val_split: int or float, optional
            Specify how the train_dataset should be split into
            train/validation datasets. Default = ``0.2``.
        &#34;&#34;&#34;
        train_sampler, val_sampler = self._get_samplers(val_split)
        self.train_samplers.append(train_sampler)
        self.val_samplers.append(val_sampler)
        self.size_train = len(self.train_samplers[0])
        self.size_val = len(self.val_samplers[0])

    def _get_samplers(
        self,
        val_split: Union[int, float] = 0.2,
    ) -&gt; Tuple[SubsetRandomSampler, SubsetRandomSampler]:
        &#34;&#34;&#34;get train and validation samplers.&#34;&#34;&#34;
        len_dataset = len(self.train_dataset)
        train_idx, val_idx = self._get_indexes(val_split,
                                               len_dataset,
                                               shuffle=self.shuffle)
        train_sampler = SubsetRandomSampler(train_idx,
                                            generator=self.generator)
        val_sampler = SubsetRandomSampler(val_idx, generator=self.generator)
        return (train_sampler, val_sampler)

    @staticmethod
    def _get_indexes(val_split: Union[int, float],
                     len_dataset: int,
                     shuffle: bool = True) -&gt; Tuple[List[int], List[int]]:
        &#34;&#34;&#34;Get train and validation sample indexes.&#34;&#34;&#34;
        if isinstance(val_split, int):
            train_len = len_dataset - val_split
            splits = [train_len, val_split]
        elif isinstance(val_split, float):
            val_len = int(np.floor(val_split * len_dataset))
            train_len = len_dataset - val_len
            splits = [train_len, val_len]
        else:
            raise ValueError(f&#34;Unsupported type {type(val_split)}&#34;)
        dataset_idx = list(range(len_dataset))
        if shuffle:
            np.random.shuffle(dataset_idx)
        train_idx, val_idx = dataset_idx[:splits[0]], dataset_idx[:splits[1]]

        return (train_idx, val_idx)

    def train_dataloader(self) -&gt; TrainDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for train.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of train dataloaders specifying training samples.
        &#34;&#34;&#34;
        num_dataloaders = len(self.train_samplers)
        if num_dataloaders == 1:
            return self._get_dataloader(0)
        dataloaders = []
        for idx in range(num_dataloaders):
            dataloaders.append(self._get_dataloader(idx))
        return dataloaders

    def val_dataloader(self) -&gt; EvalDataLoaderType:
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for validation.

        Returns
        -------
        _ : Collection of DataLoaders
            Collection of validation dataloaders specifying validation samples.
        &#34;&#34;&#34;
        num_dataloaders = len(self.val_samplers)
        if num_dataloaders == 1:
            return self._get_dataloader(0, train=False)
        dataloaders = []
        for idx in range(num_dataloaders):
            dataloaders.append(self._get_dataloader(idx, train=False))
        return dataloaders

    def _get_dataloader(self,
                        dataloader_idx: int,
                        train: bool = True) -&gt; DataLoader:
        &#34;&#34;&#34;
        Get train or validation dataloader.

        Parameters
        ----------
        dataloader_idx: int
            Dataloader index.
        train : bool, optional
            If True, return a loader for the train dataset, else for the
            validation dataset. Default = ``True``.

        Returns
        -------
        _ : DataLoader
            Train or validation dataloader.
        &#34;&#34;&#34;
        dataset = self.train_dataset if train else self.val_dataset
        sampler = (self.train_samplers[dataloader_idx]
                   if train else self.val_samplers[dataloader_idx])
        return DataLoader(
            dataset=dataset,
            batch_size=self.batch_size,
            sampler=sampler,
            num_workers=self.num_workers,
            collate_fn=self.collate_fn,
            pin_memory=self.pin_memory,
            drop_last=self.drop_last,
            worker_init_fn=self.worker_init_fn,
            generator=self.generator,
        )</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="radio.data.validation.OneFoldValidation.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self, val_split:Â Union[int,Â float]Â =Â 0.2) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>Creates train and validation collection of samplers.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>val_split</code></strong> :&ensp;<code>int</code> or <code>float</code>, optional</dt>
<dd>Specify how the train_dataset should be split into
train/validation datasets. Default = <code>0.2</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup(self, val_split: Union[int, float] = 0.2) -&gt; None:
    &#34;&#34;&#34;
    Creates train and validation collection of samplers.

    Parameters
    ----------
    val_split: int or float, optional
        Specify how the train_dataset should be split into
        train/validation datasets. Default = ``0.2``.
    &#34;&#34;&#34;
    train_sampler, val_sampler = self._get_samplers(val_split)
    self.train_samplers.append(train_sampler)
    self.val_samplers.append(val_sampler)
    self.size_train = len(self.train_samplers[0])
    self.size_val = len(self.val_samplers[0])</code></pre>
</details>
</dd>
<dt id="radio.data.validation.OneFoldValidation.train_dataloader"><code class="name flex">
<span>def <span class="ident">train_dataloader</span></span>(<span>self) â€‘>Â Union[torch.utils.data.dataloader.DataLoader,Â Sequence[torch.utils.data.dataloader.DataLoader],Â Sequence[Sequence[torch.utils.data.dataloader.DataLoader]],Â Sequence[Dict[str,Â torch.utils.data.dataloader.DataLoader]],Â Dict[str,Â torch.utils.data.dataloader.DataLoader],Â Dict[str,Â Dict[str,Â torch.utils.data.dataloader.DataLoader]],Â Dict[str,Â Sequence[torch.utils.data.dataloader.DataLoader]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Generates one or multiple Pytorch DataLoaders for train.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>_</code></strong> :&ensp;<code>Collection</code> of <code>DataLoader</code></dt>
<dd>Collection of train dataloaders specifying training samples.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_dataloader(self) -&gt; TrainDataLoaderType:
    &#34;&#34;&#34;
    Generates one or multiple Pytorch DataLoaders for train.

    Returns
    -------
    _ : Collection of DataLoader
        Collection of train dataloaders specifying training samples.
    &#34;&#34;&#34;
    num_dataloaders = len(self.train_samplers)
    if num_dataloaders == 1:
        return self._get_dataloader(0)
    dataloaders = []
    for idx in range(num_dataloaders):
        dataloaders.append(self._get_dataloader(idx))
    return dataloaders</code></pre>
</details>
</dd>
<dt id="radio.data.validation.OneFoldValidation.val_dataloader"><code class="name flex">
<span>def <span class="ident">val_dataloader</span></span>(<span>self) â€‘>Â Union[torch.utils.data.dataloader.DataLoader,Â Sequence[torch.utils.data.dataloader.DataLoader]]</span>
</code></dt>
<dd>
<div class="desc"><p>Generates one or multiple Pytorch DataLoaders for validation.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>_</code></strong> :&ensp;<code>Collection</code> of <code>DataLoaders</code></dt>
<dd>Collection of validation dataloaders specifying validation samples.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def val_dataloader(self) -&gt; EvalDataLoaderType:
    &#34;&#34;&#34;
    Generates one or multiple Pytorch DataLoaders for validation.

    Returns
    -------
    _ : Collection of DataLoaders
        Collection of validation dataloaders specifying validation samples.
    &#34;&#34;&#34;
    num_dataloaders = len(self.val_samplers)
    if num_dataloaders == 1:
        return self._get_dataloader(0, train=False)
    dataloaders = []
    for idx in range(num_dataloaders):
        dataloaders.append(self._get_dataloader(idx, train=False))
    return dataloaders</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="radio.data" href="index.html">radio.data</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="radio.data.validation.KFoldValidation" href="#radio.data.validation.KFoldValidation">KFoldValidation</a></code></h4>
<ul class="">
<li><code><a title="radio.data.validation.KFoldValidation.setup" href="#radio.data.validation.KFoldValidation.setup">setup</a></code></li>
<li><code><a title="radio.data.validation.KFoldValidation.train_dataloader" href="#radio.data.validation.KFoldValidation.train_dataloader">train_dataloader</a></code></li>
<li><code><a title="radio.data.validation.KFoldValidation.val_dataloader" href="#radio.data.validation.KFoldValidation.val_dataloader">val_dataloader</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="radio.data.validation.OneFoldValidation" href="#radio.data.validation.OneFoldValidation">OneFoldValidation</a></code></h4>
<ul class="">
<li><code><a title="radio.data.validation.OneFoldValidation.setup" href="#radio.data.validation.OneFoldValidation.setup">setup</a></code></li>
<li><code><a title="radio.data.validation.OneFoldValidation.train_dataloader" href="#radio.data.validation.OneFoldValidation.train_dataloader">train_dataloader</a></code></li>
<li><code><a title="radio.data.validation.OneFoldValidation.val_dataloader" href="#radio.data.validation.OneFoldValidation.val_dataloader">val_dataloader</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>
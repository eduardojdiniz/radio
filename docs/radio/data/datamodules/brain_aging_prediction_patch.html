<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>radio.data.datamodules.brain_aging_prediction_patch API documentation</title>
<meta name="description" content="Based on BaseDataModule for managing data. A vision datamodule that is
shareable, reusable class that encapsulates all the steps needed to process
…" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>radio.data.datamodules.brain_aging_prediction_patch</code></h1>
</header>
<section id="section-intro">
<p>Based on BaseDataModule for managing data. A vision datamodule that is
shareable, reusable class that encapsulates all the steps needed to process
data, i.e., decoupling datasets from models to allow building dataset-agnostic
models. They also allow you to share a full dataset without explaining how to
download, split, transform, and process the data.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
# coding=utf-8
&#34;&#34;&#34;
Based on BaseDataModule for managing data. A vision datamodule that is
shareable, reusable class that encapsulates all the steps needed to process
data, i.e., decoupling datasets from models to allow building dataset-agnostic
models. They also allow you to share a full dataset without explaining how to
download, split, transform, and process the data.
&#34;&#34;&#34;

from typing import Any, Dict, List, Optional, Tuple, Union, cast
from pathlib import Path
from string import Template
import torchio as tio  # type: ignore
from radio.settings.pathutils import PathType
from ..datatypes import SpatialShapeType
from .brain_aging_prediction import BrainAgingPredictionDataModule

__all__ = [&#34;BrainAgingPredictionPatchDataModule&#34;]


class BrainAgingPredictionPatchDataModule(BrainAgingPredictionDataModule):
    &#34;&#34;&#34;
    Base class For making patch-based datasets which are compatible with
    torchvision.

    To create a subclass, you need to implement the following functions:

    A VisionPatchDataModule needs to implement 2 key methods +
    an optional __init__:
    &lt;__init__&gt;:
        (Optionally) Initialize the class, first call super.__init__().
    &lt;default_transforms&gt;:
        Default transforms to use in lieu of train_transforms, val_transforms,
        or test_transforms.
    &lt;teardown&gt;:
        Things to do on every accelerator in distributed mode when finished.

    Typical Workflow
    ----------------
    data = VisionPatchDataModule()
    data.prepare_data() # download
    data.setup(stage) # process and split
    data.teardown(stage) # clean-up

    Parameters
    ----------
    root : Path or str, optional
        Root to GPN&#39;s CEREBRO Studies folder.
        Default = ``&#39;/media/cerebro/Studies&#39;``.
    study : str, optional
        Study name. Default = ``&#39;Brain_Aging_Prediction&#39;``.
    data_dir : str, optional
        Subdirectory where the data is located.
        Default = ``&#39;Public/data&#39;``.
    step : str, optional
        Which processing step to use.
        Default = ``&#39;step01_structural_processing&#39;``.
    patch_size : int or (int, int, int)
        Tuple of integers ``(w, h, d)`` to generate patches of size ``w x h x
        d``. If a single number ``n`` is provided, ``w = h = d = n``.
    train_transforms : Callable, optional
        A function/transform that takes in a sample and returns a
        transformed version, e.g, ``torchvision.transforms.RandomCrop``.
    val_transforms : Callable, optional
        A function/transform that takes in a sample and returns a
        transformed version, e.g, ``torchvision.transforms.RandomCrop``.
    test_transforms : Callable, optional
        A function/transform that takes in a sample and returns a
        transformed version, e.g, ``torchvision.transforms.RandomCrop``.
    use_augmentation : bool, optional
        If ``True``, augment samples during the ``fit`` stage.
        Default = ``True``.
    use_preprocessing : bool, optional
        If ``True``, preprocess samples. Default = ``True``.
    resample : bool, optional
        If ``True``, resample all images to ``&#39;T1&#39;``. Default = ``False``.
    probability_map : str, optional
        Name of the image in the input subject that will be used as a sampling
        probability map.  The probability of sampling a patch centered on a
        specific voxel is the value of that voxel in the probability map. The
        probabilities need not be normalized. For example, voxels can have
        values 0, 1 and 5. Voxels with value 0 will never be at the center of a
        patch. Voxels with value 5 will have 5 times more chance of being at
        the center of a patch that voxels with a value of 1. If ``None``,
        uniform sampling is used. Default = ``None``.
    label_name : str, optional
        Name of the label image in the subject that will be used to generate
        the sampling probability map. If ``None`` and ``probability_map`` is
        ``None``, the first image of type ``torchio.LABEL`` found in the
        subject subject will be used. If ``probability_map`` is not ``None``,
        then ``label_name`` and ``label_probability`` are ignored.
        Default = ``None``.
    label_probabilities : Dict[int, float], optional
        Dictionary containing the probability that each class will be sampled.
        Probabilities do not need to be normalized. For example, a value of
        {0: 0, 1: 2, 2: 1, 3: 1} will create a sampler whose patches centers
        will have 50% probability of being labeled as 1, 25% of being 2 and 25%
        of being 3. If None, the label map is binarized and the value is set to
        {0: 0, 1: 1}. If the input has multiple channels, a value of
        {0: 0, 1: 2, 2: 1, 3: 1} will create a sampler whose patches centers
        will have 50% probability of being taken from a non zero value of
        channel 1, 25% from channel 2 and 25% from channel 3. If
        ``probability_map`` is not ``None``, then ``label_name`` and
        ``label_probability`` are ignored. Default = ``None``.
    queue_max_length : int, optional
        Maximum number of patches that can be stored in the queue. Using a
        large number means that the queue needs to be filled less often, but
        more CPU memory is needed to store the patches. Default = ``256``.
    samples_per_volume : int, optional
        Number of patches to extract from each volume. A small number of
        patches ensures a large variability in the queue, but training will be
        slower. Default = ``16``.
    batch_size : int, optional
        How many patches per batch to load. Default = ``32``.
    shuffle_subjects : bool, optional
        Whether to shuffle the subjects dataset at the beginning of every epoch
        (an epoch ends when all the patches from all the subjects have been
        processed). Default = ``True``.
    shuffle_patches : bool, optional
        Whether to shuffle the patches queue at the beginning of every epoch.
        Default = ``True``.
    num_workers : int, optional
        How many subprocesses to use for data loading. ``0`` means that the
        data will be loaded in the main process. Default: ``0``.
    pin_memory : bool, optional
        If ``True``, the data loader will copy Tensors into CUDA pinned memory
        before returning them.
    start_background : bool, optional
        If ``True``, the loader will start working in the background as soon as
        the queues are instantiated. Default = ``True``.
    drop_last : bool, optional
        Set to ``True`` to drop the last incomplete batch, if the dataset size
        is not divisible by the batch size. If ``False`` and the size of
        dataset is not divisible by the batch size, then the last batch will be
        smaller. Default = ``False``.
    num_folds : int, optional
        Number of folds. Must be at least ``2``. ``2`` corresponds to a single
        train/validation split. Default = ``2``.
    val_split: int or float, optional
        If ``num_folds = 2``, then ``val_split`` specify how the
        train_dataset should be split into train/validation datasets. If
        ``num_folds &gt; 2``, then it is not used. Default = ``0.2``.
    intensities : List[str], optional
        Which intensities to load. Default = ``[&#39;T1&#39;]``.
    labels : List[str], optional
        Which labels to load. Default = ``[]``.
    dims : Tuple[int, int, int], optional
        Max spatial dimensions across subjects&#39; images. If ``None``, compute
        dimensions from dataset. Default = ``(160, 192, 160)``.
    seed : int, optional
        When `shuffle` is True, `seed` affects the ordering of the indices,
        which controls the randomness of each fold. It is also use to seed the
        RNG used by RandomSampler to generate random indexes and
        multiprocessing to generate `base_seed` for workers. Pass an int for
        reproducible output across multiple function calls. Default = ``41``.
    verbose : bool, optional
        If ``True``, print debugging messages. Default = ``False``.
    &#34;&#34;&#34;

    #: Extra arguments for dataset_cls instantiation.
    EXTRA_ARGS: dict = {}
    #: Dataset class to use. E.g., torchvision.datasets.MNIST
    dataset_cls = tio.SubjectsDataset
    #: A tuple describing the shape of the data
    dims: Optional[Tuple[int, int, int]]
    #: Dataset name
    name: str = &#34;brain_aging_prediction&#34;
    intensity2template = {
        &#34;T1&#34;: Template(&#39;wstrip_m${subj_id}_${scan_id}_T1.nii&#39;),
        &#34;FLAIR&#34;: Template(&#39;wstrip_mr${subj_id}_${scan_id}_FLAIR.nii&#39;),
    }
    label2template: Dict[str, Template] = {}

    def __init__(
        self,
        *args: Any,
        root: PathType = Path(&#39;/media/cerebro&#39;),
        study: str = &#39;Brain_Aging_Prediction&#39;,
        data_dir: str = &#39;Public/data&#39;,
        step: str = &#39;step01_structural_processing&#39;,
        train_transforms: Optional[tio.Transform] = None,
        val_transforms: Optional[tio.Transform] = None,
        test_transforms: Optional[tio.Transform] = None,
        use_augmentation: bool = True,
        use_preprocessing: bool = True,
        resample: bool = False,
        patch_size: SpatialShapeType = 96,
        probability_map: Optional[str] = None,
        label_name: Optional[str] = None,
        label_probabilities: Optional[Dict[int, float]] = None,
        queue_max_length: int = 256,
        samples_per_volume: int = 16,
        batch_size: int = 32,
        shuffle_subjects: bool = True,
        shuffle_patches: bool = True,
        num_workers: int = 0,
        pin_memory: bool = True,
        start_background: bool = True,
        drop_last: bool = False,
        num_folds: int = 2,
        val_split: Union[int, float] = 0.2,
        intensities: Optional[List[str]] = None,
        labels: Optional[List[str]] = None,
        dims: Tuple[int, int, int] = (160, 192, 160),
        seed: int = 41,
        verbose: bool = False,
        **kwargs: Any,
    ) -&gt; None:
        super().__init__(
            *args,
            root=root,
            study=study,
            data_dir=data_dir,
            step=step,
            train_transforms=train_transforms,
            val_transforms=val_transforms,
            test_transforms=test_transforms,
            use_augmentation=use_augmentation,
            use_preprocessing=use_preprocessing,
            resample=resample,
            batch_size=batch_size,
            shuffle=shuffle_subjects,
            num_workers=num_workers,
            pin_memory=pin_memory,
            drop_last=drop_last,
            num_folds=num_folds,
            val_split=val_split,
            intensities=intensities,
            labels=labels,
            dims=dims,
            seed=seed,
            verbose=verbose,
            **kwargs,
        )
        self.train_sampler: tio.data.sampler.sampler.PatchSampler

        # Init Train Sampler
        both_something = probability_map is not None and label_name is not None
        if both_something:
            raise ValueError(
                &#34;Both &#39;probability_map&#39; and &#39;label_name&#39; cannot be not None &#34;,
                &#34;at the same time&#34;,
            )
        if probability_map is None and label_name is None:
            self.train_sampler = tio.UniformSampler(patch_size)
        elif probability_map is not None:
            self.train_sampler = tio.WeightedSampler(patch_size,
                                                     probability_map)
        else:
            self.train_sampler = tio.LabelSampler(patch_size, label_name,
                                                  label_probabilities)

        self.probability_map = probability_map
        self.label_name = label_name
        self.label_probabilities = label_probabilities

        # Queue parameters
        self.train_queue: tio.Queue
        self.val_queue: tio.Queue
        self.queue_max_length = queue_max_length
        self.samples_per_volume = samples_per_volume
        self.shuffle_subjects = shuffle_subjects
        self.shuffle_patches = shuffle_patches
        self.start_background = start_background

    def setup(self, stage: Optional[str] = None) -&gt; None:
        &#34;&#34;&#34;
        Creates train, validation and test collection of samplers.

        Parameters
        ----------
        stage: Optional[str]
            Either ``&#39;fit``, ``&#39;validate&#39;``, or ``&#39;test&#39;``.
            If stage = ``None``, set-up all stages. Default = ``None``.
        &#34;&#34;&#34;
        if stage == &#34;fit&#34; or stage is None:
            train_transforms = self.default_transforms(
                stage=&#34;fit&#34;
            ) if self.train_transforms is None else self.train_transforms

            val_transforms = self.default_transforms(
                stage=&#34;fit&#34;
            ) if self.val_transforms is None else self.val_transforms

            if not self.has_train_val_split:
                train_subjects = self.get_subjects(fold=&#34;train&#34;)
                train_dataset = self.dataset_cls(
                    train_subjects,
                    transform=train_transforms,
                )
                val_dataset = self.dataset_cls(
                    train_subjects,
                    transform=val_transforms,
                )
                self.train_queue = tio.Queue(
                    cast(tio.SubjectsDataset, train_dataset),
                    max_length=self.queue_max_length,
                    samples_per_volume=self.samples_per_volume,
                    sampler=self.train_sampler,
                    num_workers=self.num_workers,
                    shuffle_subjects=self.shuffle_subjects,
                    shuffle_patches=self.shuffle_patches,
                    start_background=self.start_background,
                    verbose=self.verbose)

                self.val_queue = tio.Queue(
                    cast(tio.SubjectsDataset, val_dataset),
                    max_length=self.queue_max_length,
                    samples_per_volume=self.samples_per_volume,
                    sampler=self.train_sampler,
                    num_workers=self.num_workers,
                    shuffle_subjects=self.shuffle_subjects,
                    shuffle_patches=self.shuffle_patches,
                    start_background=self.start_background,
                    verbose=self.verbose)

                self.validation = self.val_cls(
                    train_dataset=self.train_queue,
                    val_dataset=self.val_queue,
                    batch_size=self.batch_size,
                    shuffle=False,
                    num_workers=0,
                    pin_memory=self.pin_memory,
                    drop_last=self.drop_last,
                    num_folds=self.num_folds,
                    seed=self.seed,
                )
                self.validation.setup(self.val_split)
                self.has_validation = True
                self.train_dataset = self.train_queue
                self.size_train = self.size_train_dataset(
                    self.validation.train_samplers)
                self.val_dataset = self.val_queue
                self.size_val = self.size_eval_dataset(
                    self.validation.val_samplers)
            else:
                train_subjects = self.get_subjects(fold=&#34;train&#34;)
                train_dataset = self.dataset_cls(train_subjects,
                                                 transform=train_transforms)
                self.train_queue = tio.Queue(
                    cast(tio.SubjectsDataset, train_dataset),
                    max_length=self.queue_max_length,
                    samples_per_volume=self.samples_per_volume,
                    sampler=self.train_sampler,
                    num_workers=self.num_workers,
                    shuffle_subjects=self.shuffle_subjects,
                    shuffle_patches=self.shuffle_patches,
                    start_background=self.start_background,
                    verbose=self.verbose)

                val_subjects = self.get_subjects(fold=&#34;val&#34;)
                val_dataset = self.dataset_cls(val_subjects,
                                               transform=val_transforms)
                self.train_dataset = self.train_queue
                self.size_train = self.size_train_dataset(self.train_dataset)

                self.val_queue = tio.Queue(
                    cast(tio.SubjectsDataset, val_dataset),
                    max_length=self.queue_max_length,
                    samples_per_volume=self.samples_per_volume,
                    sampler=self.train_sampler,
                    num_workers=self.num_workers,
                    shuffle_subjects=self.shuffle_subjects,
                    shuffle_patches=self.shuffle_patches,
                    start_background=self.start_background,
                    verbose=self.verbose)
                self.val_dataset = self.val_queue
                self.size_val = self.size_eval_dataset(self.val_dataset)

        if stage == &#34;test&#34; or stage is None:
            test_transforms = self.default_transforms(
                stage=&#34;test&#34;
            ) if self.test_transforms is None else self.test_transforms
            test_subjects = self.get_subjects(fold=&#34;test&#34;)
            self.test_dataset = self.dataset_cls(test_subjects,
                                                 transform=test_transforms)
            self.size_test = self.size_eval_dataset(self.test_dataset)

    def get_preprocessing_transforms(
        self,
        shape: Optional[Tuple[int, int, int]] = None,
        resample: bool = False,
    ) -&gt; tio.Transform:
        &#34;&#34;&#34;
        Get preprocessing transorms to apply to all subjects.

        Returns
        -------
        preprocess : tio.Transform
            All preprocessing steps that should be applied to all subjects.
        &#34;&#34;&#34;
        preprocess_list: List[tio.transforms.Transform] = []

        # Use standard orientation for all images
        preprocess_list.append(tio.ToCanonical())

        # If true, resample to T1
        if resample:
            preprocess_list.append(tio.Resample(&#39;T1&#39;))

        if shape is None:
            train_subjects = self.get_subjects(fold=&#34;train&#34;)
            test_subjects = self.get_subjects(fold=&#34;test&#34;)
            shape = self.get_max_shape(train_subjects + test_subjects)
        else:
            shape = self.dims

        preprocess_list.extend([
            tio.RescaleIntensity((-1, 1)),
            tio.CropOrPad(shape),
            tio.EnsureShapeMultiple(8),  # for the U-Net
            tio.OneHot()
        ])

        return tio.Compose(preprocess_list)

    @staticmethod
    def get_augmentation_transforms() -&gt; tio.Transform:
        &#34;&#34;&#34;&#34;
        Get augmentation transorms to apply to subjects during training.

        Returns
        -------
        augment : tio.Transform
            All augmentation steps that should be applied to subjects during
            training.
        &#34;&#34;&#34;
        augment = tio.Compose([
            tio.RandomAffine(),
            tio.RandomGamma(p=0.5),
            tio.RandomNoise(p=0.5),
            tio.RandomMotion(p=0.1),
            tio.RandomBiasField(p=0.25),
        ])
        return augment

    def train_dataloader(self, *args, **kwargs):
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for train.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of train dataloaders specifying training samples.
        &#34;&#34;&#34;
        return super().train_dataloader(num_workers=0, shuffle=False)

    def val_dataloader(self, *args, **kwargs):
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for validation.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of validation dataloaders specifying validation samples.
        &#34;&#34;&#34;
        return super().val_dataloader(num_workers=0, shuffle=False)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="radio.data.datamodules.brain_aging_prediction_patch.BrainAgingPredictionPatchDataModule"><code class="flex name class">
<span>class <span class="ident">BrainAgingPredictionPatchDataModule</span></span>
<span>(</span><span>*args: Any, root: Union[str, pathlib.Path] = PosixPath('/media/cerebro'), study: str = 'Brain_Aging_Prediction', data_dir: str = 'Public/data', step: str = 'step01_structural_processing', train_transforms: Optional[torchio.transforms.transform.Transform] = None, val_transforms: Optional[torchio.transforms.transform.Transform] = None, test_transforms: Optional[torchio.transforms.transform.Transform] = None, use_augmentation: bool = True, use_preprocessing: bool = True, resample: bool = False, patch_size: Union[int, Tuple[int, int, int]] = 96, probability_map: Optional[str] = None, label_name: Optional[str] = None, label_probabilities: Optional[Dict[int, float]] = None, queue_max_length: int = 256, samples_per_volume: int = 16, batch_size: int = 32, shuffle_subjects: bool = True, shuffle_patches: bool = True, num_workers: int = 0, pin_memory: bool = True, start_background: bool = True, drop_last: bool = False, num_folds: int = 2, val_split: Union[int, float] = 0.2, intensities: Optional[List[str]] = None, labels: Optional[List[str]] = None, dims: Tuple[int, int, int] = (160, 192, 160), seed: int = 41, verbose: bool = False, **kwargs: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class For making patch-based datasets which are compatible with
torchvision.</p>
<p>To create a subclass, you need to implement the following functions:</p>
<p>A VisionPatchDataModule needs to implement 2 key methods +
an optional <strong>init</strong>:
&lt;<strong>init</strong>&gt;:
(Optionally) Initialize the class, first call super.<strong>init</strong>().
<default_transforms>:
Default transforms to use in lieu of train_transforms, val_transforms,
or test_transforms.
<teardown>:
Things to do on every accelerator in distributed mode when finished.</p>
<h2 id="typical-workflow">Typical Workflow</h2>
<p>data = VisionPatchDataModule()
data.prepare_data() # download
data.setup(stage) # process and split
data.teardown(stage) # clean-up</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>root</code></strong> :&ensp;<code>Path</code> or <code>str</code>, optional</dt>
<dd>Root to GPN's CEREBRO Studies folder.
Default = <code>'/media/cerebro/Studies'</code>.</dd>
<dt><strong><code>study</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Study name. Default = <code>'Brain_Aging_Prediction'</code>.</dd>
<dt><strong><code>data_dir</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Subdirectory where the data is located.
Default = <code>'Public/data'</code>.</dd>
<dt><strong><code>step</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Which processing step to use.
Default = <code>'step01_structural_processing'</code>.</dd>
<dt><strong><code>patch_size</code></strong> :&ensp;<code>int</code> or <code>(int, int, int)</code></dt>
<dd>Tuple of integers <code>(w, h, d)</code> to generate patches of size <code>w x h x
d&lt;code&gt;. If a single number &lt;/code&gt;n&lt;code&gt; is provided, &lt;/code&gt;w = h = d = n</code>.</dd>
<dt><strong><code>train_transforms</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>A function/transform that takes in a sample and returns a
transformed version, e.g, <code>torchvision.transforms.RandomCrop</code>.</dd>
<dt><strong><code>val_transforms</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>A function/transform that takes in a sample and returns a
transformed version, e.g, <code>torchvision.transforms.RandomCrop</code>.</dd>
<dt><strong><code>test_transforms</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>A function/transform that takes in a sample and returns a
transformed version, e.g, <code>torchvision.transforms.RandomCrop</code>.</dd>
<dt><strong><code>use_augmentation</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, augment samples during the <code>fit</code> stage.
Default = <code>True</code>.</dd>
<dt><strong><code>use_preprocessing</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, preprocess samples. Default = <code>True</code>.</dd>
<dt><strong><code>resample</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, resample all images to <code>'T1'</code>. Default = <code>False</code>.</dd>
<dt><strong><code>probability_map</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of the image in the input subject that will be used as a sampling
probability map.
The probability of sampling a patch centered on a
specific voxel is the value of that voxel in the probability map. The
probabilities need not be normalized. For example, voxels can have
values 0, 1 and 5. Voxels with value 0 will never be at the center of a
patch. Voxels with value 5 will have 5 times more chance of being at
the center of a patch that voxels with a value of 1. If <code>None</code>,
uniform sampling is used. Default = <code>None</code>.</dd>
<dt><strong><code>label_name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of the label image in the subject that will be used to generate
the sampling probability map. If <code>None</code> and <code>probability_map</code> is
<code>None</code>, the first image of type <code>torchio.LABEL</code> found in the
subject subject will be used. If <code>probability_map</code> is not <code>None</code>,
then <code>label_name</code> and <code>label_probability</code> are ignored.
Default = <code>None</code>.</dd>
<dt><strong><code>label_probabilities</code></strong> :&ensp;<code>Dict[int, float]</code>, optional</dt>
<dd>Dictionary containing the probability that each class will be sampled.
Probabilities do not need to be normalized. For example, a value of
{0: 0, 1: 2, 2: 1, 3: 1} will create a sampler whose patches centers
will have 50% probability of being labeled as 1, 25% of being 2 and 25%
of being 3. If None, the label map is binarized and the value is set to
{0: 0, 1: 1}. If the input has multiple channels, a value of
{0: 0, 1: 2, 2: 1, 3: 1} will create a sampler whose patches centers
will have 50% probability of being taken from a non zero value of
channel 1, 25% from channel 2 and 25% from channel 3. If
<code>probability_map</code> is not <code>None</code>, then <code>label_name</code> and
<code>label_probability</code> are ignored. Default = <code>None</code>.</dd>
<dt><strong><code>queue_max_length</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Maximum number of patches that can be stored in the queue. Using a
large number means that the queue needs to be filled less often, but
more CPU memory is needed to store the patches. Default = <code>256</code>.</dd>
<dt><strong><code>samples_per_volume</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of patches to extract from each volume. A small number of
patches ensures a large variability in the queue, but training will be
slower. Default = <code>16</code>.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>How many patches per batch to load. Default = <code>32</code>.</dd>
<dt><strong><code>shuffle_subjects</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to shuffle the subjects dataset at the beginning of every epoch
(an epoch ends when all the patches from all the subjects have been
processed). Default = <code>True</code>.</dd>
<dt><strong><code>shuffle_patches</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to shuffle the patches queue at the beginning of every epoch.
Default = <code>True</code>.</dd>
<dt><strong><code>num_workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>How many subprocesses to use for data loading. <code>0</code> means that the
data will be loaded in the main process. Default: <code>0</code>.</dd>
<dt><strong><code>pin_memory</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, the data loader will copy Tensors into CUDA pinned memory
before returning them.</dd>
<dt><strong><code>start_background</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, the loader will start working in the background as soon as
the queues are instantiated. Default = <code>True</code>.</dd>
<dt><strong><code>drop_last</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Set to <code>True</code> to drop the last incomplete batch, if the dataset size
is not divisible by the batch size. If <code>False</code> and the size of
dataset is not divisible by the batch size, then the last batch will be
smaller. Default = <code>False</code>.</dd>
<dt><strong><code>num_folds</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of folds. Must be at least <code>2</code>. <code>2</code> corresponds to a single
train/validation split. Default = <code>2</code>.</dd>
<dt><strong><code>val_split</code></strong> :&ensp;<code>int</code> or <code>float</code>, optional</dt>
<dd>If <code>num_folds = 2</code>, then <code>val_split</code> specify how the
train_dataset should be split into train/validation datasets. If
<code>num_folds &gt; 2</code>, then it is not used. Default = <code>0.2</code>.</dd>
<dt><strong><code>intensities</code></strong> :&ensp;<code>List[str]</code>, optional</dt>
<dd>Which intensities to load. Default = <code>['T1']</code>.</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>List[str]</code>, optional</dt>
<dd>Which labels to load. Default = <code>[]</code>.</dd>
<dt><strong><code>dims</code></strong> :&ensp;<code>Tuple[int, int, int]</code>, optional</dt>
<dd>Max spatial dimensions across subjects' images. If <code>None</code>, compute
dimensions from dataset. Default = <code>(160, 192, 160)</code>.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>When <code>shuffle</code> is True, <code>seed</code> affects the ordering of the indices,
which controls the randomness of each fold. It is also use to seed the
RNG used by RandomSampler to generate random indexes and
multiprocessing to generate <code>base_seed</code> for workers. Pass an int for
reproducible output across multiple function calls. Default = <code>41</code>.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, print debugging messages. Default = <code>False</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BrainAgingPredictionPatchDataModule(BrainAgingPredictionDataModule):
    &#34;&#34;&#34;
    Base class For making patch-based datasets which are compatible with
    torchvision.

    To create a subclass, you need to implement the following functions:

    A VisionPatchDataModule needs to implement 2 key methods +
    an optional __init__:
    &lt;__init__&gt;:
        (Optionally) Initialize the class, first call super.__init__().
    &lt;default_transforms&gt;:
        Default transforms to use in lieu of train_transforms, val_transforms,
        or test_transforms.
    &lt;teardown&gt;:
        Things to do on every accelerator in distributed mode when finished.

    Typical Workflow
    ----------------
    data = VisionPatchDataModule()
    data.prepare_data() # download
    data.setup(stage) # process and split
    data.teardown(stage) # clean-up

    Parameters
    ----------
    root : Path or str, optional
        Root to GPN&#39;s CEREBRO Studies folder.
        Default = ``&#39;/media/cerebro/Studies&#39;``.
    study : str, optional
        Study name. Default = ``&#39;Brain_Aging_Prediction&#39;``.
    data_dir : str, optional
        Subdirectory where the data is located.
        Default = ``&#39;Public/data&#39;``.
    step : str, optional
        Which processing step to use.
        Default = ``&#39;step01_structural_processing&#39;``.
    patch_size : int or (int, int, int)
        Tuple of integers ``(w, h, d)`` to generate patches of size ``w x h x
        d``. If a single number ``n`` is provided, ``w = h = d = n``.
    train_transforms : Callable, optional
        A function/transform that takes in a sample and returns a
        transformed version, e.g, ``torchvision.transforms.RandomCrop``.
    val_transforms : Callable, optional
        A function/transform that takes in a sample and returns a
        transformed version, e.g, ``torchvision.transforms.RandomCrop``.
    test_transforms : Callable, optional
        A function/transform that takes in a sample and returns a
        transformed version, e.g, ``torchvision.transforms.RandomCrop``.
    use_augmentation : bool, optional
        If ``True``, augment samples during the ``fit`` stage.
        Default = ``True``.
    use_preprocessing : bool, optional
        If ``True``, preprocess samples. Default = ``True``.
    resample : bool, optional
        If ``True``, resample all images to ``&#39;T1&#39;``. Default = ``False``.
    probability_map : str, optional
        Name of the image in the input subject that will be used as a sampling
        probability map.  The probability of sampling a patch centered on a
        specific voxel is the value of that voxel in the probability map. The
        probabilities need not be normalized. For example, voxels can have
        values 0, 1 and 5. Voxels with value 0 will never be at the center of a
        patch. Voxels with value 5 will have 5 times more chance of being at
        the center of a patch that voxels with a value of 1. If ``None``,
        uniform sampling is used. Default = ``None``.
    label_name : str, optional
        Name of the label image in the subject that will be used to generate
        the sampling probability map. If ``None`` and ``probability_map`` is
        ``None``, the first image of type ``torchio.LABEL`` found in the
        subject subject will be used. If ``probability_map`` is not ``None``,
        then ``label_name`` and ``label_probability`` are ignored.
        Default = ``None``.
    label_probabilities : Dict[int, float], optional
        Dictionary containing the probability that each class will be sampled.
        Probabilities do not need to be normalized. For example, a value of
        {0: 0, 1: 2, 2: 1, 3: 1} will create a sampler whose patches centers
        will have 50% probability of being labeled as 1, 25% of being 2 and 25%
        of being 3. If None, the label map is binarized and the value is set to
        {0: 0, 1: 1}. If the input has multiple channels, a value of
        {0: 0, 1: 2, 2: 1, 3: 1} will create a sampler whose patches centers
        will have 50% probability of being taken from a non zero value of
        channel 1, 25% from channel 2 and 25% from channel 3. If
        ``probability_map`` is not ``None``, then ``label_name`` and
        ``label_probability`` are ignored. Default = ``None``.
    queue_max_length : int, optional
        Maximum number of patches that can be stored in the queue. Using a
        large number means that the queue needs to be filled less often, but
        more CPU memory is needed to store the patches. Default = ``256``.
    samples_per_volume : int, optional
        Number of patches to extract from each volume. A small number of
        patches ensures a large variability in the queue, but training will be
        slower. Default = ``16``.
    batch_size : int, optional
        How many patches per batch to load. Default = ``32``.
    shuffle_subjects : bool, optional
        Whether to shuffle the subjects dataset at the beginning of every epoch
        (an epoch ends when all the patches from all the subjects have been
        processed). Default = ``True``.
    shuffle_patches : bool, optional
        Whether to shuffle the patches queue at the beginning of every epoch.
        Default = ``True``.
    num_workers : int, optional
        How many subprocesses to use for data loading. ``0`` means that the
        data will be loaded in the main process. Default: ``0``.
    pin_memory : bool, optional
        If ``True``, the data loader will copy Tensors into CUDA pinned memory
        before returning them.
    start_background : bool, optional
        If ``True``, the loader will start working in the background as soon as
        the queues are instantiated. Default = ``True``.
    drop_last : bool, optional
        Set to ``True`` to drop the last incomplete batch, if the dataset size
        is not divisible by the batch size. If ``False`` and the size of
        dataset is not divisible by the batch size, then the last batch will be
        smaller. Default = ``False``.
    num_folds : int, optional
        Number of folds. Must be at least ``2``. ``2`` corresponds to a single
        train/validation split. Default = ``2``.
    val_split: int or float, optional
        If ``num_folds = 2``, then ``val_split`` specify how the
        train_dataset should be split into train/validation datasets. If
        ``num_folds &gt; 2``, then it is not used. Default = ``0.2``.
    intensities : List[str], optional
        Which intensities to load. Default = ``[&#39;T1&#39;]``.
    labels : List[str], optional
        Which labels to load. Default = ``[]``.
    dims : Tuple[int, int, int], optional
        Max spatial dimensions across subjects&#39; images. If ``None``, compute
        dimensions from dataset. Default = ``(160, 192, 160)``.
    seed : int, optional
        When `shuffle` is True, `seed` affects the ordering of the indices,
        which controls the randomness of each fold. It is also use to seed the
        RNG used by RandomSampler to generate random indexes and
        multiprocessing to generate `base_seed` for workers. Pass an int for
        reproducible output across multiple function calls. Default = ``41``.
    verbose : bool, optional
        If ``True``, print debugging messages. Default = ``False``.
    &#34;&#34;&#34;

    #: Extra arguments for dataset_cls instantiation.
    EXTRA_ARGS: dict = {}
    #: Dataset class to use. E.g., torchvision.datasets.MNIST
    dataset_cls = tio.SubjectsDataset
    #: A tuple describing the shape of the data
    dims: Optional[Tuple[int, int, int]]
    #: Dataset name
    name: str = &#34;brain_aging_prediction&#34;
    intensity2template = {
        &#34;T1&#34;: Template(&#39;wstrip_m${subj_id}_${scan_id}_T1.nii&#39;),
        &#34;FLAIR&#34;: Template(&#39;wstrip_mr${subj_id}_${scan_id}_FLAIR.nii&#39;),
    }
    label2template: Dict[str, Template] = {}

    def __init__(
        self,
        *args: Any,
        root: PathType = Path(&#39;/media/cerebro&#39;),
        study: str = &#39;Brain_Aging_Prediction&#39;,
        data_dir: str = &#39;Public/data&#39;,
        step: str = &#39;step01_structural_processing&#39;,
        train_transforms: Optional[tio.Transform] = None,
        val_transforms: Optional[tio.Transform] = None,
        test_transforms: Optional[tio.Transform] = None,
        use_augmentation: bool = True,
        use_preprocessing: bool = True,
        resample: bool = False,
        patch_size: SpatialShapeType = 96,
        probability_map: Optional[str] = None,
        label_name: Optional[str] = None,
        label_probabilities: Optional[Dict[int, float]] = None,
        queue_max_length: int = 256,
        samples_per_volume: int = 16,
        batch_size: int = 32,
        shuffle_subjects: bool = True,
        shuffle_patches: bool = True,
        num_workers: int = 0,
        pin_memory: bool = True,
        start_background: bool = True,
        drop_last: bool = False,
        num_folds: int = 2,
        val_split: Union[int, float] = 0.2,
        intensities: Optional[List[str]] = None,
        labels: Optional[List[str]] = None,
        dims: Tuple[int, int, int] = (160, 192, 160),
        seed: int = 41,
        verbose: bool = False,
        **kwargs: Any,
    ) -&gt; None:
        super().__init__(
            *args,
            root=root,
            study=study,
            data_dir=data_dir,
            step=step,
            train_transforms=train_transforms,
            val_transforms=val_transforms,
            test_transforms=test_transforms,
            use_augmentation=use_augmentation,
            use_preprocessing=use_preprocessing,
            resample=resample,
            batch_size=batch_size,
            shuffle=shuffle_subjects,
            num_workers=num_workers,
            pin_memory=pin_memory,
            drop_last=drop_last,
            num_folds=num_folds,
            val_split=val_split,
            intensities=intensities,
            labels=labels,
            dims=dims,
            seed=seed,
            verbose=verbose,
            **kwargs,
        )
        self.train_sampler: tio.data.sampler.sampler.PatchSampler

        # Init Train Sampler
        both_something = probability_map is not None and label_name is not None
        if both_something:
            raise ValueError(
                &#34;Both &#39;probability_map&#39; and &#39;label_name&#39; cannot be not None &#34;,
                &#34;at the same time&#34;,
            )
        if probability_map is None and label_name is None:
            self.train_sampler = tio.UniformSampler(patch_size)
        elif probability_map is not None:
            self.train_sampler = tio.WeightedSampler(patch_size,
                                                     probability_map)
        else:
            self.train_sampler = tio.LabelSampler(patch_size, label_name,
                                                  label_probabilities)

        self.probability_map = probability_map
        self.label_name = label_name
        self.label_probabilities = label_probabilities

        # Queue parameters
        self.train_queue: tio.Queue
        self.val_queue: tio.Queue
        self.queue_max_length = queue_max_length
        self.samples_per_volume = samples_per_volume
        self.shuffle_subjects = shuffle_subjects
        self.shuffle_patches = shuffle_patches
        self.start_background = start_background

    def setup(self, stage: Optional[str] = None) -&gt; None:
        &#34;&#34;&#34;
        Creates train, validation and test collection of samplers.

        Parameters
        ----------
        stage: Optional[str]
            Either ``&#39;fit``, ``&#39;validate&#39;``, or ``&#39;test&#39;``.
            If stage = ``None``, set-up all stages. Default = ``None``.
        &#34;&#34;&#34;
        if stage == &#34;fit&#34; or stage is None:
            train_transforms = self.default_transforms(
                stage=&#34;fit&#34;
            ) if self.train_transforms is None else self.train_transforms

            val_transforms = self.default_transforms(
                stage=&#34;fit&#34;
            ) if self.val_transforms is None else self.val_transforms

            if not self.has_train_val_split:
                train_subjects = self.get_subjects(fold=&#34;train&#34;)
                train_dataset = self.dataset_cls(
                    train_subjects,
                    transform=train_transforms,
                )
                val_dataset = self.dataset_cls(
                    train_subjects,
                    transform=val_transforms,
                )
                self.train_queue = tio.Queue(
                    cast(tio.SubjectsDataset, train_dataset),
                    max_length=self.queue_max_length,
                    samples_per_volume=self.samples_per_volume,
                    sampler=self.train_sampler,
                    num_workers=self.num_workers,
                    shuffle_subjects=self.shuffle_subjects,
                    shuffle_patches=self.shuffle_patches,
                    start_background=self.start_background,
                    verbose=self.verbose)

                self.val_queue = tio.Queue(
                    cast(tio.SubjectsDataset, val_dataset),
                    max_length=self.queue_max_length,
                    samples_per_volume=self.samples_per_volume,
                    sampler=self.train_sampler,
                    num_workers=self.num_workers,
                    shuffle_subjects=self.shuffle_subjects,
                    shuffle_patches=self.shuffle_patches,
                    start_background=self.start_background,
                    verbose=self.verbose)

                self.validation = self.val_cls(
                    train_dataset=self.train_queue,
                    val_dataset=self.val_queue,
                    batch_size=self.batch_size,
                    shuffle=False,
                    num_workers=0,
                    pin_memory=self.pin_memory,
                    drop_last=self.drop_last,
                    num_folds=self.num_folds,
                    seed=self.seed,
                )
                self.validation.setup(self.val_split)
                self.has_validation = True
                self.train_dataset = self.train_queue
                self.size_train = self.size_train_dataset(
                    self.validation.train_samplers)
                self.val_dataset = self.val_queue
                self.size_val = self.size_eval_dataset(
                    self.validation.val_samplers)
            else:
                train_subjects = self.get_subjects(fold=&#34;train&#34;)
                train_dataset = self.dataset_cls(train_subjects,
                                                 transform=train_transforms)
                self.train_queue = tio.Queue(
                    cast(tio.SubjectsDataset, train_dataset),
                    max_length=self.queue_max_length,
                    samples_per_volume=self.samples_per_volume,
                    sampler=self.train_sampler,
                    num_workers=self.num_workers,
                    shuffle_subjects=self.shuffle_subjects,
                    shuffle_patches=self.shuffle_patches,
                    start_background=self.start_background,
                    verbose=self.verbose)

                val_subjects = self.get_subjects(fold=&#34;val&#34;)
                val_dataset = self.dataset_cls(val_subjects,
                                               transform=val_transforms)
                self.train_dataset = self.train_queue
                self.size_train = self.size_train_dataset(self.train_dataset)

                self.val_queue = tio.Queue(
                    cast(tio.SubjectsDataset, val_dataset),
                    max_length=self.queue_max_length,
                    samples_per_volume=self.samples_per_volume,
                    sampler=self.train_sampler,
                    num_workers=self.num_workers,
                    shuffle_subjects=self.shuffle_subjects,
                    shuffle_patches=self.shuffle_patches,
                    start_background=self.start_background,
                    verbose=self.verbose)
                self.val_dataset = self.val_queue
                self.size_val = self.size_eval_dataset(self.val_dataset)

        if stage == &#34;test&#34; or stage is None:
            test_transforms = self.default_transforms(
                stage=&#34;test&#34;
            ) if self.test_transforms is None else self.test_transforms
            test_subjects = self.get_subjects(fold=&#34;test&#34;)
            self.test_dataset = self.dataset_cls(test_subjects,
                                                 transform=test_transforms)
            self.size_test = self.size_eval_dataset(self.test_dataset)

    def get_preprocessing_transforms(
        self,
        shape: Optional[Tuple[int, int, int]] = None,
        resample: bool = False,
    ) -&gt; tio.Transform:
        &#34;&#34;&#34;
        Get preprocessing transorms to apply to all subjects.

        Returns
        -------
        preprocess : tio.Transform
            All preprocessing steps that should be applied to all subjects.
        &#34;&#34;&#34;
        preprocess_list: List[tio.transforms.Transform] = []

        # Use standard orientation for all images
        preprocess_list.append(tio.ToCanonical())

        # If true, resample to T1
        if resample:
            preprocess_list.append(tio.Resample(&#39;T1&#39;))

        if shape is None:
            train_subjects = self.get_subjects(fold=&#34;train&#34;)
            test_subjects = self.get_subjects(fold=&#34;test&#34;)
            shape = self.get_max_shape(train_subjects + test_subjects)
        else:
            shape = self.dims

        preprocess_list.extend([
            tio.RescaleIntensity((-1, 1)),
            tio.CropOrPad(shape),
            tio.EnsureShapeMultiple(8),  # for the U-Net
            tio.OneHot()
        ])

        return tio.Compose(preprocess_list)

    @staticmethod
    def get_augmentation_transforms() -&gt; tio.Transform:
        &#34;&#34;&#34;&#34;
        Get augmentation transorms to apply to subjects during training.

        Returns
        -------
        augment : tio.Transform
            All augmentation steps that should be applied to subjects during
            training.
        &#34;&#34;&#34;
        augment = tio.Compose([
            tio.RandomAffine(),
            tio.RandomGamma(p=0.5),
            tio.RandomNoise(p=0.5),
            tio.RandomMotion(p=0.1),
            tio.RandomBiasField(p=0.25),
        ])
        return augment

    def train_dataloader(self, *args, **kwargs):
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for train.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of train dataloaders specifying training samples.
        &#34;&#34;&#34;
        return super().train_dataloader(num_workers=0, shuffle=False)

    def val_dataloader(self, *args, **kwargs):
        &#34;&#34;&#34;
        Generates one or multiple Pytorch DataLoaders for validation.

        Returns
        -------
        _ : Collection of DataLoader
            Collection of validation dataloaders specifying validation samples.
        &#34;&#34;&#34;
        return super().val_dataloader(num_workers=0, shuffle=False)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule" href="brain_aging_prediction.html#radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule">BrainAgingPredictionDataModule</a></li>
<li><a title="radio.data.visiondatamodule.VisionDataModule" href="../visiondatamodule.html#radio.data.visiondatamodule.VisionDataModule">VisionDataModule</a></li>
<li><a title="radio.data.basedatamodule.BaseDataModule" href="../basedatamodule.html#radio.data.basedatamodule.BaseDataModule">BaseDataModule</a></li>
<li>pytorch_lightning.core.datamodule.LightningDataModule</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="radio.data.datamodules.brain_aging_prediction_patch.BrainAgingPredictionPatchDataModule.intensity2template"><code class="name">var <span class="ident">intensity2template</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="radio.data.datamodules.brain_aging_prediction_patch.BrainAgingPredictionPatchDataModule.label2template"><code class="name">var <span class="ident">label2template</span> : Dict[str, string.Template]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule" href="brain_aging_prediction.html#radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule">BrainAgingPredictionDataModule</a></b></code>:
<ul class="hlist">
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.EXTRA_ARGS" href="../basedatamodule.html#radio.data.basedatamodule.BaseDataModule.EXTRA_ARGS">EXTRA_ARGS</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.dataset_cls" href="../basedatamodule.html#radio.data.basedatamodule.BaseDataModule.dataset_cls">dataset_cls</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.dims" href="../basedatamodule.html#radio.data.basedatamodule.BaseDataModule.dims">dims</a></code></li>
</ul>
</li>
<li><code><b><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule" href="brain_aging_prediction.html#radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule">BrainAgingPredictionDataModule</a></b></code>:
<ul class="hlist">
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.check_if_data_split" href="../visiondatamodule.html#radio.data.visiondatamodule.VisionDataModule.check_if_data_split">check_if_data_split</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.dataloader" href="../visiondatamodule.html#radio.data.visiondatamodule.VisionDataModule.dataloader">dataloader</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.default_transforms" href="brain_aging_prediction.html#radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.default_transforms">default_transforms</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.get_augmentation_transforms" href="brain_aging_prediction.html#radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.get_augmentation_transforms">get_augmentation_transforms</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.get_max_shape" href="../visiondatamodule.html#radio.data.visiondatamodule.VisionDataModule.get_max_shape">get_max_shape</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.get_paths" href="brain_aging_prediction.html#radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.get_paths">get_paths</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.get_preprocessing_transforms" href="brain_aging_prediction.html#radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.get_preprocessing_transforms">get_preprocessing_transforms</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.get_subjects" href="brain_aging_prediction.html#radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.get_subjects">get_subjects</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.get_subjects_dicts" href="brain_aging_prediction.html#radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.get_subjects_dicts">get_subjects_dicts</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.predict_dataloader" href="../visiondatamodule.html#radio.data.visiondatamodule.VisionDataModule.predict_dataloader">predict_dataloader</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.prepare_data" href="brain_aging_prediction.html#radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.prepare_data">prepare_data</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.save" href="brain_aging_prediction.html#radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.save">save</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.setup" href="brain_aging_prediction.html#radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.setup">setup</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.size_eval_dataset" href="../visiondatamodule.html#radio.data.visiondatamodule.VisionDataModule.size_eval_dataset">size_eval_dataset</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.size_train_dataset" href="../visiondatamodule.html#radio.data.visiondatamodule.VisionDataModule.size_train_dataset">size_train_dataset</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.teardown" href="../visiondatamodule.html#radio.data.visiondatamodule.VisionDataModule.teardown">teardown</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.test_dataloader" href="../visiondatamodule.html#radio.data.visiondatamodule.VisionDataModule.test_dataloader">test_dataloader</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.train_dataloader" href="../visiondatamodule.html#radio.data.visiondatamodule.VisionDataModule.train_dataloader">train_dataloader</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction.BrainAgingPredictionDataModule.val_dataloader" href="../visiondatamodule.html#radio.data.visiondatamodule.VisionDataModule.val_dataloader">val_dataloader</a></code></li>
</ul>
</li>
<li><code><b><a title="radio.data.basedatamodule.BaseDataModule" href="../basedatamodule.html#radio.data.basedatamodule.BaseDataModule">BaseDataModule</a></b></code>:
<ul class="hlist">
<li><code><a title="radio.data.basedatamodule.BaseDataModule.name" href="../basedatamodule.html#radio.data.basedatamodule.BaseDataModule.name">name</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="radio.data.datamodules" href="index.html">radio.data.datamodules</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="radio.data.datamodules.brain_aging_prediction_patch.BrainAgingPredictionPatchDataModule" href="#radio.data.datamodules.brain_aging_prediction_patch.BrainAgingPredictionPatchDataModule">BrainAgingPredictionPatchDataModule</a></code></h4>
<ul class="">
<li><code><a title="radio.data.datamodules.brain_aging_prediction_patch.BrainAgingPredictionPatchDataModule.intensity2template" href="#radio.data.datamodules.brain_aging_prediction_patch.BrainAgingPredictionPatchDataModule.intensity2template">intensity2template</a></code></li>
<li><code><a title="radio.data.datamodules.brain_aging_prediction_patch.BrainAgingPredictionPatchDataModule.label2template" href="#radio.data.datamodules.brain_aging_prediction_patch.BrainAgingPredictionPatchDataModule.label2template">label2template</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>